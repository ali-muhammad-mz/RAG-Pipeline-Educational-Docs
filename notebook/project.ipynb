{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "495ed106",
   "metadata": {},
   "source": [
    "## Data ingestion Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba24cc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import uuid\n",
    "import chromadb\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e4ccf4",
   "metadata": {},
   "source": [
    "#### Document loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32322d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39985d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = r\"D:\\RAG\\project\\data\\pdfs\"\n",
    "loader = DirectoryLoader(\n",
    "    directory_path,\n",
    "    glob=\"**/*.pdf\",  # matches all PDFs in directory and subdirectories\n",
    "    loader_cls=PyPDFLoader,\n",
    "    show_progress=False  # optional: shows loading progress\n",
    ")\n",
    "\n",
    "# Load all documents\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bad7e00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A\\nBenchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nRishi Kesav Mohan\\nrkmohan2@illinois.edu\\nRisheek Rakshit Sukumar Kanmani\\nrrs7@illinois.edu\\nKrishna Anandan Ganesan\\nkag8@illinois.edu\\nNisha Ramasubramanian\\nnr50@illinois.edu\\nABSTRACT\\nIn the era of big data, conventional RDBMS models have become\\nimpractical for handling colossal workloads. Consequently, NoSQL\\ndatabases have emerged as the preferred storage solutions for exe-\\ncuting processing-intensive Online Analytical Processing (OLAP)\\ntasks. Within the realm of NoSQL databases, various classifications\\nexist based on their data storage mechanisms, making it challenging\\nto select the most suitable one for a given OLAP workload. While\\neach NoSQL database boasts distinct advantages, inherent scalabil-\\nity, adaptability to diverse data formats, and high data availability\\nare universally recognized benefits crucial for managing OLAP\\nworkloads effectively. Existing research predominantly evaluates\\nindividual databases within custom data pipeline setups, lacking\\na standardized approach for comparative analysis across different\\ndatabases to identify the optimal data pipeline for OLAP workloads.\\nIn this paper, we present our experimental insights into how vari-\\nous NoSQL databases handle OLAP workloads within a standard-\\nized data processing pipeline. Our experimental pipeline comprises\\nApache Spark for large-scale transformations, data cleansing, and\\nschema normalization, diverse NoSQL databases as data stores, and\\na Business Intelligence tool for data analysis and visualization.\\nThe wide-ranging classifications of NoSQL databases include\\ndocument-oriented, key-value stores, columnar databases, and graph\\ndatabases. For our experiments, we selected MongoDB, Redis, Apache\\nKudu, and ArangoDB, each representing a distinct family of NoSQL\\ndatabases. Leveraging a standardized pipeline, we assessed the per-\\nformance of these databases using Koalabench, a popular NoSQL\\nbenchmarking dataset collection. Each pipeline we setup has a\\nunique NoSQL database and the performance of each pipeline has\\nbeen evaluated for data loading time and query execution time. We\\nhave also compared the performance of a standard SQL solution\\n(PostgreSQL) against the different NoSQL alternatives. Koalabench\\ngenerated datasets of varying sizes in our desired data model and\\nwe conducted a series of experiments using data belonging to two\\ndifferent data models - flat and snow. The insights gleaned from\\nthese experiments will facilitate the establishment of an optimal\\nOLAP data pipeline, pairing the ideal NoSQL database as the data\\nwarehousing solution.\\n1 INTRODUCTION\\nModern day data engineering has seen numerous enhancements.\\nEvery last ounce of data is being considered as a feature. With data\\nhaving immense significance in the modern world, the way in which\\nwe handle and process data should also evolve with requirements\\nand time. To develop a holistic data engineering pipeline, there are\\nthree crucial components:\\n• Data Loading and Processing\\n• Data Store\\n• Data Analysis\\n1.1 Data Loading and Processing\\nData loading and processing consists of the seamless ingestion,\\ntransformation, and preparation of data sourced from various ac-\\ncessible outlets. Given the enormous volume of data to be managed,\\nit requires underlying systems with enhanced processing capabili-\\nties to facilitate efficient handling. Apache’s Hadoop Distributed\\nFile System (HDFS) [1] emerges as one of the prominent solutions\\nbuilt for large-scale data storage. HDFS offers high-throughput ac-\\ncess to application data and is ideal for batch processing operations.\\nWith HDFS taking care of large storage, we would have to imple-\\nment a solution to handle data processing.Data processing is a key\\naspect of any data pipeline and it governs the process of converting\\nthe raw data into a database compatible format before loading it\\ninto the database. Apache Spark [2] stands out as a prime choice\\nfor swift and efficient data processing. Its seamless integration with\\nHDFS enables the seamless transfer of data from HDFS to Spark\\nfor processing or streaming to subsequent components within the\\npipeline. Employing these two tools in tandem establishes an opti-\\nmal platform for transforming data, rendering it ready for import\\ninto the designated data store.\\n1.2 Data Store\\nOnce the voluminous data has been stored and processed, we would\\nrequire a solution for storing the processed data. Ideally, this compo-\\nnent would be a database, which helps in storing data in a structured\\nformat and it can be accessed using query languages specific to\\nthe chosen database. The traditional relational database manage-\\nment system (RDBMS) [17] can store structured data effectively\\nbased on a predefined schema. RDBMS also requires complex query\\noperations such as joins in the event of requiring data from more\\nthan one table and these complex query operations are generally\\nresource intensive.\\nWith the rise of semi-structured data, there is scope for exploring\\nthe possibility of OLAP workloads. NoSQL databases have been\\nadept at leveraging semi-structured data formats such as CSV and\\nJSON which has allowed for the incorporation of a flexible data\\nschema, ease of scalability and support for heterogenous datatypes.\\nNoSQL databases promote flexibility in schema and their data stor-\\nage mechanism allows for easy access of different data features\\nwithout the need for executing complex joins to derive insights.\\nDespite the benefits of NoSQL over SQL, prevailing architectures\\nhave predominantly relied on SQL databases as their primary data\\narXiv:2405.17731v1  [cs.DB]  28 May 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\nstore. There exists a significant disparity in evaluating the perfor-\\nmance of NoSQL databases within an OLAP-intensive pipeline.\\n2 RELATED WORK\\nThe literature survey undertaken centered on evaluating prior re-\\nsearch within the realms of data processing and databases. These\\ndomains were identified as primary areas of interest for assessing\\nthe overarching structure of our envisioned pipeline.\\n2.1 HDFS and Spark for OLAP\\nOn the the topic of data processing, we looked far and wide to\\nunderstand the depth of research that had been performed in order\\nto gauge the progress from the perspective of data engineering\\npipelines. One of the key ideas, HDFS [22] has paved the way for\\nstoring large datasets and also provides an invaluable framework\\nfor carrying out analysis and transformation operations on the\\nlarge data using the MapReduce [10] algorithm. HDFS has paved\\nthe way for several database and data streaming solutions to be\\nbuilt on top it. One such data streaming service that has become\\nwidely popular is Apache Spark [ 25]. Apache Spark helps in the\\nstreaming and processing of voluminous data from HDFS. HDFS\\nand Spark in tandem can handle the load of data processing on any\\nindustry-level data engineering pipeline.\\nResearch done based on leveraging HDFS and Spark for OLAP\\nworkloads is rather insignificant in the context of NoSQL databases.\\nThe standalone research of OLAP pipelines was limited in the pre-\\nvious years, however certain hybrid ideas were proposed. One such\\nidea called Hybrid Transaction/Analytical Processing (HTAP) [18]\\nproposed for a unified storage for both OLTP and OLAP workloads.\\nWhile this may seem feasible considering the flexibility of use that\\nsystems like HDFS and Spark provide in connecting with different\\ndatabase solutions, this idea does not account for the data rigidity\\nthat exists in transactional and analytical processing. RDBMS solu-\\ntions require a rigid schema and lack support for heterogeneous\\ndatatypes whereas NoSQL addresses both concerns. Hence, having\\na common store and unifying OLTP and OLAP is a little far-fetched\\nfor now.\\nWildfire by IBM [4] leveraged a HTAP like solution using HDFS\\nand Spark to perform data analysis and was one of the first solutions\\nto implement a data processing component in their pipeline. This\\nwork had stressed on the importance of having distributed data\\nstore and distributed data processing to make OLAP more effective.\\nHowever, Wildfire was based on Spark SQL which is more similar to\\nRDBMS. Stream processing [21] became a popular research interest\\npost HDFS and Spark as many ideas revolved around bringing\\ntogether Spark and OLAP through the metaphorical ’cube’. This\\nprocess involved transforming raw data using Spark into an OLAP\\nfriendly format via a process called cubification. The resultant OLAP\\ncube, contained the features extracted from the provided data, in the\\nform of a cube on top of which data analysis could be performed to\\ngather insights. When distributed processing is employed, the use\\nof OLAP cubes becomes restricted to gathering insights from the\\nprocessed data and does not aid in determining a scalable strategy\\nfor processed data ingestion into the database.\\n2.2 Benchmarking for NoSQL-OLAP\\nThe second broad topic we researched on is the availability of\\nliterature for NoSQL databases being used in OLAP/OLTP work-\\nloads. This exploration was an effort to understand whether there\\nwas a possibility to use NoSQL databases for OLAP. Based on the\\nNoSQL OLAP literature we identified, we also looked for previous\\nbenchmarking experiments performed for NoSQL OLAP works.\\nOne of the premier works in the NoSQL-OLAP domain involved\\nthe creation of OLAP cubes using distributed processing without\\nthe need of RDBMS [11]. Using the MC-Cube operator developed\\nin this paper, the idea is to perform transformations on the data\\nstored in columnar NoSQL databases and the data features get bun-\\ndled into \"cubes\" which can then be used for analytical insights.\\nThis idea came up short because of the use of just a columnar\\nNoSQL database as other NoSQL databases were not tested in their\\nexperiments. Another idea on the contrary [8] used a document-\\noriented NoSQL database as the data store solution but employed\\na series of techniques such as shingling, chunck, minhashing, and\\nlocality-sensitive hashing MapReduce. These techniques do serve\\nthe purpose of extracting features from the available data but fall\\nshort when the data scales.\\nWhilst the initial literature stresses more on the usage of NoSQL\\ndatabases for OLAP workloads, there has been some research done\\non benchmarking NoSQL databases purely from a query processing\\nperspective for OLAP. The current evaluations of NoSQL databases\\nhave primarily focused on simplistic metrics, such as the loading\\ntime when used as a data warehouse. However, these comparisons\\nfail to capture the nuanced performance differences within a pro-\\ncessing pipeline. Extensive research [6, 7] has delved into NoSQL\\nbenchmarks, yet these studies often prove inadequate for com-\\nprehensive NoSQL database comparisons in a processing context.\\nThey emphasize the stark disparities between new benchmarks and\\ntraditional Relational benchmarks, illustrating the superiority of\\nthe former. For instance, Chevalier et al. ’s [6] introduction of Koal-\\naBench — a versatile benchmarking tool—enables the analysis of\\nboth relational and NoSQL databases, thereby addressing this gap.\\nNumerous studies explore different aspects of NoSQL databases or\\ncompare various NoSQL solutions. For instance, [13] highlights the\\nsignificance of Graph Databases in data analysis, while [7] provides\\na comparative analysis of three popular NoSQL databases in the\\ncontext of Big Data and Cloud Computing. Moreover, [12] details\\nthe evaluation of Columnar stores, particularly focusing on HBase\\nand utilizing the Star schema as a metric. Additionally, [13] con-\\nducts a comparative study between HBase and MongoDB using a\\nstar schema setup, further enriching our understanding of these\\ndatabase systems.\\n3 THE GENERIC PIPELINE\\nThe generic pipeline that we envisioned consists of three broad\\ncomponents - data processing, the data store and the data analytics\\nsolution. By feeding data to the data processor, we transform the\\ndata into a usable format for the data store. All the pipelines we have\\ncreated uses a different type of NoSQL database as its data store.\\nFinally, all the pipelines culminate with a data analytics solution\\nthat reads off the processed data stored in the database to create'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nFigure 1: Schema of the dataset based off the Snow Logical\\nData Model generated by Koalabench\\nanalytical insights. The pipeline can be further explained in the\\nfollowing sections:\\n3.1 Data Processing\\nThe data processor section consists of two major components -\\nApache HDFS and Apache Spark. All the data files required for the\\npipeline are first loaded on to HDFS. Thanks to the distributed\\nstorage and distributed processing capabilities of using HDFS and\\nSpark in tandem, we were able to load data from HDFS and utilize\\nPySpark to load the transformed data into the database.\\nThe raw data we load into HDFS is provided to us by Koalabench\\n[14]. Koalabench can be broadly defined as a decision support bench-\\nmark for Big Data requirements. It is derived from the TPC-H\\nbenchmark, the reference benchmark in research and industry for\\ndecision support systems. It has been adapted to support Big Data\\ntechnologies such as NoSQL databases and HDFS. It can generate\\ndata in different data formats and in different data models. There\\nare four primary data logical models supported by Koalabench, out\\nof which the two data models that we have used are the following :\\n3.1.1 Snowflake Data Model . This data model is very close to\\nthe one used in the TPC- H benchmark with small modifications.In\\na classic snowflake data model, there are three major components.\\nFact Tables are the prominent tables based on which most of the data\\nquerying is performed - ideally they are the centre of attraction in\\na snowflake data model. Component Tables are supporting tables to\\nthe fact table in the sense that they contain most of the required data\\nattributes to know more about the fact table records. Relationship\\ntables are used when multiple component tables need to be linked\\nto create a compound component table that aids in providing more\\ninformation about the fact table records.\\nAs per Figure 1, the element in green, Lineitem would be the only\\nfact table for our data model. All tables except PartSupplier will be\\ncomponent tables which aid in providing attributes to Lineitem via\\nsimple relationships. PartSupplier is a relationship table composed\\nof attributes from Part and Supplier component tables.\\n3.1.2 Flat Data Model . This is the simplest data model of all\\nthe supported ones. As per Figure 2, Lineitem carries over as the\\nFigure 2: Schema of the dataset based off the Flat Logical Data\\nModel generated by Koalabench\\npremier entity based on which all the data features are developed\\non. In the flat data model, the significant data parameters from\\ncomponent tables is included within the Lineitem fact table. All\\nrelationship tables removed as the flat data model promotes a sim-\\npler way of storing data without the need for establishing complex\\nrelationships.\\nThe common file format used by all the pipelines for their raw\\ndata is CSV and we have utilized the datasets generated using the\\nflat logical data model for the NoSQL databases and the dataset\\nbased on the Snowflake logical data model has been used in the\\nPostgreSQL pipeline. This helps us evaluate the performance of the\\ndatabases with the way in which they handle scalable data upon\\ndata ingestion.\\nThe data from HDFS is pushed to Spark for performing data\\ntransformations. As our test environment had limited hardware re-\\nsources to play with, we resorted to utilizing thenum_partitions fea-\\nture of Spark. In Apache Spark, num_partitions refers to the param-\\neter used to specify the number of partitions to divide an Resilient\\nDistributed Dataset (RDD) or dataframe into during processing.\\nProperly setting num_partitions can optimize performance by bal-\\nancing data distribution across the available computing resources.\\nIt is essential to choose an appropriate value for num_partitions to\\nensure efficient parallel processing and avoid resource contention\\nin Spark applications.\\n3.2 Data Store\\nOnce the processsed data is ready from Spark, we begin the process\\nof inserting the records into the data store component. For simplic-\\nity, we have taken up different NoSQL databases to act as the data\\nstore for the processed data from Spark. Spark supports interactions\\nwith multiple popular SQL and NoSQL databases which has allowed\\nus to use four different NoSQL databases based on the four broad\\nclassifications of NoSQL databases. They are as follows:\\n3.2.1 MongoDB. MongoDB [15] is a popular NoSQL database\\nknown for its flexible document-oriented data model, which stores\\ndata in BSON (Binary JSON) format. It offers scalability and high per-\\nformance, with features like sharding and replication for handling\\nlarge-scale deployments. MongoDB’s expressive query language\\nand rich ecosystem of tools make it well-suited for a wide range of\\napplications, from web development to analytics.\\n3.2.2 ArangoDB. ArangoDB [24] is a multi-model database sys-\\ntem supporting document, key/value, and graph data models in\\na single database core. It offers a versatile query language, AQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\n(ArangoDB Query Language), and boasts features like distributed\\ngraph processing and geo-spatial indexing. With its flexible data\\nmodel and powerful querying capabilities, ArangoDB is suitable for\\ndiverse use cases including social networks, recommendation en-\\ngines, and real-time analytics. For the purposes of our experiments,\\nwe have leveraged the graph data models of ArangoDB owing to\\nthe Graph databases family of NoSQL solutions.\\n3.2.3 Apache Kudu. Apache Kudu [3] is an open-source columnar\\nstorage engine designed for fast analytics on rapidly changing data.\\nIt combines the performance of traditional columnar databases with\\nthe ease of use of Hadoop. With features like automatic partition-\\ning and fault tolerance, Apache Kudu is ideal for real-time data\\ningestion and interactive analytics applications.\\n3.2.4 Redis. Redis [19] is an in-memory data store known for its\\nhigh performance and versatility in caching, session management,\\nand real-time analytics. It supports various data structures like\\nstrings, hashes, lists, sets, and sorted sets, making it suitable for a\\nwide range of use cases.\\nApart from the mentioned NoSQL databases, we have performed\\none additional experiment using PostgreSQL [23] as the RDBMS\\nsolution for the data store. This experiment has helped us compare\\nand contrast the performance of SQL and NoSQL databases in\\nthe generic pipeline. The processed data has been saved on the\\nrespective databases thanks to the versatility provided by PySpark\\nin developing seamless interaction modules between Spark and the\\ndatabases.\\nOnce the data has been loaded, we have executed a subset of\\nthe queries belonging to the TPC-H benchmark. As Koalabench\\nderives its existence from TPC-H, it has allowed us to reuse the\\nsame template of the documented benchmark queries. However,\\nbased on how the data is stored in the different databases, we have\\ncome up with equivalent queries for each of the NoSQL databases\\nand executed them on the databases respectively.\\n4 ARCHITECTURE\\nFor the evaluation of each proposed data pipeline as part of our\\nbenchmarking process, we plan to execute datasets of varying scale\\nfactors generated from the Koalabench suite. These datasets will\\nbe run through each pipeline within our standardized local testing\\nenvironment, which consists of a system running MacOS Sonoma\\n14.4.1, powered by an Apple M2 chip with 8GB of RAM and 128GB\\nof storage. To support distributed data processing capabilities for\\nHadoop and Spark, we intend to create a dockerized environment\\nwithin the local system. This environment will comprise a master-\\nworker setup facilitating the execution of distributed tasks.\\nThe architecture shown in Figure 3 consists of one master node\\nconcurrently acting as a worker, along with three dedicated worker\\nnodes. The master node maintains bidirectional communication\\nroutes with each worker node, enabling seamless coordination\\nfor distributed processing across HDFS and Spark clusters. The\\ndockerized environment is built on Debian Linux v11 across all\\nnodes. To facilitate distributed data processing capabilities, each\\nnode is equipped with Hadoop-3.3.6 and Spark-3.4.1.\\nWe have incorporated the latest docker images for all the databases\\nunder evaluation, including Postgres-alpine-3.19, MongoDB-7.0.8,\\nFigure 3: Dockerized experimental setup consisting of one\\nmaster and three worker nodes\\nArangoDB-3.12.0, Redis-7.2.4, and Apache Kudu-1.17. These database\\nimages have been instantiated within the same docker network\\nas the experiment nodes, enabling efficient measurement of time-\\nbased metrics without introducing potential biases stemming from\\nnetwork latency. The data analysis component of our pipeline will\\nleverage a Business Intelligence (BI) tool connected to the respective\\ndatabase docker image for accessing and analyzing stored data. The\\ntentative BI tools identified for this purpose areMetabase v0.49.3 and\\nTableau 2024.1. The BI container will reside within the same docker\\nnetwork as the experimental setup to ensure seamless integration\\nand communication.\\n5 EXPERIMENTS\\nThe following section elaborates on the dataset that we have used\\nto evaluate the performance of the various pipelines and documents\\nthe findings from the individual experiments we have carried out\\nin each data pipeline we developed.\\n5.1 Dataset\\nTo evaluate our pipelines across varying data scales, we have used\\ndifferent datasets generated from Koalabench using the flat data\\nmodel based on varying scale factor(sf). The primary objective of\\nutilizing different sf values is to maximize the dataset size, enabling\\ncomprehensive testing of our pipelines’ performance and scalability\\nacross a broad range of data volumes. We have conducted exper-\\niments on all the pipelines using sf1, sf2, sf3, sf4 and sf5 datasets\\nwith the data format set as CSV for all datasets. The minimum\\nsize of the collection belongs to the sf1 dataset at 2.38 GB and the\\nmaximum size of the collections belongs to sf5 which scales up to\\napproximately 11GB. For the PostgreSQL pipeline, we have lever-\\naged the snow data model to generate datasets ranging from sf1 to\\nsf5 in the CSV format. The least size is with sf1 at 1.4GB and the\\nmaximum size is with sf5 at approximately 6GB.\\n5.2 Data Loading Time\\nTable 1 portrays the load times for different datasets across the five\\ndifferent databases.\\nApache Kudu has shown the smallest load times across all five\\ndatasets. One of the primary reasons for this observation is the\\ncolumnar-storage mechanism that Apache Kudu employs coupled\\nwith its superior in-memory management that flushes out records'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nTable 1: Data Loading times for all databases across the five\\ndatasets (time in seconds)\\nDatabases SF1 SF2 SF3 SF4 SF5\\nPostgreSQL 37s 375s 857s 1089s 1481s\\nMongoDB 90s 1250s 1701s 2275s 2810s\\nArangoDB 295s 2249s 3964s 12169s 15162s\\nRedis 1495s 3245s 5023s 7748s 10289s\\nApache Kudu 42s 95s 146s 192s 240s\\nas and when the threshold is reached. This allows Kudu to han-\\ndle scaling data efficiently and keep the overall loading time to\\nagreeable levels.\\nMongoDB seems to have progressively higher data ingestion\\ntimes. MongoDB’s document-oriented storage model and lack of\\nnative sharding capabilities may lead to increased data ingestion\\ntimes due to increased indexing and write operations overhead.\\nAdditionally, as the dataset size grows, MongoDB’s reliance on\\nmemory-mapped files for storage may result in slower write per-\\nformance and increased disk I/O operations.\\nRedis displays the poorest performance on all 5 datasets. The\\nvery high data loading times observed in Redis can be attributed\\nprimarily to the schema we used for the processed data. In order\\nto keep the schema uniform across all NoSQL databases, we opted\\nfor having the linenumber_id as the only key and all the remaining\\ndata features were aggregated into a single list and attributed to\\nthe corresponding linenumber_id. In general, we ended up creating\\na record that had a single key and 40 values. Additionally, redis\\nbeing a single threaded solution meant that for every insertion\\noperation had two call operations to make. With data scaling, this\\nwould increase exponentially and in the end be double of what an\\nideal PostgreSQL implementation would execute.\\nArangoDB was implemented using its document model and its\\ndata insertion times are exorbitant owing to the document model\\nthat was implemented. ArangoDB is definitely not an ideal solution\\nshould the data scale as for sf5, the data insertion time is almost\\n50x what it was for sf1.\\nPostgreSQL shows a gradual increase in data insertion times from\\nsf1 all the way upto sf5. It is able to handle scaling data much more\\nefficiently has compared to some of its NoSQL counterparts.\\nBased on this experiment, it is clear that with a NoSQL database\\nas your data store, it is preferrable to choose a columnar storage\\nsolution in case the pipeline has to be robust enough to handle\\nscaling data.\\n5.3 Query Processing Time\\nAs Koalabench is based off the TPC-H benchmark, we selected five\\nout of the seventeen benchmark queries that TPC-H has to offer. The\\nTPC-H benchmark is meant for running benchmark experiments\\non RDBMS solutions. Hence, the queries can be directly applied\\nto the PostgreSQL data pipeline as the schema is same as the one\\nproposed by TPC-H. For all NoSQL data pipelines that leverage\\nthe flat data model, we prepared queries in the native querying\\nlanguage of the NoSQL database present in the pipeline.\\nThe five queries we selected were different from one another\\nbased on the significant operation that was being performed by the\\nFigure 4: Query Execution time (in seconds) for PostgreSQL\\nacross the five datasets\\nquery. For context, Query 1 was aggregation intensive in which\\nalmost eight out of the ten values that were being fetched were\\nbased off aggregation. Query 2 had a good balance of aggregation\\nand join operations. Query 3 had an aggregation along with a sub-\\nquery based filter. Query 4 had five join operations and Query 5\\nwas the simplest query off them all with just a single aggregation\\noperation.\\n5.3.1 PostgreSQL. Figure 4 shows a graph depicting the query\\nexecution times for the PostgreSQL data pipeline. Across the five\\ndatasets and for the five queries, we see a steady increase by a factor\\nof 2x for almost all queries except Queries 3 and 4. The reason for\\nthe observation of exponential rise in query execution times for\\nthose two queries alone is because of the inherently costly join\\noperations of RDBMS solutions which tend to compare every row\\nof the two tables. For filtering and aggregation queries, the increase\\nin query execution time is 0.33x for every 2x scale in data which is\\npretty commendable.\\n5.3.2 MongoDB. Figure 5 shows a graph displaying the query\\nexecution times for the MongoDB pipeline for the five selected\\nbenchmark queries across the five datasets. MongoDB uses the\\ntranslated version of the benchmark queries and the major change\\nbetween the original and translated versions are that all join related\\noperations become filter related operations. This is because of the\\nnon-availability of support for joins in NoSQL databases.\\nMongoDB handles most of the queries easily with scaling data\\nwith a maximum increase in execution times by about 0.8x for every\\n2x increase in data. This can be attributed to MongoDB’s inbuilt\\nsharding capabilities, that promote horizontal scaling approach that\\nallows it to handle larger data sets and provide high throughput\\noperations by distributing data across multiple shards. One aspect\\nwhere MongoDB struggles is with intense aggregation queries\\nsuch as Query 1. With data scaling, MongoDB is forced to look\\nup multiple document objects and perform compute operations on\\nthem which are CPU intensive and tend to consume more time\\nthan rest of the query operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\nFigure 5: Query Execution time (in seconds) for MongoDB\\nacross the five datasets\\nFigure 6: Query Execution time (in seconds) for ArangoDB\\nacross the five datasets\\n5.3.3 ArangoDB. Figure 6 shows a graph displaying the query\\nexecution times for the ArangoDB pipeline for the five selected\\nbenchmark queries across the five datasets. ArangoDB uses the\\nArango Query Language (AQL) version of the benchmark queries.\\nArangoDB sees a 1.5x scale in query execution times for every\\n2x scale in data size. Whilst ArangoDB does not selectively out-\\nperform other data pipelines in certain scenarios, it does show a\\nsteady increase in query execution time with scaling data. This can\\nbe attributed to the dynamic query optimizer that selects the per-\\nfect query execution plan depending on the query being executed.\\nArangoDB employs efficient memory management techniques to\\nminimize disk I/O and maximize query performance. It utilizes\\nmemory caches for frequently accessed data and implements buffer\\nmanagement strategies to optimize disk access.\\nFigure 7: Query Execution time (in seconds) for Redis across\\nthe five datasets\\nFigure 8: Query Execution time (in seconds) for Apache Kudu\\nacross the five datasets\\n5.3.4 Redis. Figure 7 portrays a graph containing information on\\nthe query execution times of the five benchmark queries across the\\nfive datasets on Redis. Irrespective of the size of data, Redis takes\\nlonger than all the other databases to execute the queries. For sf1,\\nthe smallest dataset, Redis displays a best case of 3x increase in\\nterms of query execution times. Redis suffers primarily from the\\ndata schema we had selected for the flat data model. By assigning\\nall features to a single key, linenumber_id, a single row of record\\nbecomes huge for redis to retrieve from memory. By default, redis\\nis an in-memory intensive data store and hence, holding such a big\\nrecord in memory is bound to make the query execution slow and\\nCPU intensive.\\n5.3.5 Apache Kudu. Figure 8 shows the recorded query execu-\\ntion times for the five benchmark queries across the five datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nBased on these readings, it is clear that Apache Kudu performs\\nthe best amongst all the selected databases in terms of query exe-\\ncution. Kudu stores data in a columnar format and this is highly\\nefficient for analytical workloads because it allows queries to read\\nonly the columns needed for the query, minimizing I/O operations\\nand improving query performance. This can be visibly seen from\\nthe very minimal increase in query execution times across datasets\\nfor queries 2 to 5. Even when data scales at 2x, query execution\\ntimes have tended to stay flat and do not see a proportional or\\nexponential increase like the rest of the databases. The reason why\\nquery 2 sees an exponential increase with scaling data is related\\nto a potentially incorrect time measurement on behalf of Apache\\nKudu. When fetching data records based on given filter conditions,\\nKudu tends to write the retrieved records onto the impala shell.\\nThe write operation time is also included in the overall execution\\ntime of the query and thus it seems beefed up with scaling data\\ncompared to the rest of the queires.\\n6 CONCLUSION\\nThrough this paper, we envisioned our idea of building the ideal data\\nengineering pipeline, that would be take care of data processing and\\ndata store. We experimented with datasets of varying scale from the\\nKoalabench dataset where data was generated in two different data\\nmodels. Our experiments measured the query execution times for\\n5 benchmark queries and recorded the data ingestion time for all\\ndatabases. The effect of the data schema was evident in the varied\\ndata ingestion times. Certain databases did not handle the scaling\\nlevels of data and ended up seeing exponential increase in the query\\nexecution times of complex queries. A definitive finding from our\\nexperiments is the prominence of Columnar data storage options\\nin future OLAP research. Columnar storages tend to load data 2x\\nfaster than other SQL and NoSQL options and the same goes for\\nquery execution as well.\\nThese experiments also highlight the immense scaling and dis-\\ntributed processing capabilities that modern-day NoSQL solutions\\nhave and how these can be put to use to solve complex data analyt-\\nical problems. We believe this idea would be a pioneer towards the\\nadoption of more NoSQL-based data store solutions for evaluating\\nOLAP workloads. As part of future work, we do recommend the in-\\ntegration of a data analysis tool to commensurate the data pipeline.\\nResearch can be extended across various columnar storage options\\nto identify the ideal columnar storage option to evaluate OLAP\\nworkloads.\\nREFERENCES\\n[1] Apache. 2006. Hadoop Distributed File System. (2006). https://hadoop.apache.\\norg/\\n[2] Apache. 2014. Apache Spark. (2014). https://spark.apache.org/\\n[3] Apache. 2022. Apache Kudu. (2022). https://kudu.apache.org\\n[4] Ronald Barber, et.al. 2017. Evolving Databases for New-Gen Big Data Applica-\\ntions.\\n[5] Zane Bicevska and Ivo Oditis. 2017. Towards NoSQL-based Data Warehouse\\nSolutions. Procedia Computer Science 104, 104–111. https://doi.org/10.1016/j.\\nprocs.2017.01.080 ICTE 2016, Riga Technical University, Latvia.\\n[6] Max Chevalier, Mohammed El Malki, Arlind Kopliku, Olivier Teste, and Ronan\\nTournier. 2015. Benchmark for OLAP on NoSQL technologies comparing NoSQL\\nmultidimensional data warehousing solutions. In 2015 IEEE 9th International\\nConference on Research Challenges in Information Science (RCIS) . 480–485. https:\\n//doi.org/10.1109/RCIS.2015.7128909\\n[7] Max Chevalier, Mohammed El malki, Arlind Kopliku, Olivier Teste, and Ronan\\nTournier. 2015. Implementing Multidimensional Data Warehouses into NoSQL.\\nICEIS 2015 - 17th International Conference on Enterprise Information Systems,\\nProceedings 1. https://doi.org/10.5220/0005379801720183\\n[8] Farnaz Davardoost, Amin Babazadeh Sangar, and Kambiz Majidzadeh. 2020.\\nExtracting OLAP Cubes From Document-Oriented NoSQL Database Based on\\nParallel Similarity Algorithms. Canadian Journal of Electrical and Computer\\nEngineering 43, 2 (2020), 111–118.\\n[9] Lucas de Carvalho Scabora, Jaqueline Joice Brito, Ricardo Rodrigues Ciferri,\\nand Cristina Dutra de Aguiar Ciferri. 2016. Physical Data Warehouse Design\\non NoSQL Databases - OLAP Query Processing over HBase. In International\\nConference on Enterprise Information Systems . https://api.semanticscholar.org/\\nCorpusID:2636732\\n[10] Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified Data Process-\\ning on Large Clusters.\\n[11] Khaled Dehdouh. 2016. Building OLAP Cubes from Columnar NoSQL Data\\nWarehouses. In Model and Data Engineering , Ladjel Bellatreche, Óscar Pastor,\\nJesús M. Almendros Jiménez, and Yamine Aït-Ameur (Eds.).\\n[12] Khaled Dehdouh, Omar Boussaid, and Fadila Bentayeb. 2014. Columnar NoSQL\\nStar Schema Benchmark. In Model and Data Engineering , Yamine Ait Ameur,\\nLadjel Bellatreche, and George A. Papadopoulos (Eds.). Cham.\\n[13] David Dominguez-Sal, Norbert Martinez-Bazan, Victor Muntes-Mulero, Pere\\nBaleta, and Josep Lluis Larriba-Pey. 2010. A discussion on the design of graph\\ndatabase benchmarks. In Technology Conference on Performance Evaluation and\\nBenchmarking. Springer, 25–40.\\n[14] Mohammed El Malki, et al. 2022. Benchmarking Big Data OLAP NoSQL\\nDatabases. In Ubiquitous Networking, 2018, pp. 82–94 .\\n[15] Kevin P. Ryan Eliot Horowitz, Dwight Merriman. 2009. MongoDB. (2009).\\nhttps://www.mongodb.com/\\n[16] Abdelhak Khalil and Mustapha Belaissaoui. 2023. An Approach for Implementing\\nOnline Analytical Processing Systems under Column-Family Databases. IAENG\\nInternational Journal of Applied Mathematics 53, 1 (2023).\\n[17] Astrahan M., et. al. [n.d.]. System R: Relational Approach to Database Manage-\\nment.\\n[18] Fatma Özcan, Yuanyuan Tian, and Pinar Tözün. 2017. Hybrid Transac-\\ntional/Analytical Processing: A Survey. In Proceedings of the 2017 ACM Interna-\\ntional Conference on Management of Data (SIGMOD ’17). Association for Com-\\nputing Machinery, 1771–1775. https://doi.org/10.1145/3035918.3054784\\n[19] Salvatore Sanfilippo. 2009. Redis. (2009). https://redis.io/\\n[20] Nadia Ben Seghier and Okba Kazar. 2021. Performance benchmarking and\\ncomparison of NoSQL databases: Redis vs mongodb vs Cassandra using YCSB\\ntool. In 2021 International Conference on Recent Advances in Mathematics and\\nInformatics (ICRAMI). IEEE, 1–6.\\n[21] Salman Ahmed Shaikh and Hiroyuki Kitagawa. 2020. StreamingCube: Seamless\\nIntegration of Stream Processing and OLAP Analysis. IEEE Access 8 (2020),\\n104632–104649. https://doi.org/10.1109/ACCESS.2020.2999572\\n[22] Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. 2010.\\nThe Hadoop Distributed File System. In 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) . 1–10. https://doi.org/10.1109/MSST.\\n2010.5496972\\n[23] Michael Stonebreaker. 1996. PostgreSQL. (1996). https://www.postgresql.org/\\n[24] Claudius Weinberger and Frank Celle. 2015. ArangoDB. (2015). https://arangodb.\\ncom/\\n[25] Matei Zaharia, et al. 2016. Apache Spark. Communications of the ACM, vol. 59,\\nno. 11, 28 Oct. 2016, pp. 56–65 .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 14 \\n \\nInfluence of CAP Theorem on Big Data Analysis  \\nDr Anand Kumar Pandey [2], Rashmi Pandey [2] \\n[1] Computer Science and Application, ITM University \\n[2] Computer Science, ITM Group of Institute - Gwalior \\n \\nABSTRACT \\nIn the current modern world different computing society has developed their innovative solutions to meet the difficult \\nchallenges to handle linear expansion in large data collection by different sources. In this paper, we took a research oriented \\nview to achieve more significant ideas in the field of big data world and data science engineering. The CAP Theorem is a \\ncommonly cited unfeasibility outcome in distributed computing systems, particularly with NoSQL distributed databases. Cap \\ntheorem is very much influenced by cloud providers in the reference of their usability, the latency limit and system requirement. \\nThe Cap theorem is also very much influenced by distributer database also. To discover a substitute for CAP with a latency-\\ncentric point of view we have to observe how operation latency is exaggerated by network latency at dissimilar levels of \\nconsistency. \\nKeywords:-  CAP, Big Data, Analysis, Distributed System, NoSQL. \\n \\nI.     INTRODUCTION \\n   The CAP theorem is also known as Brewer’s Theorem, \\nbecause it was introduced by MIT Professor Eric A. Brewer \\nduring 2000 with the concept of distributed computing. Our \\nmain purpose in this paper is to consider the influence of CAP \\nTheorem in the broader perspective of big data analysis and \\ndistributed computing theory. \\nData analysis is usually applicable on current distributed big \\ndata centres to accomplish high performance and accessibility. \\nMost of the data science services try to preserve their services \\nstability in all situations. In modern world big data analysis \\nand distributed database system is bound to have partitions in \\na real-world system due to network failure or some other \\nreason. To describe the practical implementation of CAP \\ntheorem we can choose any real big data computing \\nenvironment for data analysis such as MongoDB, Cassandra \\nand with NoSQL database [2] . CAP theorem describes tha t \\nbefore choosing any Database including distributed database, \\naccording to your requirement we have to choose only \\nappropriate properties out of three. CAP theorem allows us to \\nfind out how we want to operate our distributed database \\nsystems when some other database servers decline to \\ncommunicate with each other due to some imperfection in the \\nsystem. During data analysis operations we try to retain the \\noriginality of actual data received from big data pool and \\nfollows all the suitable rules and regulations. Different types \\nof database segments, operators and users are activated during \\nthe task of appropriate data analysis. Therefore it is very \\nimportant to know about that which segment of dataset is \\nconsistent and suitable to apply some partition tolerance \\nrelated operations. Services availability and database partition \\ntolerance are inter-dependent to each other. It seems \\nmotivating to investigate which levels of regularity and \\nreliability are strong enough to be straight implicit by the CAP \\nconstraints. \\n \\nII.     THE CONCEPTUAL ASPECTS OF CAP \\nTHEOREM \\nThe CAP theorem explained the thought that there is a \\nelementary transaction between availability, consistency, and \\npartition tolerance. This transaction, which has become \\nidentified as the CAP Theorem, has been commonly discussed \\never since.  CAP theorem supports non-relational database \\n(NoSQl) are best for distributed network applications and big \\ndata analysis [1]. \\n   With description of CAP theorem Professor Brewer \\nillustrates that it not possible that a distributed computer \\nsystem can support the 3 following properties at a time: \\n \\nFig. 1  CAP Theorem and Distributed Database \\nmanagement \\n    \\n     CAP Theorem is a considered that a distributed database \\norganization can only consist of 2 of the 3 properties : \\nConsistency, Availability and Partition Tolerance as shown in \\nFigure 1. Some of the well known attention about the CAP \\nTheorem which derived from the fact is as follows: \\n \\n \\nRESEARCH ARTICLE                                                OPEN ACCESS'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 15 \\nA. Safety \\nCAP theorem has standard protection property, because \\nevery client sent correct request and receives correct response \\nfrom data centres. These data centre can be personalised or \\ngeneralized.   \\nB. Liveness \\nThe theorem is well defined but overall, it is quite lively. \\nAvailability is a classic liveness property because, every \\nrequest receives a response. \\nC. Unreliable \\n   There are so many different ways in which a system seems \\nunreliable. Since every request at last receives a response so \\nthat structure can be unreliable. There may be partitions, as is \\ndiscussed in the CAP Theorem. \\nIII. THE ROLE OF CAP THEOREM IN BIG \\nDATA ANALYSIS \\n     During the review of CAP theorem as a consider its role \\nand its influence for Big data analysis to achieve the \\ndistributed solutions. May be the first time solution is not \\nperfect, so it needs to repeat the process of data analysis and \\nagain identify the best appropriate solution. In this case we are \\ntrying to get solutions for such kind of big data distributed \\nsystem it is impossible to assurance all three features of Cap \\ntheorem like Consistency, availability and partition tolerance \\nall at the equivalent time. Here we try to analyses the \\napplicability and relationship of the CAP theorem with Big \\nData and distributed systems [1]. The CAP theorem is also \\nrelate with Hadoop, Big data Analytics, DBMS, network \\ncommunication system and with advanced data structure.  \\n    During data analysis at first we have to partition the data in \\nto appropriate segments. At the time of partitioning the user \\ninteracts with operator and operator interacts with database. \\nLets discuss the relationship of CAP theorem with big data \\ncentres and their nodes associated with wireless area network. \\nAs shown in figure 2 we have two data centres with their \\nindividual single server nodes and interconnected with \\ndedicated network [3]. Here we try to propose a framework of \\ninterconnected big data centres for specifying a huge set of \\ndistributed data consistency model. The relationship of \\ndistributed big data described the correlated aspects of CAP \\ntheorem as per their requirements and their needs in this real \\nworld also. \\n \\n \\n \\n \\nFig. 2  Relationship of database \\n    In this framework the CAP theorem involves as a software \\nservice consist in distributed system which makes wisdom to \\nbelieve those reliability models in this extent. In above \\nrelational database, we have achieved Availability of the both \\ndata centres with respect to their concerned nodes as well \\nas Partition Tolerance in both data centres even if they cannot \\ncorrespond [5]. A communicated network divider is a \\nparticular type of communication defect that divides the \\nnetwork into subsets of nodes such that nodes in one subset \\ncannot communicate with other nodes.\\n \\n \\nIV. FUTURE OF CAP THEOREM \\n   The concept of CAP theorem is best possible futuristic \\napproach to discuss basic trade off for available big data \\nanalytic solutions and distributed systems. In future it will \\neasily scrutinize the inbuilt trade off some insights into that \\nhow system can be considered to gather an application’s needs, \\nin spite of unpredictable networks [6]. With the reference of \\nCAP theorem we must know all theoretical aspects to achieve \\nthese challenges, and some modern techniques for supporting \\nwith the problem in real big data world system.  \\nIn this artificial intelligent and Big data world, the \\nnetworked world has altered significantly in the last two \\ndecades, creating new challenges for system designers, and \\nnew areas in which these same inherent trade-offs can be \\nexplored. We need new theoretical insights to address these \\nchallenge, and new techniques for coping with the problem in \\nreal-world systems. \\n \\nA. Mobile Wireless Network \\n    The CAP Theorem primarily determined on modern \\nwireless network services. Now days, we illustrate that its \\nconsiderable growth proportion of network communication \\ngoing to initiated by advanced mobile devices. A big data \\ndistributed database system is dedicated to comprise partitions \\nin a real-world system due to network failure or some other \\nreason. \\n    Especially, wireless network communication is particularly \\nuntrustworthy. The main problem that the frequency is going \\nto change quickly, so it is not easy to motivated the CAP \\nTheorem for stable partitions. In every wireless networks, \\npartitions are less common. After re-evaluating the CAP \\nTheorem in the framework of wireless networks, we expect to \\nbetter recognize the best appropriate solutions that take place \\nin these types of scenarios [4]. By re-examining the CAP \\nTheorem in the situation of wireless networks, we might hope \\nto enhanced understand the unique trade-offs that occur in \\nthese types of scenarios. \\n \\nB. Scalability \\n    The CAP Theorem describes that in the current scenario of \\na network partition, the administrator has to decide between \\nconsistency and availability. Gradually, we involve that our \\nsystems be designed, scalable not just for today’ s consumers \\nbut also for future growth . Spontaneously, we believe that'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 16 \\nstructure as scalable if it can mature resourcefully, using \\ninnovative resources capably to handle extra load. \\n \\nC. Tolerating Attacks \\nPartition tolerance describes those clusters that must \\ncontinue to work even though any number of communication \\nbreakdowns between nodes in the system. The CAP Theorem \\nfocuses on network partitions: occasionally, a number of \\nservers did not communicate consistently [5]. Progressively, \\nhowever, we felt that more rigorous attacks on networks. \\nTolerating these extra problematic forms of interruption \\nrequires a somewhat different understanding of the \\nfundamental consistency/ availability trade-offs. A rejection \\nof service attack, however, cannot basically be modeled as a \\nnetwork partition. \\n \\nV.    SEGMENTING TASK OF DATA   \\n  ANALYSIS \\nThose systems who are using aspects of CAP theorem some \\nof them many systems do not include a single uniform \\nrequirement. Some aspects of the system require strong \\nconsistency, and some require high availability. In this section \\nof the paper, we describe few of the dimensions along which a \\nsystem might be partitioned. It is not always clear the specific \\nguarantees that such segmentation provides, as it tends to be \\nspecific to the given application and the particular partitioning. \\n \\nA. Data Partitioning \\n     In this big data world different types of data analysis may \\nrequire different levels of consistency and availability. For \\nexample, an on-line shopping cart may be highly available, \\nresponding rapidly to user requests; yet it may be occasionally \\ninconsistent, losing a recent update in anomalous \\ncircumstances. The on-line product information for an e-\\ncommerce site may be somewhat inconsistent: users will \\ntolerate somewhat out- of-date inventory information. The \\ncheck-out/billing/shipping records, however, have to be \\nstrongly consistent. \\n \\nB. Functional Partitioning \\n     Many services can be divided into different subservices \\nwhich have different requirements. For example, an \\napplication might use a service for coarse-grained locks and \\ndistributed coordination. Whatever service or function we \\nneed to be use can be partition as per condition and \\nrequirement. \\n \\nC. Operation Partitioning \\n     Different operations may require different levels of \\nconsistency and availability. Moreover, different types of \\nupdates might provide different levels of consistency. The \\nCAP theorem and its data analysis provide with differing \\ntrade-offs for different types of read and write operations. \\n \\n \\n \\nD. User Partitioning \\n     Network partitions, and unfortunate network performance \\nin general, normally correlate with real geographic distance: \\nusers that are far away are more likely to see poor \\nperformance. Usually, one could imagine that a social \\nnetworking site might try to partition its users, ensuring high \\navailability among groups of friends. \\nVI.    CONCLUSIONS \\n   In this paper we discussed several aspects of the CAP \\ntheorem: the definitions, the conceptual aspects of Cap \\ntheorem, the role of cap theorem in Big data analysis, future \\nof CAP theorem in the literature are fairly paradoxical and \\ncounter- intuitive. The Cap theorem is also very much \\ninfluenced by distributer database also and it is not possible to \\nmake available reliable data on both the nodes and \\naccessibility of complete data. The CAP theorem describes \\nthat proposed distributed database system has to compose a \\ntransaction between Consistency and Availability when a \\nPartition occurs. \\n  \\nREFERENCES \\n[1] Brewer EA (2012) CAP twelve years later: How the \\n“rules” have changed. IEEE Computer 45(2):23–29, DOI \\n10.1109/MC.2012.37. \\n[2] Daniel J Abadi. Consistency tradeoffs in modern \\ndistributed database system design. IEEE Computer \\nMagazine, 45(2):37 – 42, February 2012. \\ndoi:10.1109/MC.2012.33. \\n[3] Dobre D, Viotti P, Vukolic M (2014) Hybris: Robust \\nhybrid cloud storage. In: ACM Symposium on Cloud ´ \\nComputing (SoCC), Seattle, WA, USA, pp 12:1 –12:14, \\nDOI 10.1145/2670979.2670991. \\n[4] Fekete A, Gupta D, Luchangco V, Lynch NA, \\nShvartsman AA (1996) Eventually-serializable data \\nservices. In: 15th ACM Symposium on Principles of \\nDistributed Computing (PODC), Philadelphia, PA, USA, \\npp 300–309. \\n[5] Francesc D. Munoz-Esco, Ruben de Juan-Martin, J. R. \\nGonzalez de, , Jose M. Bernabeu, CAP Theorm: \\nRevision of its Related Consistency Models, Technical \\nReport TR-IUMTI-SIDI-2017/002, Universitat \\nPolitecnica de Valencia, 46022 Valencia (Spain). \\n[6] Martin Kleppmann, A critique of the CAP Theorem, \\narticle published in researchgate on Sept 2015.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='Reﬂection on MongoDB Database Logical and\\nPhysical Modeling\\n1st Pieter Willem Jordaan\\nSchool for Electrical, Electronic and Computer Engineering\\nNorth-West University\\nPotchefstroom, South Africa\\npieterwjordaanpc@gmail.com\\n2nd Johann Erich Wolfgang Holm\\nSchool for Electrical, Electronic and Computer Engineering\\nNorth-West University\\nPotchefstroom, South Africa\\njohann.holm@nwu.ac.za\\nAbstract—Traditional relational database design uses concep-\\ntual, logical, and physical modeling based on Peter Chen’s\\nmethods. UML (Uniﬁed Modeling Language), though being a\\nset of software development tools, is often used to conceptually\\nmodel data and relationships. This paper presents a methodical\\napproach to logically and physically model data in MongoDB\\nby utilizing UML conceptual modeling aids. Application of data\\nmodels greatly impacts performance, scalability and ﬂexibility of\\ndata systems. Furthermore, application life-cycle and expansion\\nimpact long-term decisions made during modeling. This paper\\ntakes into consideration application requirements, access patterns\\nand life-cycle during logical and physical modeling in a cloud\\nenvironment. The ﬂexibility that a NoSQL modeling paradigm\\nembodies makes logical and physical modeling complex, with\\nno hard-and-fast rules. This paper presents logical and physical\\nmodeling concepts required during designing database applica-\\ntions with MongoDB.\\nIndex Terms—database, modeling, UML, MongoDB, cloud\\nI. I NTRODUCTION\\nEntity-relationship modeling methods and tools developed\\nby Peter Chen [1] have become the de facto standard for\\nmodeling and designing databases. Though designed for and\\nused by relational databases, it is being applied to NoSQL\\n(Not Only SQL) databases to similar effect [2].\\nNoSQL databases are, however, very different to traditional\\ndatabases when it comes to modeling, since scalability, con-\\nsistency and performance greatly depend on the underlying\\ndesign, especially considering their ﬂexible schema options\\n[3].\\nCloud-based development deﬁnes application criteria re-\\nquirements for developers. Additional emphasis is placed on\\nthe following requirements speciﬁc to cloud systems [4]:\\n• Ease of use;\\n• Scalability;\\n• Availability;\\n• Security.\\nScalability and availability cannot always be predicted due to\\nchanging environments and requirements - these factors should\\nbe taken into account when designing a database to allow for\\nﬂexibility, which ﬁt well into NoSQL database systems traits\\n[5].\\nMongoDB is a NoSQL database capable of running in the\\ncloud. It provides high availability, horizontal scalability and\\na ﬂexible data structure [6]. MongoDB has full relational\\ncapability, which suits traditional entity-relational modeling\\ntechniques [7]. Recently MongoDB gained ACID (Atomicity\\nConsistency Isolation Durability) support, which allows for\\ntransactions, much like with relational databases [8], [9].\\nFlexible schemas with no hard-and-fast rules such as the\\ntraditional 3NF (third normal form) [10] for designing con-\\nceptual and physical models, make NoSQL database modeling\\ncomplex.\\nThis paper provides a broad, but detailed, overview of\\ndatabase modeling concepts that have a bearing on design\\nchoices for NoSQL databases. We provide speciﬁc guidelines\\non how to employ MongoDB’s unique feature set. As a case\\nin point, we conceptually, logically and physically model a\\nsimplistic use-case, based on Chen’s methods. A discussion\\non the complexity of NoSQL modeling is provided, followed\\nby a conclusion with further research suggestions.\\nII. R ELATED WORK AND CONTRIBUTIONS\\nRelated studies have contributed to NoSQL modeling re-\\nsearch. Shin et al. [2] has provided a framework for mod-\\neling NoSQL databases generically from a conceptual UML\\nmodel based on Chen’s framework. However, the study does\\nnot discuss the physical modeling phase, and has a generic\\ndocument-based logical model.\\nAn algorithmic approach was developed by Abdelhedi et\\nal. [11] to transform conceptual UML class models into\\nphysical models for various NoSQL databases. The logical\\nmodel is used as an intermediate abstraction layer for a subset\\nof features in three NoSQL database types: column-based,\\ndocument-based and graph-based.\\nNoAM (NoSQL Abstract Data Model) presented by Atzeni\\net al. [12] offers a database-independent data model for\\nNoSQL databases, also making use of a UML-based concep-\\ntual data model.\\nDe Lima et al. [3] has published an algorithmic approach to\\nNoSQL logical modeling. Workﬂow measurements are to be\\nspeciﬁed along with the conceptual model when developing\\nlogical models. Physical modeling and its effects are not\\ndemonstrated.\\nThe previously mentioned research publications does not\\nconsider application life-cycle with regards to growth, scala-\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='bility and ﬂexibility. Furthermore, our study does not attempt\\nto generically model multiple NoSQL databases, but rather\\nreﬂects on factors supporting and leveraging MongoDB and\\nits speciﬁc set of features.\\nIII. D ATABASE MODELING\\nA. Uniﬁed Modeling Language\\nUML, originally developed as a set of tools for software\\nengineering, can be used for data modeling. Class diagrams\\nare a superset of ER (Entity-Relationship) diagrams used in\\nChen’s methods [13].\\nTools that this paper will use for modeling include use-case\\ndiagrams and class diagrams. Use-case diagrams, as in Fig.1,\\nare used to model actors performing activities on systems [14].\\nThis diagram is used to identify the data elements used in\\nconceptual modeling.\\nClass diagrams represent data elements, attributes, methods,\\nrelationships, inheritance and cardinalities as shown in Fig.2.\\nUML stereotypes can be used to add additional information\\nduring logical and physical modeling, such as embedding,\\nreferences and other MongoDB speciﬁc features [2].\\nComposition and aggregation annotations in class diagrams\\nfurther give an indication of the nature of relationships, leading\\nto either referencing (entities are standalone) or embedding\\n(entities are not independent) [14].\\nUML was found to provide better support and be more\\ncomprehensive than ER (entity-relationship) diagrams at data\\nmodeling by De Lucia et al. [15]. Since UML could be used\\nto model software classes, documentation could be shared\\nbetween application and data models.\\nB. Conceptual Modeling\\nThe ER design approach starts with a need or problem fol-\\nlowed by a requirements analysis [16]. From the requirements\\nanalysis actors, interactions or activities and some attributes\\nshould be known [10]. From this a use case diagram can be\\ndrafted [17].\\nWhen requirements are satisfactorily determined, the con-\\nceptual model can be designed. This model still does not\\ncontain database speciﬁc annotations, and could be equivalent\\nto a relational database conceptual design. Conceptual designs\\ncontain high-level data entities, attributes, relationships and\\ncardinalities [10], [18]. Even during this phase the conceptual\\ndesign should have foresight and consideration of the full life-\\ncycle, and proper requirements elicitation will signiﬁcantly\\ncontribute to this cause [19].\\nThe following relationship cardinalities are possible [10]:\\n• One-to-One;\\n• One-to-Many;\\n• Many-to-Many.\\nCardinalities could also be speciﬁc, for example, a department\\nmay have between ﬁve and ﬁfteen employees [18].\\nC. MongoDB Logical Modeling\\nMongoDB is a document-oriented database and data objects\\nhave the familiar JSON (JavaScript Object Notation) format,\\nthough stored in a more efﬁcient binary format. Documents\\nare contained within collections which in turn belong to a\\ndatabase [20].\\nUnlike typical relational databases, MongoDB’s data struc-\\nture is not structured as rows and columns, but can have\\nhierarchy - arrays and sub-documents - and often closely\\nresembles data structures used in an application. There are\\ntherefore many ways to correctly model data and relationships,\\nbut performance characteristics could vary [20].\\nTraditionally, during logical modeling, the entity-relational\\nconceptual model is decomposed into 3NF as tables and rela-\\ntionships [1], [10]. Since MongoDB does not typically follow\\na normalization design paradigm, choices about relationships\\nboil down to deciding on:\\n• Referencing (normalization);\\n• Embedding (denormalization).\\nEven within these two options many variations exist [21].\\nSome of these options are discussed in the following sections,\\nwith some advanced patterns to consider during physical\\nmodeling.\\n1) Referencing: Referencing is the MongoDB equivalent\\nto primary/foreign key relationships in relational databases [9].\\nA collection can be referenced by including a ﬁeld referencing\\na unique attribute of a document in another collection. A\\nspecial case of this is referencing another document in the\\nsame collection, called self-joining [20]. MongoDB does not\\nhowever automatically ensure referential integrity and relies\\non application level functionality to achieve that [22].\\nOne factor that is always a certain scenario for referencing\\nis whether the relationship has a high arity. In other words,\\na distinction has to be made between many and few [22]. In\\nthe case of few, embedding could be considered, otherwise\\na single document would grow large (maximum of 16 MiB),\\nrequiring more RAM and cause slower data transfers [20].\\nHigh-arity many-to-many relationships should typically be\\nmodeled with a referenced join-collection, much like in a\\nrelational table. Again, if either side was few, embedding could\\nbe considered [20].\\nAdvantages of referencing include [20]:\\n• No redundancy;\\n• Immediate consistency;\\n• Supports high-arity relationships;\\n• Flexibility in queries and indexing (physical model);\\n• Expanded sharding options.\\nDisadvantages of referencing are [20]:\\n• Application level joins;\\n• Depending on query patterns, limited scalability;\\n2) Embedding: Embedding, here, refers to the process of\\nadding a sub-document or ﬁelds of a conceptual entity wholly\\nand directly into a related document or documents, instead of\\ncreating a separate collection and referencing only the primary\\nidentiﬁer of related documents [9].\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='Embedding has the following advantages [20]:\\n• Application side joins aren’t needed;\\n• Inherent atomic and isolated operations for one-to-few\\nrelationships exist;\\n• Data locality is utilized;\\n• Inheritance is efﬁciently modeled;\\n• Faster query performance is available.\\nDisadvantages of embedding data include [20]:\\n• Redundant copies are present;\\n• Eventual consistency is achieved;\\n• Application level cascading is required for consistency;\\n• Poor performance is offered for high-arity relationships;\\n• Less ﬂexibility is available as collections are pre-joined.\\n3) Summary: Table I summarizes UML class diagram\\nconcepts relating to MongoDB’s document model.\\nTABLE I\\nUML CLASSES TO MONGO DB ANALOGY\\nUML Concept MongoDB Concept\\nClass Collection\\nObject Document\\nAttribute Field\\nAssociation Embed or reference\\nAggregation Usually reference\\nComposition Usually embed\\nInheritance Usually embed\\nD. MongoDB Physical Modeling\\nDuring physical modeling, the database designer consid-\\ners access-path-independent and access-path-dependent design\\nchoices. This includes application access patterns, indexing\\nschemes and sharding keys [1], [2], [11]. MongoDB physical\\nmodeling consists of many factors to consider [9]:\\n• _id choice;\\n• Schema validation;\\n• Indexing;\\n• Sharding;\\n• Replication;\\n• Consistency level choices;\\n• Application access patterns;\\n• MongoDB advanced features.\\nMany of these factors will also have a bearing on the underly-\\ning logical modeling choices, especially indexing and sharding\\nkey choices [9].\\nAll documents in MongoDB must have an _id ﬁeld that\\nis unique. In fact all collections include a unique index on\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='Content Missing\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='Content Missing\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='TABLE II\\nCONCEPTUAL DESIGN OPTIONS\\nConcept Referencing/Embedding Design Pros Cons\\nLibrary\\nReference (collection) Shardable Joins may be required\\nEmbed within Book Data locality Eventual consistency; difﬁcult to query\\nPartially embed within Book Shardable; data locality Eventual consistency\\nBook\\nReference (collection) Shardable Joins required\\nEmbed within Library Data locality; single read operations Boundless; complex queries;\\nPartially embed within Library Additional feature options Additional complexity\\nBook\\nCopies\\nReference (collection) Shardable Disjoint from Books\\nEmbed within Library Data locality Boundless\\nEmbed within Book Data locality; single view on checkout status Larger working set\\nCheckout\\nReference (collection) Shardable Transactions required\\nEmbed within Reader Single view; quick access Transactions required; possibly boundless\\nEmbed within Reader with bucket pattern Single view; quick access; not bounded Transactions required; outlier pattern\\nEmbed within Book or Book Copies No transactions No history\\nPartially embed within Book Copies No transactions History; eventual consistency\\nReview\\nReference (collection) Shardable; single full-text index Joins may be required\\nEmbed within Checkout Single review per checkout Complex queries to group by book/reader\\nEmbed within Book Data locality Boundless\\nRatings\\nReference (collection) Shardable; single full-text index Joins may be required\\nEmbed within Checkout Single rating per checkout Complex queries to group by book/reader\\nEmbed within Book as counts Data locality; pre-calculation options Calculation required per read\\n<<collection>>\\nReader\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tname:\\tstring\\n+\\tcheckouts:\\t[<<embedded>>]\\n*\\n<<collection>>\\nBook\\n+\\tisbn:\\tstring\\t<<PK>>\\n+\\ttitle:\\tstring\\n+\\tauthor:\\t[string]\\n+\\tdescription:\\tstring\\n+\\tlibbooks:\\t[<<embedded>>]\\n+\\tratingtotal:\\tnumber\\n+\\tratingcount:\\tnumber\\n<<collection>>\\nReview\\n+\\tid:\\tstring\\t<<PK>>\\n+\\treader:\\t<<FK>>\\n+\\tlibbook:\\t<<partial\\tembed>>\\n+\\treview:\\tstring\\n+\\treviewdate:\\tdate\\n<<embedded>>\\nCheckout\\n+\\tlibbook:\\t<<partial\\tembed>>\\n+\\tcheckout:\\tdate\\n+\\tcheckin:\\tdate\\t<<nullable>>\\n*\\n* 1\\n<<collection>>\\nLibrary\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tname:\\tstring\\n+\\taddress:\\tstring\\n<<embedded>>\\nLibraryBook\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tbook:\\t<<FK>>\\n+\\tlibrary:\\t<<embedded>>\\n+\\tcheckout:\\tdate\\t<<nullable>>\\n1\\n*\\n1\\n1\\n1\\n1\\n*\\n1\\nFig. 4. Library logical model\\nﬁlter with the same constraints. The resulting list of documents\\nis then grouped and summed by reader. The result is sorted\\nand limited to 10 - the top ten readers of the month.\\nCounting the number of books rented requires an index on\\nthe embedded checkouts’ checkout date. Using the aggregation\\npipeline to match only readers that have checkouts in that\\nmonth. Unwinding and grouping by library, while summing\\neach checkout will deliver the number of rented books per\\nlibrary.\\nAverage ratings can be calculated on demand by taking\\nthe total summed ratings, divided by the count - the ﬁelds\\nwere already included in the logical model for Book. In order\\nto optimize this, an additional ﬁeld should be added to pre-\\ncalculate the monthly ratings. This can be scheduled daily for\\nexample.\\nConsidering the writes to the database, only checkouts are\\ncomplicated, since a library may have more than one copy of\\na book and there are more than one library. The LibraryBook\\nembedded array allows for a single book document to keep\\ntrack of each copy along with an optional checkout date ﬁeld.\\nIf this is set to null the book is available. During checkout\\nthis is set to the checkout date and when checked in it gets set\\nback to null. Similarly in order to keep track of each reader’s\\ncheckouts and check-ins, a transaction is done to modify the\\nLibraryBook element and modify the reader checkout array.\\nFor this project strong consistency can be employed for all\\ntransactions since the few embedded ﬁelds, such as library\\naddress and book ISBN rarely change.\\nIt is unlikely that sharding would be required for such a\\nproject, since reads and writes occur rarely and in person.\\nIf needed, though, Book and and Reader can be sharded on\\nprimary key.\\nIf it had been required that data per library be isolated a\\nmore normalized approach would have had to be followed\\nin order to make the shard key per library. This would\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='render checkouts, and library books as their own collection.\\nShard tags could then be employed to isolate speciﬁc library\\ninformation to speciﬁc database servers.\\nIn order to keep working sets small it could become\\nnecessary to move checkouts to a separate collection, which\\nwould require embedding reader information in the checkouts\\nand cascading updates to user details in order for aggregations\\nto group by reader.\\nReading from replicas could also improve read scalability,\\nwithout compromising on consistency for critical operations.\\nA. Discussion\\nAll design options given in Table II, though not inclusive\\nof every permutation, can validly be modeled in logical an\\nphysical form while still meeting requirements. However, each\\noption has an direct impact on the complexity and performance\\nof the application and database design. There is no golden\\nrule, such as the 3NF in relational databases, to aid NoSQL\\ndatabase designers.\\nSince many design choices rely on application logic, it can\\nbe said that the application and database have to be designed\\ntogether.\\nConsider, for example, that a reader may only rate a book\\nonce. A requirement such as that dramatically impacts design\\nchoices for the application and database. The current rating\\nsystem would not be able to support such a requirement.\\nThe complexity of the above use-case could be exponen-\\ntially increased with the addition of a new concept such as\\naccount management. The use-case is not intended to serve\\nas a complete and all-encompassing solution, but instead offer\\nan overview of the complexities involved with NoSQL data\\nmodeling due to its inherent ﬂexibility.\\nFurthermore, speciﬁc deployment requirements have not\\nbeen speciﬁed and also have a bearing on underlying design\\nchoices, adding an additional layer of complexity.\\nV. C ONCLUSION\\nFrom the example use case it can be seen that MongoDB\\nmakes it possible to implement use cases in a variety of ways.\\nHow to design a database depends on use case requirements,\\nwhich must be properly elicited in order to avoid large changes\\nlater on. The processes and factors weighing in on the design\\nchoices have been discussed in the previous sections and\\nprovide a guide to concepts and tradeoffs required during the\\ndesign phase.\\nThe complexity of designs are affected by the complexity\\nof requirements. Complex designs will inevitably cost more to\\ndevelop and maintain, though complexity handled during the\\ndesign phases may result in simpler and more robust solutions\\nin the future.\\nForm-follows-function [24] - this common term has the\\nimplication for database and application design, that function\\ndictates underlying form, structure and design choices. It is the\\nconjecture of the authors that the traditional entity-relational\\napproaches suffer from lack of foresight and understanding of\\nthe whole system and its functions and resource requirements.\\nIt is instead based on functions-follow-form where entities\\nwhich have data-value are modeled very directly as acted upon\\nby users. Rarely, application access patterns and underlying\\nresources are discussed and weighed in on design choices -\\nespecially when considering full life-cycle requirements.\\nMore speciﬁcally, future growth is often not accounted for\\nas it is usually addressed in the physical model as a means of\\noptimizing queries as dictated by the structure of the logical\\ndesign. As shown in the use-case example the physical model\\ndesign is greatly impacted by and reliant on the prior designed\\nlogical model. Furthermore, changes required in the physical\\nmodel requires changes on the underlying logical model - they\\nare clearly interdependent.\\nThis paper therefore submits that physical and logical\\nmodeling phases in NoSQL, and more speciﬁcally MongoDB,\\nshould coincide as a single detail design phase. This is due to\\nthe fact that choices on either side are dependent on the other.\\nWhen considered as a unit, it may lead to shorter design phases\\nand less back-to-the-drawing-board situations.\\nIn large projects, the requirement for an interactive design\\neffort (physical and logical) implies good communication and\\ncoordination between design teams, should such teams be\\nused.\\nFurthermore, requirements elicitation should provide insight\\ninto potential growth and outlier query patterns and require-\\nments, which often impact the general case. The requirements\\ndeﬁne the functions and constraints, which in turn dictates the\\nform that the application and database (the resources) should\\ntake on. It is an outside-in approach taking into consideration\\nthe application design and user access patterns chieﬂy.\\nThis is in contrast with applications of Chen’s methods,\\nwhich start off with data-yielding entities, leading to concepts,\\nlogical forms and ultimately a physical database. It is typically\\nonly after this where application access patterns are typically\\nﬁtted to pre-existing underlying data structures. This is an\\ninside-out data-oriented approach.\\nEven though the example use case was imagined and\\nartiﬁcial, it has conveyed the type of design choices and\\nmethods that should be followed for MongoDB applications.\\nFurther research is needed to formally design a database\\nand application framework for modeling. It should address\\nthe following problems and considerations:\\n• User activity workﬂows;\\n• Impact of requirements;\\n• Maintenance workﬂow impacts;\\n• Design and maintenance costs;\\n• Deployment impacts.\\nIn general, NoSQL modeling techniques are under-\\nresearched and due to their ﬂexible nature, may bring forth\\nnew design approaches previously impossible or too complex\\nto attempt with only rows and columns.\\nFinally, with cloud-based applications usually being the\\ntarget for NoSQL development, the impacts of its constraints\\nand features on design choices should be further investigated.\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='VI. A UTHORS AND AFFILIATIONS\\nPieter Jordaan (MEng) is currently pursuing his PhD in\\nEngineering at the North-West University in South Africa.\\nHe is a student member of IEEE. His research ﬁeld includes\\ntopics on cloud applications, NoSQL database development\\nand scalability.\\nJohann Holm (PhD, PrEng) is an associate professor\\nat the School for Electrical, Electronic and Computer\\nEngineering of the North-West University in South Africa.\\nHe is a member of IEEE and a member of INCOSE, and\\nis actively involved in research and development, as well as\\nconsulting in operations and product development.\\nREFERENCES\\n[1] P. P.-S. Chen, “The entity-relationship model—toward a uniﬁed view of\\ndata,” ACM Transactions on Database Systems, vol. 1, no. 1, pp. 9–36,\\n3 1976.\\n[2] K. Shin, C. Hwang, and H. Jung, “NoSQL Database Design Using\\nUML Conceptual Data Model Based on Peter Chens Framework,”\\nInternational Journal of Applied Engineering Research, vol. 12, no. 5,\\npp. 632–636, 2017.\\n[3] C. de Lima and R. dos Santos Mello, “A workload-driven logical design\\napproach for NoSQL document databases,” in Proceedings of the 17th\\nInternational Conference on Information Integration and Web-based\\nApplications &Services - iiWAS ’15. New York, New York, USA:\\nACM Press, 2015, pp. 1–10.\\n[4] P. W. Jordaan and J. E. W. Holm, “Implementation and veriﬁcation of\\na cloud-based machine-to-machine data management system,” in 2013\\nAfricon. IEEE, 9 2013, pp. 1–5.\\n[5] K. Chodorow, Scaling MongoDB, 1st ed. Sebastopol: O’Reilly, 2011.\\n[6] A. Kanade, A. Gopal, and S. Kanade, “A study of normalization and\\nembedding in MongoDB,” Souvenir of the 2014 IEEE International\\nAdvance Computing Conference, IACC 2014, pp. 416–421, 2014.\\n[7] G. Zhao, W. Huang, S. Liang, and Y . Tang, “Modeling MongoDB\\nwith relational model,” Proceedings - 4th International Conference on\\nEmerging Intelligent Data and Web Technologies, EIDWT 2013, 2013.\\n[8] E. Horowitz, “MongoDB Drops ACID,” 2018. [Online]. Avail-\\nable: https://www.mongodb.com/blog/post/multi-document-transactions-\\nin-mongodb\\n[9] MongoDB, “The MongoDB 4.0 Manual,” 2019. [Online]. Available:\\nhttps://docs.mongodb.com/manual/\\n[10] C. Coronel, S. Morris, and P. Rob, Database Systems: Design, Im-\\nplementation, and Management, 11th ed. Stamford, CT: CENGAGE\\nLearning, 2009.\\n[11] F. Abdelhedi, A. A. Brahim, F. Atigui, and G. Zurﬂuh, “UMLtoNoSQL:\\nAutomatic transformation of conceptual schema to NoSQL databases,”\\nProceedings of IEEE/ACS International Conference on Computer Sys-\\ntems and Applications, AICCSA, vol. 2017-Octob, no. 1, pp. 272–279,\\n2018.\\n[12] P. Atzeni, F. Bugiotti, L. Cabibbo, and R. Torlone, “Data modeling in\\nthe NoSQL world,” Computer Standards & Interfaces, no. October, pp.\\n0–1, 10 2016.\\n[13] M. Yusufu, H. J. Zhang, G. Yusufu, Z. D. Liu, P. Cheng, and D. Dilisati,\\n“Modeling and Analysis of Complex System with UML: A Case Study,”\\nApplied Mechanics and Materials, vol. 513-517, pp. 1346–1351, 2014.\\n[14] Object Management Group, “OMG Uniﬁed Modeling Language\\nTM (OMG UML),” p. 794, 2015. [Online]. Available:\\nhttp://www.omg.org/spec/UML/2.5/\\n[15] A. De Lucia, C. Gravino, R. Oliveto, and G. Tortora, “An experimental\\ncomparison of ER and UML class diagrams for data modelling,”\\nEmpirical Software Engineering, vol. 15, no. 5, pp. 455–492, 2010.\\n[16] C. Fahrner and G. V ossen, “A survey of database design transforma-\\ntions based on the Entity-Relationship model,” Data and Knowledge\\nEngineering, vol. 15, no. 3, pp. 213–250, 1995.\\n[17] C. Churcher, Beginning database design. New York: Apress, 2012.\\n[18] P. Ponniah, Database design and development: an essential guide for\\nIT professionals. John Wiley & Sons, Inc., 2003.\\n[19] H. Sammaneh, “Requirements Elicitation with the Existence of Similar\\nApplications: A Conceptual Framework,” 2018 International Conference\\non Computer and Applications, ICCA 2018, pp. 444–449, 2018.\\n[20] R. Copeland, MongoDB Applied Design Patterns, 1st ed. Sebastopol:\\nO’Reilly, 2013.\\n[21] D. Coupal and K. W. Alger, “Building with Patterns: A Summary,” 2019.\\n[Online]. Available: https://www.mongodb.com/blog/post/building-with-\\npatterns-a-summary\\n[22] K. Chodorow and M. Dirolf, MongoDB: the deﬁnitive guide, 2nd ed.\\nSebastopol: O’Reilly, 2013.\\n[23] D. Coupal and K. W. Alger, “Building With Patterns: The Outlier\\nPattern,” 2019.\\n[24] M. H. Wen and V . O. Li, “Form Follows Function: Designing Smart Grid\\nCommunication Systems Using a Framework Approach,” IEEE Power\\nand Energy Magazine, vol. 12, no. 3, pp. 37–43, 2014.\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Abstract – Amazon DynamoDB is a next-gen NoSQL (Not \\nonly SQL), key-value and document database that delivers \\nsingle-digit millisecond performance at any scale. As more \\ncomplex web-based applications adopt DynamoDB, it is \\nimperative to develop sound design strategies for DynamoDB \\nschemas. Large-scale web-based applications often exhibit \\nconflicting requirements. Hence, the research problem is to \\ndesign the DynamoDB schemas of a given application with \\nconflicting requirements such that the database must satisfy \\ncertain predetermined performance goals. In the era of \\nrelational databases, normal forms were developed to guide \\nthe schema design process. Past knowledge and experiences, \\nwe believe, are also applicable to DynamoDB schema design. \\nSpecifically, based upon Nested Normal Form and XML \\ndatabases, this paper demonstrates the feasibility of a design \\nstrategy on DynamoDB schemas, particularly with the access \\npatterns of the data in mind. Simulation further substantiates \\nthe feasibility of our approach. \\n \\nKeywords – Amazon DynamoDB Schema Design, Nested \\nNormal Form, XML Databases \\n \\nI.  INTRODUCTION \\n \\n As large-scale web-based applications demand speed, \\nflexibility and scalability, NoSQL database systems, a \\nnext-gen database systems, are being developed. Many \\ncommercial NoSQL database systems are now available. \\nNotable examples include MongoDB [10], Cassandra [5], \\nRedis [12] and Amazon DynamoDB [1] and others. The \\nunderlying data models of NoSQL databases are quite \\ndifferent from the tabular data model of the traditional \\nrelational databases. Some prevalent data models for \\nNoSQL databases are the document model, the graph \\nmodel and the key-value model [11]. Amazon DynamoDB \\nadopts the key-value model and the document model, and \\nmany top-level entities of a database are modeled as nodes \\nand their relationships as edges as in graphs [1]. \\n The results of system analysis, an indispensable \\nactivity of any large-scale system development, include \\nmany different types of diagrams focusing on various \\naspects of the system under development. UML diagrams \\n[4], promoted by robust modeling tools like Visual \\nParadigm [13] and others, have become a software industry \\nstandard. The proposed DynamoDB schema design \\nstrategy also begins with a graphical diagram, called a \\nconceptual-model hypergraph, which is in a way similar to \\nbut simpler than UML domain model class diagrams.  The \\npurpose of such a hypergraph, like a UML domain model \\n`class diagram, is to capture the static, rather than the \\ndynamic, aspects of a system of interest. \\n Nested Normal Form [9] was originally designed for \\nnested relational databases, which allow tables to be nested \\nwithin other tables, resulting in embedded tables and thus \\nhierarchical structures. Since XML databases [3] also \\nallow hierarchical structures, it is natural to extend Nested \\nNormal Form to XML databases. A critical contribution of \\nNested Normal Form is that XML databases that conform \\nto Nested Normal Form are guaranteed to be free of \\nunwanted redundant data. In [8], we devised algorithms \\nthat generate Nested Normal Form XML databases. We \\nshall shortly show that the algorithms in [8], with some \\nmodifications, are also applicable to DynamoDB as well. \\n We believe conceptual-model hypergraphs and the \\nXML database design algorithms form the conceptual \\nframework in which the solution for the research problem \\nstated in the abstract can be formulated. In contrast, in one \\nsingle page the documentation of Amazon DynamoDB \\nstates some NoSQL design principles for DynamoDB \\nschemas. Although the suggested design principles are \\nsound, they are so vague to formulate a formal DynamoDB \\ndesign strategy. \\n To convey the central idea of the proposed design \\nstrategy, the rigorous mathematics underlying [7,8,9] is \\navoided. Instead, this paper adopts a high-level, step-by-\\nstep presentation. To proceed, Section 2 concisely presents \\nthe methodology and shows that such an approach is \\nfeasible and desirable. Section 3 presents the results of our \\nexperiments, which were conducted on the AWS (Amazon \\nWeb Services) Educate platform  [2]. Section 4 discusses \\nmany-to-many relationships and we shall conclude and \\nstate the future research goals of this project in Section 5. \\n \\nII.  METHODOLOGY \\n \\n In a nutshell, the proposed design strategy is \\nsummarized as follows: \\n• Capture an application of interest with a \\nconceptual-model hypergraph [8], or some other \\nmeans that can be translated to a conceptual-\\nmodel hypergraph. \\n• Identify the access patterns of the most commonly \\nexecuted queries and operations in the \\nconceptual-model hypergraph.  \\n• Simplify the conceptual-model hypergraph by \\nidentifying object sets that are in one-to-one \\ncorrespondence with each other. \\n• Generate a set of hierarchical schemas from the \\naccess patterns based on Nested Normal Form [9]. \\nA Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal \\nForm Approach \\n \\nWai Yin Mok \\nDepartment of Information Systems, The University of Alabama in Huntsville, Huntsville, USA \\n (mokw@uah.edu) \\n903978-1-5386-7220-4/20/$31.00 ©2020 IEEE\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='• Load the data of the application to Amazon \\nDynamoDB in accordance to the generated \\nhierarchical schemas. \\n \\nCrnSecID\\nSection Pair\\nStudent SSN\\nSemester\\nGrade\\nCourse−\\n \\n \\nFig. 1: A sample conceptual-model hypergraph. \\n \\nA. Conceptual-Model Hypergraphs \\n  \\n A highly abstracted conceptual-model hypergraph for \\na simple reporting system of a small college is shown in \\nFig. 1. To show the essential features of the proposed \\ndesign strategy, many details of the hypergraph are \\nomitted. For example, the name, address, picture, and other \\npersonal items of a student are nowhere to be found in the \\nhypergraph. Here, we identify the salient features of the \\nhypergraph, although similar explanations can also be \\nfound in [8]. \\n The hypergraph of Fig. 1 has six sets of objects, which \\nare called Student, SSN (Social Security Number), \\nSemester, Course-Section Pair, CrnSecID, and Grade. The \\nsets Student, SSN, Semester, and Grade are all self-\\nexplanatory. The set Course-Section Pair contains the pairs \\nof every course and all the sections of that course. As an \\nexample, an element of that set might be IS301-01, which \\nrepresents Section 1 of the course IS301: Introduction of \\nInformation Systems. We use a six characters string in this \\npaper to represent a course-section pair and CrnSecID is \\nthe set of all such strings. \\n There are three relationship sets among the sets of \\nobjects in Fig. 1. The first relationship set, represented as a \\nline, is between the sets Student and SSN. It is called a \\nrelationship set because it is a set of relationships. As an \\nexample, relationships, or elements, of that set might be \\n(student\\n1, 111223333) and (student 2, 222334444), \\nrepresenting student1 is related to the SSN 111223333 and \\nstudent2 to 222334444. The line between Student and SSN \\nhas arrow heads at both ends, which represents a one-to-\\none correspondence between the sets Student and SSN. In \\nour example, student\\n1 is thus paired with 111223333 and \\nso is student2 with 222334444. The second relationship set \\nis between the sets Course-Section Pair and CrnSecID, \\nwhich is also represented as a line. Because both ends of \\nthat line have arrow heads, each course-section pair is \\npaired with exactly one CrnSecID. The third relationship \\nset is among the sets Student, Semester, Grade, and \\nCourse-Section Pair, represented as a diamond. There is a \\nsingle arrow head of this relationship set, pointing at the \\nobject set Grade. It represents the functional dependency \\n(FD) Student, Course-Section Pair, Semester → Grade, \\nwhich means that for each tuple of a Student element, a \\nCourse-Section Pair element and a Semester element, there \\ncan only be one Grade element. The circle close to the set \\nCourse-Section Pair means that a particular course-section \\npair might or might not relate to the elements of the other \\nthree sets. In other words, there are some existing course-\\nsection pairs are not used in the relationship set. \\n \\nB. Access Patterns of the most commonly executed Queries \\nand Operations \\n \\n Suppose that the current semester is Summer 2020 \\n(2020S). At the end of 2020S, every instructor will enter a \\ngrade for every student in his/her classes. The reporting \\nsystem thus needs to present a class list to each instructor \\nand the instructor will enter a grade for every SSN on the \\nclass list. Hence, there are two access patterns, which are \\nshortened as AP1 and AP2, and their involved object sets \\nof Fig. 1 are outlined in Fig. 2. \\n \\nAccess Pattern 2\\nSection Pair\\nStudent SSN\\nSemester\\nGrade\\nCrnSecID\\nAccess Pattern 1\\nCourse−\\n \\n \\nFig. 2: Access patterns of two commonly executed query and update \\noperation at the end of 2020S. \\n \\nAP1: The query that generates a class list of 2020S will \\nfirst select the semester 2020S and a particular CrnSecID. \\nThen a list of SSNs is generated. Thus, this query involves \\nall of the object sets in Fig. 1 except Grade.  \\nAP2: The update operation is to record the grade earned by \\na student for any class of 2020S for which the student \\nregistered. Thus, this operation involves every object set of \\nthe relationship set in Fig. 1. \\n \\nC. Simplification of Conceptual-model Hypergraphs  \\n \\n Because of the one-to-one correspondence between the \\nobject sets Student and SSN, each student can be replaced \\nby his/her SSN. The same is also true for the object sets \\nProceedings of the 2020 IEEE IEEM\\n904\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Course-Section Pair and CrnSecID. We thus obtain the \\nsimplified hypergraph in Fig. 3. \\n \\nAccess Pattern 2\\nSemester\\nGrade\\nAccess Pattern 1\\nSSN\\nCrnSecID\\n \\n \\nFig. 3: A simplified conceptual-model hypergraph. \\n \\nD. Generation of Hierarchical Schemas \\n \\n The algorithms in [8] can generate hierarchical \\nschemas from any conceptual-model hypergraph with any \\nnumber of relationship sets and each relationship set can \\ncontain any number of object sets. The hypergraph in Fig. \\n3 only has one single relationship set that contains four \\nobject sets. First, note that the single relationship set of Fig. \\n3 is in BCNF [6], meaning that it does not have data \\nredundancy with respect to the FD SSN, CrnSecID, \\nSemester → Grade. Based on Nested Normal Form, the \\nobject sets of the relationship set can be organized in any \\nsingle path hierarchical schema without any unwanted \\nredundant data. Two possible Nested Normal Form \\nhierarchical schemas are shown in Fig. 4. \\n The left hierarchical schema is derived from Access \\nPattern 1, denoting the fact that for each pair of CrnSecID \\nand Semester, there is a set of SSNs. Note that the object \\nset Grade is left out because Grade is not needed in the \\nquery that generates the class list of any class of 2020S. \\nThe right hierarchical schema is derived from Access \\nPattern 2. It means that for each pair of SSN and Semester, \\nthere is a list of pairs of the form CrnSecID and Grade. \\n  \\nIII.  RESULTS \\n \\nA.  Simulation Background \\n \\nThe experiments were conducted on AWS Educate, a \\nfree platform for cloud learning [2]. A Cloud9 \\nenvironment, which is a cloud IDE (Integrated \\nDevelopment Environment) for writing, running, and \\ndebugging code, was first created. We selected the default \\nsettings for the environment whose EC2 instance type is \\nt2.micro that has 1 virtual CPU, 1 GB of memory, and a \\nLinux OS. (Amazon Elastic Compute Cloud (EC2) is a web \\nservice that provides secure, resizable compute capacity in \\nthe cloud.) A real-world production environment will \\nclearly have a more substantial virtual computing \\nenvironment. \\n \\n \\n \\nFig. 4: Two hierarchical schemas generated from the access patterns in \\nFig. 3. \\n \\nAfter which, a Python program generated two \\nDynamoDB tables, called IEEM2020-CrnSec and \\nIEEM2020-Students where IEEM2020-CrnSec is to hold \\nthe data of the left hierarchical schema in Fig. 4 and \\nIEEM2020-Students for the right one. The read and write \\ncapacity of each table were set to 500 units, which costs \\nabout $290 monthly for each table. The partition key of \\nIEEM2020-CrnSec is set to Semester and the sort key to \\nCrnSecID. This is appropriate because the records of \\nIEEM2020-CrnSec ought to be partitioned according to \\nSemester, and within each semester the records are further \\nsorted according to their Cr nSecIDs. Each Semester and \\nCrnSecID pair in IEEM2020-CrnSec has a list of SSNs \\nwhere a DynamoDB list is similar to a Python list. For the \\ntable IEEM2020-Students, the partition key is SSN and the \\nsort key is Semester. This is also appropriate because \\nstudents are the focus of IEEM2020-Students and thus SSN \\npartitions the table. The records of a student are further \\nsorted according to Semester. Each SSN and Semester pair \\nin IEEM2020-Students is associated with a list of maps \\nwhere a DynamoDB map is similar to a Python dictionary. \\nNote that the root nodes of the two hierarchical \\nschemas each consists of two object sets. Although such \\nchoices are dictated in part by the semantics of the \\napplication, they are chosen also because primary keys of \\nDynamoDB tables can only contain two or less attributes. \\nNote that this is a severe limitation because the primary key \\nof the relationship set in Fig. 3 is in fact SSN, Semester, \\nCrnSecID and many real-world primary key may consist of \\nmore than two attributes. \\n \\nB.  Simulation Data \\n \\n We’ve randomly generated 1,000 through 10,000, in \\nthe increment of 1,000, active students to simulate the \\nworkload of the reporting system at the end of 2020S. Only \\nactive students have been simulated because only active \\nstudents have registered for classes in 2020S and only they \\nwill receive a grade for their classes of 2020S. Each active \\nstudent has enrolled in classes for a number x of semesters \\nwhere the integer x follows an exponential distribution with \\nthe scale set to 6. Hence, most active students have enrolled \\nin less than 5 semesters and at most 12 semesters. Each \\nactive student has registered a number x of courses in a \\nAccess Pattern 1\\nCrnSecID, Semester\\nSSN\\nSemesterSSN,\\nGradeCrnSecID,\\nAccess Pattern 2\\nProceedings of the 2020 IEEE IEEM\\n905\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='semester where the integer x follows a normal distribution \\nwith the mean and standard derivation respectively set to 4 \\nand 1. All of these data were written out to JSON \\n(JavaScript Object Notation) files, preparing to be loaded \\ninto the DynamoDB tables. \\n \\n[ \\n{ \\n\"SSN\": 100327004, \\n\"Semester\": \"2020S\", \\n\"CrnSecList\":{\"432754\": \" \",\"567017\": \" \",\"716933\": \" \",\"290347\": \" \"} \\n}, \\n{ \\n\"SSN\": 100327004, \\n\"Semester\": \"2020W\", \\n\"CrnSecList\":{\"317239\": \"D\",\"312013\": \"A\",\"772832\": \"A\",\"294905\": \"D\"} \\n}, \\n... \\n] \\nFig. 5: Some randomly generated active student records. \\nFig. 5 shows a part of the JSON file that contains some \\nrandomly generated student records about an active student \\nwhose SSN is 100327004, and the semesters for which \\nhe/she enrolled (2020S and 2020W), and the grade he/she \\nearned for each CrnSecID of which he/she registered. Note \\nthat the grades for 2020S are all blank. \\n \\n[ \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"116447\", \\n\"StdList\": [143267942, 167456489, 191522130, 203634078, 218949345, 228395488, \\n262223274, 299359064, 340291307, 383893398, 402528408, 429940466, 491781834, \\n503895266, 524376955, 555747721, 588971187, 622489021, 632925410, 683107756, \\n707819456, 736073179, 746724915, 753864030, 759208116, 763603644, 764238408, \\n775280136, 783255531, 808439142, 814791444, 827636210, 865237592, 921601262, \\n942888972, 969574148, 972276135] \\n}, \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"143078\", \\n\"StdList\": [111183936, 143267942, 156225049, 157319776, 163586751, 224053635, \\n233214797, 234530365, 262458151, 295119560, 321794227, 329578153, 331832654, \\n393320569, 406218919, 459185184, 503871987, 509132188, 523055023, 547079471, \\n587846611, 589394822, 614437773, 655053546, 659577543, 665339504, 665548465, \\n738868279, 741704917, 775280136, 816330964, 823199550, 877078492, 897820459, \\n923783761, 944155639, 977837091] \\n}, \\n... \\n] \\nFig. 6: Some randomly generated class lists.   \\n Fig. 6 shows a part of the JSON file that contains the \\nclass lists of certain pairs of CrnSecID and 2020S. Each \\nclass list is essentially a list of SSNs \\n \\nC.  Simulation Results. \\n The query that retrieves every class list of 2020S and \\nthe update operation that assigns a grade to every class of \\neach active student of 2020S are written as parts of a \\nPython program, shown in Fig. 7. The code segment was \\nrepeated 5 times, and the elapsed time was printed out at \\nthe end of the loop.  \\nimport timeit \\n \\ncode_to_test = \"\"\" \\nimport random \\nimport boto3 \\nfrom boto3.dynamodb.conditions import Key \\n \\ndynamodb = boto3.resource(\\'dynamodb\\', region_name=\\'us-east-1\\') \\ntableC = dynamodb.Table(\\'IEEM2020-CrnSec\\') \\ntableS = dynamodb.Table(\\'IEEM2020-Students\\') \\n \\nrespC = tableC.query(KeyConditionExpression=Key(\\'Semester\\').eq(\\'2020S\\')) \\n \\nfor item in respC[\\'Items\\']: \\n    for sid in item[\\'StdList\\']: \\n        respS = tableS.update_item( \\n            Key = {\\'SSN\\': sid, \\'Semester\\': \\'2020S\\'}, \\n            ExpressionAttributeNames={ \\n                \"#crnsec\": item[\\'CrnSec\\'], \\n            }, \\n            ExpressionAttributeValues={ \\n                \":grade\": random.choice([\\'A\\',\\'B\\',\\'C\\',\\'D\\',\\'E\\']), \\n            }, \\n            UpdateExpression=\"set CrnSecList.#crnsec = :grade\", \\n            ) \\n     \\n\"\"\" \\nelapsed_time = timeit.timeit(code_to_test, number=5) \\nprint(elapsed_time) \\nFig. 7: The Python program that retrieves every class list of 2020S and \\nupdates the grade of every class of every active student.  \\nSince the SSNs of every class list are randomly \\ngenerated, every student has an equal probability of being \\nupdated. Therefore, there are no localities of updates. Table \\n1 shows the average time the Python program took to \\nperform an update and Fig. 8 shows the information \\ngraphically. Note that DynamoDB took less than 0.0085 \\nsecond to perform an update for no more than 10,000 \\nstudents.\\n \\n  TABLE I \\nAVERAGE TIME OF AN UPDATE \\n \\nNum of \\nstudents \\nNum of \\ncourse \\nsection \\npairs \\nNum of \\niterations \\nNum of \\nupdates \\nElapsed \\ntime \\nAverage \\ntime of \\neach \\nupdate  \\n1000 100 5 3975 140.821 0.00709 \\n2000 200 5 7924 298.602 0.00754 \\n3000 300 5 12027 448.486 0.00746 \\n4000 400 5 16109 547.187 0.00679 \\n5000 500 5 20015 746.519 0.00746 \\n6000 600 5 24031 885.607 0.00737 \\n7000 700 5 28020 1116.735 0.00797 \\n8000 800 5 31924 1293.276 0.00810 \\n9000 900 5 35851 1353.335 0.00755 \\n10000 1000 5 40207 1470.509 0.00731 \\n \\n \\n \\nFig. 8: Plotting average time of an update against number of updates.  \\n \\n To perform a stress test on the update operations on the \\ndatabase, we’ve randomly generated a long text string to \\nstore with each SSN and Semester pair in the table \\nIEEM2020-Students, where the length of every text string \\nfollows a normal distribution with the mean and standard \\nderivation set to 4,000 and 100 bytes respectively. We’ve \\nalso inserted a long text string for every pair of Semester \\nand CrnSecID in the table IEEM2020-CrnSec. The purpose \\nof these long text stings in both tables are to ensure that \\nthere are great physical distances among the updates. The \\nresults of the stress test are shown in Table 2. Note that \\neven though long text strings have been inserted in both \\ntables, the average time for each update in IEEM2020-\\nStudents is not much different than the case that has no long \\ninserted text strings. \\n \\n \\n \\nProceedings of the 2020 IEEE IEEM\\n906\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='TABLE II \\nAVERAGE TIME OF AN UPDATE WITH LONG TEXT STRINGS \\nINSERTED. \\n \\nNum of \\nstudents \\nNum of \\ncourse \\nsection \\npairs \\nNum of \\niterations \\nNum of \\nupdates \\nElapsed \\ntime \\nAverage \\ntime of \\neach \\nupdate  \\n1000 100 5 3992 140.662 0.00705 \\n2000 200 5 7976 300.682 0.00754 \\n3000 300 5 11997 470.264 0.00784 \\n4000 400 5 15956 567.652 0.00712 \\n5000 500 5 19935 732.344 0.00735 \\n \\nIV.  DISCUSSION \\n \\n Astute readers might discover that the relationship \\nbetween a SSN, a Semester and a CrnSecID is being stored \\nin two distinct places, once in the table IEEM2020-CrnSec \\nand once in IEEM2020-Students, although the same \\nrelationship is being stored with different focuses in the \\ntwo tables. The Developer Guide for Amazon DynamoDB \\nin fact recommends using adjacency list to model many-to-\\nmany relationships, where we can see that a many-to-many \\nrelationship is also being stored in two distinct places in the \\nadjacency list. A basic tenet of the theory of relational \\ndatabases is that redundant data always lead to high update \\ncost. However, many practitioners of NoSQL databases \\noftentimes champion denormalization in order to improve \\nperformance. This, of course, deserves more study. \\n \\nV.  CONCLUSION \\n \\n In this paper, we have presented evidence for the \\nfeasibility of a DynamoDB schema design strategy, which \\nis based on Nested Normal Form and past experiences on \\nXML databases. Such a design strategy begins with a \\nconceptual-model hypergraph, which is in a way similar to \\nUML domain class diagrams. It then generates hierarchical \\nschemas in Nested Normal Form that are guaranteed to be \\nfree of unwanted redundant data. The generated \\nhierarchical schemas are then mapped to DynamoDB \\ntables for implementation. \\n We have performed experiments to further show the \\nusefulness of the proposed design strategy. We have \\nrandomly generated 1,000 through 10,000 active students, \\nin the increment of 1,000, and each active student has \\nenrolled in a number of semesters that follows an \\nexponential distribution. In addition, each active student \\nhas registered for a number of courses that follows a \\nnormal distribution in each semester. The results show that \\non average each update has taken less than 0.0085 second. \\nWe’ve also subjected the database to a stress test where a \\nlong text string was inserted into a student record and a \\ncourse-section record. The purpose of which is to ensure \\nthat updates are not done in close localities. It turns out that \\nthe average update time is about the same with or without \\nthe inserted long text strings. \\n Much more work remains to be done. Currently we \\nconstruct a hierarchical schema for each assess pattern, \\nwhich may lead to redundancy. However, we shall study \\nthe update cost for making such a choice. As more results \\nare being developed, a future journal paper shall report the \\nfiner details of the proposed schema design strategy.  \\n \\nREFERENCES \\n \\n[1] “Amazon DynamoDB: Fast and flexible NoSQL database \\nservice for any scale,” 2020. Accessed on: May 30, 2020. \\n[Online].Available: https://aws.amazon.com/dynamodb/ \\n[2] “AWS Educate: Your Cloud Journey Starts Here,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://aws.amazon.com/education/awseducate/ \\n[3] “BaseX: The XML Framework,” 2020. Accessed on: May 30, \\n2020. [Online]. Available: http://basex.org/ \\n[4] G. Booch, J. Rumbaugh and, I. Jacobson, The Unified \\nModeling Language User Guide . Addison Wesley object \\ntechnology series, Addison-Wesley, 2005 \\n[5] “Cassandra: Manage massive amounts of data, fast, without \\nlosing sleep,” 2020. Accessed on: May 30, 2020. [Online]. \\nAvailable: https://cassandra.apache.org/\\n \\n[6] D. Maier, The Theory of Relational Databases . Computer \\nScience Press, 1983. \\n[7] W.Y. Mok, and D.W. Embley, “Generating Compact \\nRedundancy-Free XML Documents from Conceptual-Model \\nHypergraphs,” IEEE Trans. Knowl. Data Eng. , vol. 18, no. \\n8, pp. 1082–1096, 2006. \\n[8] W.Y. Mok, J. Fong, and D.W. Embley, “Generating the \\nfewest redundancy-free scheme trees from acyclic \\nconceptual-model hypergraphs in polynomial time,” Inf. \\nSyst., vol. 41, pp. 20-44, 2014. \\n[9] W.Y. Mok, Y.K. Ng, and D.W. Embley, “A Normal Form \\nfor Precisely Characterizing Redundancy in Nested \\nRelations,” ACM Trans. Database Syst. , vol. 21, no. 1, pp. \\n77-106, 1996. \\n[10] “MongoDB: The database for modern applications,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.mongodb.com/\\n \\n[11] “NoSQL Databases.” 2020. A ccessed on: May 30, 2020. \\n[Online]. Available: https://www.trustradius.com/nosql-\\ndatabases \\n[12] “Redis: Experience the fastest NoSQL database in the \\ncloud.” 2020. Accessed on: May 30, 2020. [Online]. \\nAvailable: https://redislabs.com/ \\n[13] “Visual Paradigm: Stay Competitive and Responsive to \\nChange Faster & Better in the Digital World,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.visual-paradigm.com/\\n \\nProceedings of the 2020 IEEE IEEM\\n907\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n54 | P a g e  \\n \\nSQL vs. NoSQL: Choosing the Right Database for Your E-\\ncommerce Platform \\n1Lakshmi Nivas Nalla, 2Vijay Mallik Reddy \\n1Data Engineer Lead, Florida International University,11200 SW 8th St, Miami, FL 33199, \\nEmail:nallanivas@gmail.com \\n2Member of Technical Staff, University of North Carolina at Charlotte, Email: \\nvijaymr1012@gmail.com \\nAbstract: Selecting the appropriate database technology is a critical decision for e-commerce \\nplatforms aiming to scale effectively and accommodate the ever -increasing volume and \\ncomplexity of data. This paper provides an in -depth comparison of SQL (Structured Query \\nLanguage) and NoSQL (Not Only SQL) databases, elucidating their respecti ve strengths, \\nweaknesses, and suitability for e -commerce applications. Through a comprehensive analysis of \\nfactors such as data structure, scalability, performance, consistency, and flexibility, we offer \\nguidance to help e-commerce businesses make informed decisions when choosing between SQL \\nand NoSQL databases. By understanding the distinctive features and trade -offs associated with \\neach database paradigm, businesses can optimize their database architecture to support seamless \\noperations, enhance user experiences, and drive sustainable growth in the competitive e-commerce \\nlandscape. \\nKeywords: SQL, NoSQL, Database Management Systems, E -commerce, Scalability, \\nPerformance, Data Modeling, Consistency, Flexibility, Decision-Making. \\nIntroduction \\nIn the contempora ry digital landscape, the proliferation of e -commerce platforms has \\nrevolutionized the way consumers interact with businesses, ushering in an era of unprecedented \\nconvenience, choice, and accessibility. At the heart of these dynamic digital ecosystems lies  the \\ndatabase, serving as the foundational infrastructure that underpins the storage, retrieval, and \\nmanagement of vast volumes of transactional data, user profiles, and product catalogs. As e -\\ncommerce platforms strive to meet the evolving needs and expectations of modern consumers, the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n55 | P a g e  \\n \\nselection of an appropriate database technology emerges as a critical determinant of scalability, \\nperformance, and operational efficiency. \\nThe dichotomy between SQL (Structured Query Language) and NoSQL (Not Only SQL) \\ndatabases represents a fundamental choice faced by e -commerce businesses when architecting \\ntheir database systems. SQL databases, characterized by their relational data model and adherence \\nto ACID (Atomicity, Consistency, Isolation, Durability) properties, have long been the cornerstone \\nof traditional database management systems (DBMS). Conversely, NoSQL databases embrace a \\nmore flexible, schema -less approach, catering to the diverse data structures and distributed \\narchitectures prevalent in modern e-commerce applications. \\nThe decision -making process surrounding the selection of SQL versus NoSQL databases is \\nmultifaceted, encompassing a myriad of technical, operational, and business considerations. From \\ndata modeling and scalability to performance optimization and  data consistency, each database \\nparadigm presents unique strengths and trade-offs that must be carefully evaluated in the context \\nof the specific requirements and constraints of the e-commerce platform. \\nThis paper embarks on a comprehensive exploration of the SQL versus NoSQL debate in the realm \\nof e-commerce database management, with a fervent commitment to scientific rigor, empirical \\nvalidation, and practical relevance. Through a synthesis of existing literature, empirical studies, \\nand real-world case examples, we endeavor to elucidate the distinctive features, advantages, and \\nlimitations associated with each database paradigm. By providing a nuanced understanding of the \\ntechnical nuances and strategic implications inherent in the SQL versus NoSQL decisio n, this \\npaper aims to empower e -commerce businesses to make informed choices that align with their \\noverarching goals and objectives. \\nMoreover, the conduction of data relevant to the topics at hand forms the cornerstone of this \\ninquiry. By drawing upon a di verse array of data sources, including benchmarking studies, \\nperformance evaluations, and case studies from industry practitioners, we seek to ground our \\nanalysis in empirical evidence and real -world insights. Through meticulous data collection, \\nvalidation, and analysis, we strive to offer actionable recommendations and best practices that \\nresonate with the evolving needs and challenges faced by e -commerce businesses in an \\nincreasingly competitive marketplace. Thus, the scientific values upheld in this stud y underscore'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n56 | P a g e  \\n \\nour commitment to advancing knowledge and fostering informed decision -making within the e -\\ncommerce domain. \\nLiterature Review \\nThe discourse surrounding the SQL versus NoSQL debate in the context of e-commerce database \\nmanagement has been a focal point of scholarly inquiry and practical deliberation in recent years. \\nThis section offers a comprehensive review of the literature, synthesizing key findings, \\ncomparisons, and trends elucidated by researchers and industry experts. \\nHistorical Evolution: The historical evolution of database management systems (DBMS) sets the \\nstage for understanding the divergent trajectories of SQL and NoSQL databases. SQL databases, \\nrooted in the relational model pioneered by Edgar F. Codd in the 1970s, have long been \\nsynonymous with structured data storage, declarative querying, and transactional integrity. In \\ncontrast, the emergence of NoSQL databases in the late 2000s marked a paradigm shift towards \\ndistributed, non -relational data stores optimized for scalability, flex ibility, and performance in \\nweb-scale applications (Brewer, 2012). \\nPerformance and Scalability:  One of the primary motivations driving the adoption of NoSQL \\ndatabases in e-commerce applications is their superior performance and scalability characteristics \\ncompared to traditional SQL databases. Research by Stonebraker et al. (2007) demonstrated the \\nlimitations of SQL databases in handling the high volume and velocity of data generated by e -\\ncommerce transactions, highlighting the need for alternative approaches to database management. \\nNoSQL databases, with their distributed architectures and horizontal scalability, offer compelling \\nsolutions to the scalability challenges inherent in e -commerce platforms, enabling seamless \\nhandling of massive datasets and concurrent user interactions (Kaufman et al., 2010). \\nData Modeling and Flexibility:  The relational data model enforced by SQL databases imposes \\nrigid schema structures that can be cumbersome to adapt in the dynamic, rapidly evolving context \\nof e-commerce applications (Bruns, 2018). NoSQL databases, by contrast, embrace a schema-less \\nor schema -flexible approach, allowing for agile data modeling and iteration in response to \\nchanging business requirements (Fowler, 2013). This flexibility is particularly advantageous in e-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content=\"International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n57 | P a g e  \\n \\ncommerce environments characterized by diverse product catalogs, evolving user preferences, and \\ncomplex transactional workflows (Cattell, 2010). \\nConsistency and Transactional Integrity: A recurring critique leveled against NoSQL databases \\npertains to their relaxed consistency models and eventual consistency guarantees, which deviate \\nfrom the strict ACID properties upheld by traditional SQL databases (Bailis et al., 2013). While \\nthis eventual consistency model may suffice for certain e -commerce use ca ses, such as \\nrecommendation engines and content delivery systems, it may fall short in scenarios requiring \\nstrong transactional guarantees, such as order processing and inventory management (Coulouris et \\nal., 2012). \\nReal-World Case Studies:  Several real -world case studies provide empirical evidence of the \\nefficacy and trade -offs associated with SQL and NoSQL databases in e -commerce applications. \\nFor instance, Amazon's transition from a monolithic SQL -based architecture to a distributed, \\nmicroservices-based architecture powered by NoSQL databases has been well -documented, \\nhighlighting the scalability and agility advantages afforded by NoSQL technologies in handling \\nthe company's massive scale and dynamic workload patterns (Vogels, 2009). \\nRecent Trends and Emerging Technologies: Recent years have witnessed the convergence of \\nSQL and NoSQL paradigms through the emergence of NewSQL databases, which aim to combine \\nthe scalability and flexibility of NoSQL with the transactional integrity of traditional SQL \\ndatabases (Stonebraker, 2010). These hybrid approaches, exemplified by technologies such as \\nGoogle Spanner and CockroachDB, represent promising avenues for reconciling the divergent \\nrequirements of e -commerce applications while preserving the robustness and con sistency \\nguarantees of SQL databases (Corbett et al., 2012). \\nConclusion: In conclusion, the literature review underscores the multifaceted nature of the SQL \\nversus NoSQL debate in e -commerce database management, encompassing considerations of \\nperformance, scalability, flexibility, consistency, and transactional integrity. While NoSQL \\ndatabases offer compelling solutions to the scalability challenges inherent in e -commerce \\nplatforms, they come with trade -offs in terms of consistency and transactional guarant ees. \\nConversely, SQL databases provide strong consistency guarantees but may struggle to scale \\neffectively in high -volume, distributed environments. As e -commerce platforms continue to\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n58 | P a g e  \\n \\nevolve and scale, the selection of an appropriate database technology r emains a pivotal decision, \\nnecessitating a nuanced understanding of the technical nuances and strategic implications \\nassociated with SQL and NoSQL paradigms. \\nLiterature Review \\nThe debate surrounding SQL versus NoSQL databases in e -commerce has been fueled by a \\nplethora of empirical studies and industry reports, each offering nuanced insights into the \\ncomparative advantages and limitations of these database paradigms. A seminal study by \\nStonebraker et al. (2007) compared the performance of SQL and NoSQL databases in handling e-\\ncommerce workloads, revealing that NoSQL databases outperformed their SQL counterparts in \\nterms of throughput and latency under high concurrency scenarios. These findings underscored the \\nscalability advantages of NoSQL databases in acco mmodating the unpredictable traffic patterns \\nand surges characteristic of e-commerce platforms. \\nConversely, research by Das et al. (2011) highlighted the trade-offs inherent in NoSQL databases, \\nparticularly in terms of consistency and transactional integrity. Through a series of benchmarking \\nexperiments, Das et al. demonstrated that NoSQL databases, while excelling in scalability and \\navailability, often sacrificed strong consistency guarantees, leading to potential data inconsistency \\nand integrity issues in  e-commerce applications. These findings underscored the importance of \\ncarefully evaluating the consistency requirements and trade-offs associated with different database \\ntechnologies in the context of specific e-commerce use cases. \\nMoreover, the advent of  cloud computing and distributed computing architectures has further \\ncomplicated the SQL versus NoSQL debate, introducing new considerations related to data \\nlocality, network latency, and cost -effectiveness. Research by Strauch et al. (2015) explored the \\nperformance implications of deploying SQL and NoSQL databases in cloud environments, \\nrevealing that while NoSQL databases offered inherent scalability benefits in distributed \\nenvironments, they incurred higher operational overhead and complexity compared to  SQL \\ndatabases. These findings underscored the need for e-commerce businesses to weigh the trade-offs \\nbetween scalability and operational simplicity when choosing between SQL and NoSQL databases \\nin cloud deployments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n59 | P a g e  \\n \\nFurthermore, the proliferation of real-time analytics and personalized customer experiences in e -\\ncommerce has spurred interest in database technologies capable of supporting complex event \\nprocessing and real-time data ingestion. Studies by Chen et al. (2018) investigated the suitability \\nof SQL and NoSQL databases for real-time e-commerce analytics, highlighting the challenges and \\nopportunities associated with processing high -velocity, high -volume data streams in real -time. \\nWhile NoSQL databases offered inherent advantages in handling unstructured and semi-structured \\ndata, SQL databases excelled in complex query processing and ad-hoc analytics, underscoring the \\ncomplementary roles of both database paradigms in supporting diverse e-commerce use cases. \\nIn summary, the literature review provides a comprehensive overview of the SQL versus NoSQL \\ndebate in e -commerce database management, encompassing considerations of performance, \\nscalability, consistency, and suitability for real -time analytics. While NoSQL databases offer \\ninherent advantages in scalability and flexibility, they come with trade-offs in terms of consistency \\nand operational complexity. Conversely, SQL databases provide strong consistency guarantees \\nand robust query capabilities but may struggle to scale effectively in distributed, high-concurrency \\nenvironments. As e -commerce platforms continue to evolve and innovate, the selection of an \\nappropriate database technology remains a pivotal decision, necessitating a holistic understanding \\nof the technical nuances and strategic implications associated with SQL and NoSQL paradigms. \\nMethodology \\nResearch Design: This study adopts a comparative research design to evaluate the suitability of \\nSQL and NoSQL databases for e -commerce applications. The research design encompasses a \\nsystematic analysis of relevant literature, benchmarking studies, and real-world case examples to \\nelucidate the distinctive features, advantages, and limitations associated with each database \\nparadigm. \\nLiterature Review:  A comprehensive literature review was conducted to synthesiz e existing \\nresearch and scholarly discourse on SQL and NoSQL databases in the context of e -commerce. \\nThis entailed a thorough examination of peer-reviewed journals, conference proceedings, industry \\nreports, and academic textbooks to identify key findings, comparisons, trends, and empirical \\nstudies relevant to the research objectives.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n60 | P a g e  \\n \\nData Collection: Data collection encompassed the gathering of scholarly articles, research papers, \\nand industry reports from reputable sources such as academic databases (e.g., IEEE Xplore, ACM \\nDigital Library) and scholarly search engines (e.g., Google Scholar). Additionally, real-world case \\nstudies and benchmarking studies published by industry practitioners and database vendors were \\ncurated to provide empirical insights and validation for the research findings. \\nData Analysis: The collected data were subjected to rigorou s analysis and synthesis to extract \\nmeaningful insights and trends pertaining to the comparative evaluation of SQL and NoSQL \\ndatabases in e -commerce applications. This involved thematic analysis, content analysis, and \\nqualitative synthesis techniques to identify common themes, patterns, and discrepancies across the \\nliterature corpus. \\nComparison Framework: A structured comparison framework was developed to systematically \\nevaluate the performance, scalability, consistency, flexibility, and suitability for real-time analytics \\nof SQL and NoSQL databases in e -commerce contexts. This framework served as a guiding \\nframework for organizing and synthesizing the research findings into a coherent narrative and \\nfacilitating objective comparisons between the two database paradigms. \\nCase Study Analysis:  Real-world case studies and benchmarking studies were analyzed to \\ncomplement the theoretical insights derived from the literature review with empirical evidence and \\npractical implications. This involved scrutinizing the met hodologies, findings, and \\nrecommendations presented in case studies from leading e -commerce companies and database \\nvendors to glean actionable insights and best practices for database selection and architecture. \\nValidation and Reliability:  To enhance the v alidity and reliability of the research findings, \\nmultiple data sources were triangulated, and cross -referenced to ensure consistency and \\nrobustness. Moreover, peer -review feedback and expert validation were sought to verify the \\naccuracy and credibility of the research methodology, findings, and conclusions. \\nEthical Considerations: Throughout the research process, ethical considerations were upheld to \\nensure the responsible handling and use of copyrighted materials, proprietary data, and sensitive \\ninformation. Proper attribution and citation practices were adhered to, and permissions were \\nobtained where necessary to avoid plagiarism and copyright infringement.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n61 | P a g e  \\n \\nLimitations: It is important to acknowledge the limitations inherent in the research methodology, \\nincluding potential biases in the selection of literature and case studies, as well as the \\ngeneralizability of the findings to specific e-commerce contexts. Additionally, the dynamic nature \\nof database technology and e-commerce trends may introduce temporal limitations to the research \\nfindings, necessitating continuous monitoring and updating of the literature base. \\nConclusion: In conclusion, the research methodology adopted in this study provides a rigorous \\nand systematic approach to evaluating the suitabil ity of SQL and NoSQL databases for e -\\ncommerce applications. By integrating theoretical insights with empirical evidence and practical \\ncase examples, the methodology offers a comprehensive framework for informed decision-making \\nand strategic planning in database selection and architecture for e-commerce businesses. \\n \\nData Collection Methods:  The data collection process involved gathering scholarly articles, \\nresearch papers, and industry reports from reputable sources such as academic databases (e.g., \\nIEEE Xpl ore, ACM Digital Library) and scholarly search engines (e.g., Google Scholar). \\nAdditionally, real-world case studies and benchmarking studies published by industry practitioners \\nand database vendors were curated to provide empirical insights and validation  for the research \\nfindings. \\nFormulas Used in Analysis: \\n1. Performance Comparison: \\nPerformance metrics such as throughput (TP) and latency (LT) were calculated using the following \\nformulas: \\nThroughput (TP)=Total number of transactions processedTotal timeThroughput (TP)=Total time\\nTotal number of transactions processed \\nLatency (LT)=Total timeTotal number of transactions processedLatency (LT)=Total number of t\\nransactions processedTotal time \\n2. Scalability Assessment: \\nScalability was evaluated based on the scalability index (SI), calculated as:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n62 | P a g e  \\n \\nScalability Index (SI)=Throughput (TP2)Throughput (TP1)×100%Scalability Index (SI)=Throug\\nhput (TP1)Throughput (TP2)×100% \\nAnalysis Procedure: \\n1. Literature Review:  A comprehensive literature review was conducted to synth esize \\nexisting research and scholarly discourse on SQL and NoSQL databases in the context of \\ne-commerce. Key findings, comparisons, and trends were extracted from peer -reviewed \\njournals, conference proceedings, industry reports, and academic textbooks. \\n2. Data Synthesis: The collected data were subjected to thematic analysis, content analysis, \\nand qualitative synthesis techniques to identify common themes, patterns, and \\ndiscrepancies across the literature corpus. This involved organizing and categorizing the \\nresearch findings according to predefined comparison criteria and evaluation dimensions. \\n3. Performance Evaluation:  Performance metrics such as throughput and latency were \\ncalculated based on benchmarking studies and real -world case examples. Comparative \\nanalysis was conducted to assess the relative performance of SQL and NoSQL databases \\nin handling e-commerce workloads under varying conditions. \\n4. Scalability Analysis:  Scalability was evaluated using scalability index calculations, \\ncomparing the throughput of SQL and NoSQL databases under increasing workload levels. \\nThe scalability index provided insights into the ability of each database paradigm to scale \\nlinearly with growing transaction volumes. \\n5. Validation and Reliability:  To enhance the validity and reliabilit y of the research \\nfindings, multiple data sources were triangulated, and cross -referenced to ensure \\nconsistency and robustness. Peer -review feedback and expert validation were sought to \\nverify the accuracy and credibility of the analysis. \\nOriginal Work Published: \\nThe original work resulting from this analysis has been published in [Journal Name], [Year], by \\n[Author Name(s)]. The findings and insights presented in this study contribute to the body of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n63 | P a g e  \\n \\nknowledge on database management in e -commerce and offer p ractical guidance for database \\nselection and architecture in e-commerce businesses. \\nResults: \\nTo demonstrate the comparative performance of SQL and NoSQL databases in an e -commerce \\ncontext, we conducted a series of benchmarking experiments using synthetic workloads simulating \\ntypical e-commerce transaction scenarios. The results revealed notable differences in throughput \\nand latency between the two database paradigms under varying concurrency levels. \\nThroughput Analysis:  SQL databases exhibited robust throug hput performance in low to \\nmoderate concurrency scenarios, with transactional throughput averaging 1000 transactions per \\nsecond (TPS) under steady-state conditions. However, as concurrency levels increased beyond 100 \\nconcurrent users, SQL databases experie nced diminishing throughput gains, plateauing at \\napproximately 1200 TPS due to resource contention and locking overhead. \\nIn contrast, NoSQL databases demonstrated superior scalability and throughput efficiency, \\nachieving linear scalability up to 10,000 TPS under high concurrency conditions. The distributed \\narchitecture and partitioning strategies inherent in NoSQL databases enabled seamless scaling to \\naccommodate the influx of concurrent transactions, resulting in sustained high throughput levels \\neven at peak loads. \\nLatency Analysis:  Latency analysis revealed compelling performance advantages for NoSQL \\ndatabases compared to SQL counterparts, particularly under high concurrency conditions. SQL \\ndatabases exhibited increasing transaction latency as concurrency levels rose, peaking at 100 \\nmilliseconds (ms) for 100 concurrent users and escalating further to 200 ms for 500 concurrent \\nusers. \\nConversely, NoSQL databases maintained low and consistent latency levels across all concurrency \\nlevels, with transaction response times averaging below 50 ms even at peak loads. The distributed, \\nhorizontally scalable architecture of NoSQL databases facilitated efficient data partitioning and \\nparallel query processing, minimizing contention and latency overhead. \\nDiscussion:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n64 | P a g e  \\n \\nThe results of the benchmarking experiments underscore the significant performance disparities \\nbetween SQL and NoSQL databases in e -commerce applications, particularly in terms of \\nthroughput, scalability, and latency. While SQL databases offer robust transaction al capabilities \\nand strong consistency guarantees, they exhibit limitations in scalability and throughput efficiency \\nunder high concurrency scenarios. \\nConversely, NoSQL databases excel in scalability and throughput efficiency, leveraging \\ndistributed archit ectures and partitioning strategies to accommodate the dynamic workload \\npatterns and concurrency demands inherent in e -commerce platforms. The linear scalability and \\nlow-latency characteristics of NoSQL databases make them well -suited for handling the \\nunpredictability and variability of e -commerce transactions, enabling seamless scaling to meet \\ngrowing demands without sacrificing performance or user experience. \\nThe findings of this study have practical implications for e -commerce businesses seeking to \\noptimize their database architecture for performance and scalability. By leveraging NoSQL \\ndatabases, e-commerce platforms can enhance their ability to handle peak loads, support real-time \\ntransaction processing, and deliver responsive user experiences, ultimate ly driving customer \\nsatisfaction and business growth. \\nMoreover, the comparative analysis highlights the importance of aligning database technology \\nchoices with specific application requirements and performance objectives. While SQL databases \\nmay suffice fo r transactional workloads with predictable concurrency patterns and stringent \\nconsistency requirements, NoSQL databases offer a compelling alternative for e -commerce \\nplatforms prioritizing scalability, flexibility, and low -latency performance in dynamic, h igh-\\nconcurrency environments. \\nIn conclusion, the results and discussion presented in this study underscore the critical role of \\ndatabase technology in shaping the performance and scalability of e -commerce platforms. By \\nunderstanding the strengths and limit ations of SQL and NoSQL databases and their implications \\nfor transaction processing and user experience, e -commerce businesses can make informed \\ndecisions to optimize their database architecture and drive competitive advantage in the digital \\nmarketplace.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n65 | P a g e  \\n \\nThroughput Analysis: \\nTo quantify the transactional throughput of SQL and NoSQL databases under varying concurrency \\nlevels, we conducted benchmarking experiments using synthetic workloads. The results are \\nsummarized in Table 1 below: \\nTable 1: Transactional Throughput (Transactions per Second, TPS) \\nConcurrency Level SQL Database Throughput (TPS) NoSQL Database Throughput (TPS) \\n10 800 1000 \\n50 1000 2500 \\n100 1200 5000 \\n500 1300 10000 \\n \\n \\nLatency Analysis: \\nTransaction latency was measured as the time taken for a transaction to be processed and \\ncompleted by the database system. The results are presented in Table 2 below: \\nTable 2: Transactional Latency (Milliseconds, ms) \\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n10 20 10 \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n1 2 3 4\\nTransactional Throughput (Transactions per Second, TPS)\\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n66 | P a g e  \\n \\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n50 50 15 \\n100 100 20 \\n500 200 30 \\n \\n \\nFormulas Used: \\n1. Throughput (TPS): \\nThroughput (TPS)=Total number of transactionsTotal timeThroughput (TPS)=Total timeTotal n\\number of transactions \\n2. Latency (ms): \\nLatency (ms)=Total timeTotal number of transactionsLatency (ms)=Total number of transactions\\nTotal time \\nExcel File for Charts: \\nThe data provided in Tables 1 and 2 can be easily exported to an Excel file for chart creation. By \\nutilizing the values from these tables, you can generate visual representations such as line charts \\nor bar charts to depict the throughput and latency trends of SQL and NoSQL databases under \\nvarying concurrency levels. This will facilitate a more intuitive understanding of the performance \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n1 2 3 4\\nTransactional Latency (Milliseconds, ms)\\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n67 | P a g e  \\n \\ndifferences between the two database paradigms and enable stakeholders to make informed \\ndecisions regarding database selection and optimization for e-commerce applications. \\nConclusion \\nIn this study, we conducted benchmarking experiments to compare the performance of SQL and \\nNoSQL databases in e -commerce applications, focusing on transactional throughput and latency \\nunder varying concurrency levels. The results of our analysis highlight notable differences between \\nthe two database paradigms, with implications for scalability, performance, and user experience in \\ne-commerce platforms.  From the throughput analysis, it is evident that NoSQL databases \\ndemonstrate superior scalability and throughput efficiency compared to SQL counterparts. Under \\nincreasing concurrency levels, NoSQL databases exhibit linear scalability, accommodating higher \\ntransaction volumes with minimal degradation in throughput. This scalability advantage enables \\ne-commerce platforms to handle dynamic workload patterns and peak traffic lo ads without \\nsacrificing performance or user experience.  Similarly, the latency analysis reveals compelling \\nperformance advantages for NoSQL databases, particularly under high concurrency scenarios. \\nNoSQL databases consistently maintain low-latency response times, ensuring responsive and real-\\ntime transaction processing even at peak loads. In contrast, SQL databases experience escalating \\nlatency under high concurrency levels, potentially leading to user frustration and degraded \\nperformance. The findings of t his study have practical implications for e -commerce businesses \\nseeking to optimize their database architecture for scalability and performance. By leveraging \\nNoSQL databases, e-commerce platforms can enhance their ability to support growing transaction \\nvolumes, deliver responsive user experiences, and capitalize on peak demand periods without \\nexperiencing performance bottlenecks or downtime.  Moreover, the comparative analysis \\nunderscores the importance of aligning database technology choices with specific application \\nrequirements and performance objectives. While SQL databases may suffice for transactional \\nworkloads with predictable concurrency patterns and stringent consistency requirements, NoSQL \\ndatabases offer a compelling alternative for e -commerce pla tforms prioritizing scalability, \\nflexibility, and low -latency performance in dynamic, high -concurrency environments.  In \\nconclusion, the results of this study shed light on the critical role of database technology in shaping \\nthe performance and scalability of e -commerce platforms. By understanding the strengths and \\nlimitations of SQL and NoSQL databases and their implications for transaction processing and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n68 | P a g e  \\n \\nuser experience, e-commerce businesses can make informed decisions to optimize their database \\narchitecture and drive competitive advantage in the digital marketplace. \\nReferences: \\n1. Gadde, S. S., & Kalli, V. D. R. (2020). Descriptive analysis of machine learning and its \\napplication in healthcare. Int J Comp Sci Trends Technol, 8(2), 189-196. \\n2. Bommu, R. (2022). Advancements in Medical Device Software: A Comprehensive Review \\nof Emerging Technologies and Future Trends.  Journal of Engineering and \\nTechnology, 4(2), 1-8. \\n3. Gadde, S. S., & Kalli, V. D. (2021). The Resemblance of Library and Information Science \\nwith Medic al Science.  International Journal for Research in Applied Science & \\nEngineering Technology, 11(9), 323-327. \\n4. Gadde, S. S., & Kalli, V. D. R. (2020). Technology Engineering for Medical Devices -A \\nLean Manufacturing Plant Viewpoint. Technology, 9(4). \\n5. Bommu, R.  (2022). Advancements in Healthcare Information Technology: A \\nComprehensive Review. Innovative Computer Sciences Journal, 8(1), 1-7. \\n6. Gadde, S. S., & Kalli, V. D. R. (2020). Medical Device Qualification Use.  International \\nJournal of Advanced Research in Computer and Communication Engineering, 9(4), 50-55. \\n7. Bommu, R. (2022). Ethical Considerations in the Development and Deployment of AI -\\npowered Medical Device Software: Balancing Innovation with Patient Welfare. Journal of \\nInnovative Technologies, 5(1), 1-7. \\n8. Gadde, S. S., & Kalli, V. D. R. (2020). Artificial Intelligence To Detect Heart Rate \\nVariability. International Journal of Engineering Trends and Applications, 7(3), 6-10. \\n9. Brian, K., & Bommu, R. (2022). Revolutionizing Healthcare IT through AI and \\nMicrofluidics: From Drug Screening to Precision Livestock Farming. Unique Endeavor in \\nBusiness & Social Sciences, 1(1), 84-99. \\n10. Gadde, S. S., & Kalli, V. D. R. (2020). Applications of Artificial Intelligence in Medical \\nDevices and Healthcare.  International Journal  of Computer Science Trends and \\nTechnology, 8, 182-188.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n69 | P a g e  \\n \\n11. Brandon, L., & Bommu, R. (2022). Smart Agriculture Meets Healthcare: Exploring AI -\\nDriven Solutions for Plant Pathogen Detection and Livestock Wellness \\nMonitoring. Unique Endeavor in Business & Social Sciences, 1(1), 100-115. \\n12. Gadde, S. S., & Kalli, V. D. (2021). Artificial Intelligence at Healthcare \\nIndustry. International Journal for Research in Applied Science & Engineering \\nTechnology (IJRASET), 9(2), 313. \\n13. Thunki, P., Reddy, S. R. B., Raparthi, M., Ma ruthi, S., Dodda, S. B., & Ravichandran, P. \\n(2021). Explainable AI in Data Science -Enhancing Model Interpretability and \\nTransparency. African Journal of Artificial Intelligence and Sustainable \\nDevelopment, 1(1), 1-8. \\n14. Gadde, S. S., & Kalli, V. D. An Innovative Study on Artificial Intelligence and Robotics. \\n15. Gadde, S. S., & Kalli, V. D. (2021). Artificial Intelligence and its Models.  International \\nJournal for Research in Applied Science & Engineering Technology, 9(11), 315-318. \\n16. Raparthi, M., Dodda, S. B., Redd y, S. R. B., Thunki, P., Maruthi, S., & Ravichandran, P. \\n(2021). Advancements in Natural Language Processing -A Comprehensive Review of AI \\nTechniques. Journal of Bioinformatics and Artificial Intelligence, 1(1), 1-10. \\n17. Gadde, S. S., & Kalli, V. D. R. A Qualitative Comparison of Techniques for Student \\nModelling in Intelligent Tutoring Systems. \\n18. Raparthi, M., Maruthi, S., Reddy, S. R. B., Thunki, P., Ravichandran, P., & Dodda, S. B. \\n(2022). Data Science in Healthcare Leveraging AI for Predictive Analytics a nd \\nPersonalized Patient Care. Journal of AI in Healthcare and Medicine, 2(2), 1-11. \\n19. Gadde, S. S., & Kalli, V. D. Artificial Intelligence, Smart Contract, and Islamic Finance. \\n20. Kalli, V. D. R. (2022). Human Factors Engineering in Medical Device Software Desi gn: \\nEnhancing Usability and Patient Safety. Innovative Engineering Sciences Journal, 8(1), 1-\\n7. \\n21. Kalli, V. D. R. (2022). Improving Healthcare Delivery through Innovative Information \\nTechnology Solutions. MZ Computing Journal, 3(1), 1-6.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='An Empirical Study on the Design and\\nEvolution of NoSQL Database Schemas\\nStefanie Scherzinger1 and Sebastian Sidortschuck2\\n1 OTH Regensburg, Regensburg, Germany\\nstefanie.scherzinger@oth-regensburg.de\\n2 OTH Regensburg, Regensburg, Germany\\n& SPARETECH.io, Stuttgart, Germany\\nsebastian.sidortschuck@sparetech.io\\nAbstract. We study how software engineers design and evolve their\\ndomain model when building applications against NoSQL data stores.\\nSpeciﬁcally, we target Java projects that use object-NoSQL mappers to\\ninterface with schema-free NoSQL data stores. Given the source code\\nof ten real-world database applications, we extract the implicit NoSQL\\ndatabase schema. We capture the sizes of the schemas, and investigate\\nwhether the schema is denormalized, as is recommended practice in data\\nmodeling for NoSQL data stores. Further, we analyze the entire project\\nhistory, and with it, the evolution history of the NoSQL database schema.\\nIn doing so, we conduct the so far largest empirical study on NoSQL\\nschema design and evolution.\\nKeywords: Schema Evolution · NoSQL Databases · Empirical Study.\\n1 Introduction\\nSchema-ﬂexible NoSQL data stores have become popular backends for building\\ndatabase applications. Systems like MongoDB allow for ﬂexible changes to the\\ndomain model during application development. In particular, they have proven\\nthemselves in settings where applications are frequently deployed to their pro-\\nduction environment, e.g., when web applications are built in an agile approach.\\nWhile the data stores do not enforce a global schema, the application code\\ngenerally assumes that persisted entities adhere to a certain (if loose) domain\\nmodel. Given that schema-ﬂexibility is one of the major selling points of NoSQL\\ndata stores, this raises the question how the domain model, and thereby the\\nimplied NoSQL database schema , actually evolves. We empirically study the\\ndynamics of NoSQL database schema evolution. Further, we investigate the\\nquestion whether the NoSQL database schema is denormalized, as commonly\\nrecommended in literature, e.g. [15].\\nUnfortunately, real-world data dumps of NoSQL data stores are hard to come\\nby. We therefore resort to analyzing the source code of applications hosted on\\nGitHub. We focus on the relevant software stack shown in Figure 1a, namely Java\\napplications that use an object-NoSQL mapper to store data in either Google\\narXiv:2003.00054v1  [cs.DB]  28 Feb 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='2 Scherzinger and Sidortschuck\\nCloud Datastore3 or MongoDB4, both popular and mature data stores. Among\\nover 1.2K open source GitHub repositories with this stack, we have identiﬁed\\nthe ten projects with the largest NoSQL schemas (a notion introduced shortly).\\nPrevious studies on schema evolution have primarily focused on schema-full,\\nrelational databases [5,11,13,18,20,25]. About NoSQL schema evolution in real-\\nworld applications, little is known that is based on systematic, empirical studies\\n(versus anecdotal evidence): Earlier studies have a diﬀerent focus (such as the\\nusage of certain mapper features [14]), or analyze a single project (c.f. [12]).\\nIn this paper, we introduce our notion of the NoSQL database schema,\\nwhich is implicit in object mapper class declarations, even though the under-\\nlying NoSQL data stores are schema-free. In this setting, this paper makes the\\nfollowing contributions:\\n– We formulate three research questions, namely (RQ1) whether the NoSQL\\ndatabase schema is denormalized (as recommended in literature), (RQ2)\\nwhich growth in complexity we can observe in NoSQL database schemas\\nover the project development time, and (RQ3) how the NoSQL database\\nschema evolves, thereby identifying the common changes.\\n– We analyze the ten projects with the largest NoSQL database schemas\\namong over 1.2K candidate projects, based on static code analysis and the\\ncommit history. We are able to conﬁrm that denormalization is common in\\nNoSQL database schemas. We are further able to show evidence of evolu-\\ntionary changes to the NoSQL database schema in all analyzed projects.\\n– We discuss our ﬁndings w.r.t. related studies on relational schema evolution\\nand ﬁnd that the churn rate of NoSQL schemas is comparatively high.\\nStructure. Next, we introduce preliminaries in Section 2. In Section 3, we describe\\nour methodology, and state our research questions. In Section 4, we present the\\nresults of our study, which we then discuss in Section 5. We point out threats to\\nthe validity of our results in Section 6, and give an overview over related work\\nin Section 7. We conclude with an outlook on future work.\\n2 Preliminaries\\nWe next introduce the software stack studied, as well as our terminology.\\nPhysical entities. We consider two popular NoSQL data stores: Google Cloud\\nDatastore (called Datastore hereafter) is commercial and hosted on the Google\\nCloud Platform, MongoDB is open source. Both data stores are schema-free\\n(however, MongoDB oﬀersoptional schema validation). Both manage document-\\nlike data, which we refer to as the (physical) entities. On an abstract level, an\\nentity is a collection of key-value pairs, or properties. Entities may be nested\\nand properties may be multi-valued. We sketch a Datastore entity representing\\na player and his or her missions in a role playing game in Figure 1a, in (simpliﬁed)\\nJSON notation, to abstract away from system-proprietary storage formats.\\n3 https://cloud.google.com/datastore/, available since 2009.\\n4 https://www.mongodb.com/, available since 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Design and Evolution of NoSQL Database Schemas 3\\n(a) The software stack.\\n (b) Code changes to class Player.\\nFig. 1. (a) The object-NoSQL mapper separates the domain model from the NoSQL\\ndata store (adapted from [6]). (b) Not all code changes are actually schema-relevant.\\nDomain models. In principle, each entity in a schema-free data store may have\\nits very own, unique structure. However, in database applications, it is safe\\nto assume that the software engineers have agreed on some domain model , as\\nsketched in Figure 1a. In our setting, the domain model is captured by Java class\\ndeclarations, yet in the Figure, we use the more compact UML notation. (For\\nnow, we ignore the @-labeled annotations.) Class Player declares attributes for\\nan identiﬁer, a name, an amount of credits, and a list of missions. Each mission\\nalso has an identiﬁer, a title, a level of diﬃculty, and tracks its completion.\\nObject-NoSQL Mappers. Object mappers are state-of-the-art in building data-\\nbase applications [6]. Like object-relational mappers, the object-NoSQL map-\\npers Objectify 5 and Morphia6 map Java objects to entities. Objectify is tied to\\nDatastore, and Morphia to MongoDB. With object-NoSQL mappers, develop-\\ners merely specify their domain model as Java classes that are annotated with\\nthe keyword @Entity. Each entity-class has a unique key (annotated with @Id).\\nThe object mapper provides methods for saving and loading: In Figure 1a, the\\nclass name and the identifying attribute are mapped to the designated proper-\\nties _kind and _id. Objectify maps the player’s list of missions to an array of\\nnested entities. Yet at application runtime, an entity-class declaration may not\\nmatch the structure of all persisted entities, as discussed next.\\nLazy data migration. The data store may also store legacy versions of entities.\\nFigure 1b shows a new version of entity-class Player, with changes due to new\\nrequirements in the software development project. Attribute coins has replaced\\ncredits. Merely changing the entity-class in the application code does not af-\\nfect any existing entities. Instead, persisted entities are only migrated lazily,\\n5 https://github.com/objectify/objectify\\n6 https://github.com/MorphiaOrg/morphia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='4 Scherzinger and Sidortschuck\\nupon loading: The new version of entity-class Player in Figure 1b is backwards-\\ncompatible with Figure 1a. Once the legacy entity for Frodo has been loaded, the\\ncorresponding Java object will have an attributecoins, as annotation @AlsoLoad\\nlazily renames attributes.\\nThus, to obtain a summary of the structural variety of physical entities in\\nthe data store (based on code analysis alone, not having access to the data store\\ncontents itself), we need to consider the entire evolution history of entity-classes.\\nNoSQL database schema evolution. We base our notion of the NoSQL database\\nschema (or shorter, NoSQL schema) on the domain model. This idea of treating\\nentity-classes as schema declarations is re-current in literature, c.f. [4, 17]. Note\\nthat not all Java attributes are relevant for the NoSQL database schema: At-\\ntributes that are transient, e.g., carrying Objectify annotation @Ignore, are not\\nschema-relevant: The value of hoursSinceLastLogin in Figure 1b is not per-\\nsisted (it may be derived from lastLogin). Also, class methods are not schema-\\nrelevant. Thus, code changes that only aﬀect transient attributes or class meth-\\nods are part of software evolution, but not of schema evolution. Therefore, they\\nare not considered schema changes by us.\\nDenormalized entity-classes. The recommendation in working with Datastore\\nand MongoDB is to intentionally denormalize the schema. 7 This can be done\\nby either nesting entities, or by using multi-valued properties, such as the array\\nof Missions in Figure 5. There are various motivations for denormalization, one\\nbeing that traditionally, the query languages do not provide a join operator\\n(such is still the case in Google Cloud Datastore, and this also used to be the\\ncase for MongoDB), so joined data is materialized in the data store. Another\\nreason is that transactions between an arbitrary number of entities may not\\nbe supported 8. Consequently, transactionally safe updates are often realized by\\nupdates to a single, aggregate entity.\\nIn the following, we say an entity-class is denormalized if it does not declare\\nﬂat, relational-style tuples in ﬁrst normal form, i.e., with atomic attribute values\\nonly. So unless all schema-relevant attributes have Java primitive types (such as\\nInteger, String, Boolean, . . . ), we say the entity-class is denormalized. As we\\ndiscuss in Section 3.3, this is a practical yet conservative approach.\\nAs an example, the entity-class declarations for players, sketched in Figure 1,\\nis denormalized, due to the multi-valued attribute listOfMissions.\\n3 Methodology\\nIn the following, we describe our methodology, such as the context of our analysis,\\nthe research questions, and the analysis process. While our outline has strong\\nanalogies to Qiu et al. [13] and their analysis of relational schema evolution, our\\n7 E.g. “6 Rules of Thumb for MongoDB Schema Design” at https://www.mongodb.\\ncom/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-2, June 2015.\\n8 We point to the concepts such of entity groups and cross-group transactions in the\\nclassic Google Cloud Datastore [16], which is in the process of being deprecated.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Design and Evolution of NoSQL Database Schemas 5\\nT able 1. Characteristics of the studied database applications.\\nProject Life Cycle # Commits # Entity-\\nclasses LoC (K)\\nObjectify\\nCryptonomica/cryptonomica 04/16 ∼ 09/18 185 0 ∼ 29 0 ∼ 526\\nFraunhoferCESE/madcap 12/14 ∼ 03/18 853 0 ∼ 82 0 ∼ 17\\ngoogle/nomulus 03/16 ∼ 09/18 2,025 51 ∼ 55 138 ∼ 224\\nnareshPokhriyal86/testing 01/15 ∼ 02/15 25 0 ∼ 79 0 ∼ 449\\nNekorp/Tikal-Technology 04/15 ∼ 11/15 59 0 ∼ 43 0 ∼ 49\\nMorphia\\naltiplanogao/tallyframework 06/15 ∼ 06/16 167 0 ∼ 24 0 ∼ 5\\nbujilvxing/QinShihuang 10/16 ∼ 12/16 154 0 ∼ 36 0 ∼ 21\\ncatedrasaes-umu/NoSQLDataEngineering 11/16 ∼ 09/18 711 0 ∼ 28 0 ∼ 280\\nGBPeters/PubInt 10/16 ∼ 02/18 69 0 ∼ 27 0 ∼ 5\\nMKLab-ITI/simmo 07/14 ∼ 02/17 142 0 ∼ 51 0 ∼ 5\\nprocess is rather diﬀerent: we cannot analyze schemas declared in a declarative\\ndata deﬁnition language, such as SQL. Rather, we need to parse raw Java code.\\n3.1 Context\\nWe used BigQuery9 to identify relevant open source repositories on GitHub, as\\nof September 4th, 2018. We consider a repository (which we synonymously refer\\nto as a project) relevant if it contains Java import statements for Objectify or\\nMorphia. We cloned over 1.2K candidate repositories and excluded any reposi-\\ntories that (1) have fewer than 20 commits (to exclude tinker projects), (2) are\\nthe Morphia or Objectify source code (or forks thereof), (3) or are ﬂagged as\\nforks from repositories already covered, with no schema-relevant code changes\\nafter the fork. We analyze the project history using git log 10. This allows us\\nto re-trace the development history of all entity classes. We parse and aggregate\\nthe log output using Python scripts.\\nAmong all projects analyzed, we determined the maximum number of entity-\\nclasses throughout the project history, and settled on the top-5 projects for\\nObjectify and Morphia respectively. Table 1 lists these projects with their life\\ncycles up to the latest commit at the time of our analysis. We also state the total\\nnumber of commits at the time. We state the minimum and maximum number\\nof entity-classes throughout the project history, as well as the total number of\\nlines of code between the ﬁrst and last analyzed commit (measured with cloc11\\nand reported in thousands).\\n9 Google BigQuery is a commercial cloud service. This data warehousing tool allows\\nfor querying the GitHub open data collection, mostly non-forked projects with an\\nopen source license: https://cloud.google.com/bigquery/.\\n10 We state the exact command pattern for reproducability: git log\\n--before=2018-09-04T00:00:00 --cherry-pick --date-order --pretty=format:\"%H;%aI;%cI;%P\" .\\n11 https://github.com/AlDanial/cloc'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='6 Scherzinger and Sidortschuck\\n3.2 Research Questions\\nRQ1: Are NoSQL schemas denormalized? We analyze the structure of entity-\\nclasses, whether they map to ﬂat tuples in ﬁrst normal form, or whether they\\nrepresent denormalized data.\\nRQ2: What is the growth in complexity of the NoSQL schema? We cap-\\nture schema complexity based on metrics recognized in literature.\\nRQ3: How does the NoSQL schema evolve? We automatically identify and\\nclassify evolutionary changes to the NoSQL schema.\\n3.3 Analysis Process\\nLocating entity-classes. We replay the commit histories and use the Java parser\\nQDox12 to parse class declarations. We identify entity-classes by the object map-\\nper annotation @Entity, which may also be inherited. 13\\nDenormalization. To determine whether an entity-class is denormalized, we parse\\nits Java declaration and strip away attributes that are not relevant to the NoSQL\\nschema. We then analyze the types of the remaining attributes. Unless all have\\nprimitive types (such as Integer, String, or Boolean), we assume that the\\nentity-class is denormalized.\\nIn most cases, we correctly recognize denormalization: (1) if the entity class\\ndeclaration contains container classes (e.g., a Java Collection), and therefore\\nan attribute is multi-valued. (2) Equally, the entity-class may contain nested\\nentity classes, giving it a hierarchical structure.\\nHowever, there are also cases where this approach is a conservative sim-\\npliﬁcation, and we might falsely categorize an entity-class as denormalized: an\\nattribute type may be declared in a third-party library, which is inaccessible\\nto us (see also our discussion in Section 6). Also, an attribute type may be a\\ncustom type that the developers declared. To realize that a custom type is just a\\nwrapper for a basic Java type, we would have to run more involved code analysis.\\nYet typically, polymorphic types are involved, and we are confronted with the\\ninherent limitations of static code analysis.\\nIdentifying schema changes. We identify commits with schema-relevant changes\\nby comparing succeeding versions of the application source code: We register\\nwhen (1) a new entity-class is added or an entity-class is removed, (2) a schema-\\nrelevant attribute is added or removed in an entity-class declaration, and (3) fur-\\nther, changes to schema-relevant attributes, such as to their types, default ini-\\ntializations, or even object mapper annotations. We only focus on changes which\\nwe can recognize programmatically. Recognizing renaming or splitting an entity-\\nclass, or renaming an attribute, are instances of the challenge of schema matching\\nand mapping [2], and cannot be fully automated.\\n12 https://github.com/paul-hammant/qdox\\n13 In earlier versions of the mapper libraries, this annotation was only optional, so it\\ncannot be relied upon. We therefore also search for the mandatory annotation @Id,\\nand thus reliably detect polymorphic entity-classes. (c.f. Section 6).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Design and Evolution of NoSQL Database Schemas 7\\nCryptonomica/\\ncryptonomica\\nFraunhoferCESE/\\nmadcap\\ngoogle/\\nnomulus\\nnareshPokhriyal86/\\ntesting\\nNekorp/\\nTikal-Technology\\n(a) Objectify-based projects.\\naltiplanogao/\\ntallyframework\\nbujilvxing/\\nQinShihuang\\ncatedrasaes-umu/\\nNoSQLDataEngineering\\nGBPeters/\\nPubInt\\nMKLab-ITI/\\nsimmo\\nDenormalized entity-class\\nOther\\n(b) Morphia-based projects.\\nFig. 2. Visualization of denormalized NoSQL database schemas.\\n4 Results of the Study\\n4.1 RQ1: Are NoSQL schemas denormalized?\\nWe analyze the entity-class declarations in their most current version w.r.t. de-\\nnormalization. The results are visualized in Figure 2. For each analyzed project,\\nwe show a dot matrix chart. The number of dots represents the number of entity-\\nclasses. The brighter (orange) dots represent the entity-class declarations which\\nwe must assume to be denormalized, due to the limits of static code analysis.\\nThe darker (blue) dots represent the other entity-classes.\\nNotably, each project contains at least one denormalized entity-class, so all\\nschemas are denormalized. With the exception of two Objectify-based projects,\\ndenormalized entity-classes dominate the NoSQL database schemas. There are\\neven two Morphia-based projects where all entity-classes are denormalized.\\nResults. We ﬁnd that each project analyzed has denormalized entity-classes in its\\nNoSQL schema. This shows that developers make active use of denormalization.\\nHowever, without qualitative studies based on developer surveys, we do not\\nknow whether (1) the developers consciously chose a database which allows for\\na denormalized database schema, as this better suits their conceptual model.\\nHowever, it could also be that (2) they are actually forced denormalize their\\ndata model, due to the technological limitations of NoSQL data stores (brieﬂy\\ndiscussed in Section 2).\\n4.2 RQ2: What is the growth in complexity of the NoSQL schema?\\nIn empirical studies on relational schema evolution, the number of tables is\\nconsidered a simple approximation for schema complexity [7]. Accordingly, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='8 Scherzinger and Sidortschuck\\n0% 20% 40% 60% 80% 100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nCryptonomica/cryptonomica\\n0% 20% 40% 60% 80% 100%\\nFraunhoferCESE/madcap\\n0% 20% 40% 60% 80% 100%\\ngoogle/nomulus\\n0% 20% 40% 60% 80% 100%\\nnareshPokhriyal86/testing\\n0% 20% 40% 60% 80% 100%\\nNekorp/Tikal-Technology\\n# Entity-classes Schema-LoC\\n(a) Objectify-based projects.\\n0% 20% 40% 60% 80% 100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\naltiplanogao/tallyframework\\n0% 20% 40% 60% 80% 100%\\nbujilvxing/QinShihuang\\n0% 20% 40% 60% 80% 100%\\ncatedrasaes-umu/NoSQLD...\\n0% 20% 40% 60% 80% 100%\\nGBPeters/PubInt\\n0% 20% 40% 60% 80% 100%\\nMKLab-ITI/simmo\\n(b) Morphia-based projects.\\nFig. 3. Evolution trend of entity classes. The horizontal axes show the project progress,\\nin percentage of commits analyzed. The vertical axes show the complexity of the schema\\nw.r.t. its maximum, for two alternative metrics. (Visualization modeled after [13].)\\ntrack the number of entity-classes over time in Figure 3 (based on a visualization\\nidea from [13]). For each project, one chart is shown. On the horizontal axis, we\\ntrack the progress of the project, measured as the percentage of git commits\\nanalyzed. For the madcap project, this is based on 853 commits (c.f. Table 1).\\nOn the vertical axis, we track the size of the NoSQL database schema using two\\nmetrics. One is the number of entity classes (blue solid line). This metric is also\\nnormalized w.r.t. its maximum throughout the project history. So for madcap,\\nthe 100% peak corresponds to 82 entity classes, some of which were removed in\\nthe later phase of the project.\\nThe second line denotes a “proxy metric” [7] for approximating the size of the\\nNoSQL schema, where we count the lines of code of the entity-classes (including\\nsuperclasses, excluding comments and empty lines), and thereby compute the\\nSchema-LoC.14 There is shrinkage, yet overall, schema complexity increases.\\nResults. 1) As in the study by Qiu et al. on relational software evolution [13], we\\ncan conﬁrm that while the projects diﬀer in their life-spans and commit activity,\\nin nearly all projects, the NoSQL schema grows over time. However, there may be\\nphases of refactoring, leading to dips in the curves. 2) Apparently, Schema-LoC\\nlends itself nicely as a proxy-metric, and we obtain high correlation coeﬃcients\\n14 We ﬁnd this proxy-metric preferable over counting (schema-relevant) attributes, as\\nis common in studies on relational schema evolution: (1) Entity-classes with more\\nschema-relevant attributes have more lines of code accordingly. (2) In static code\\nanalysis, we cannot reliably count nested attributes: Abstract container classes and\\nthe use of polymorphism in general, make it impossible to know the number and na-\\nture of nested attributes at compile time. With Schema-LoC, we are able to abstract\\nfrom this issue.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Design and Evolution of NoSQL Database Schemas 9\\nFig. 4. Visualizing relative schema sizes and churn. Each rectangle represents an entity-\\nclass, its area proportional to its size in lines of code (speciﬁcally Schema-LoC). The\\nhue represents the relative frequency of schema changes within the same project.\\nwhen comparing to the number of entity classes. As Schema-LoC depends on\\nthe number of attributes in an entity class, we can retrace an eﬀect reported\\nin [13], namely that entity-classes and their attributes (corresponding to tables\\nand columns) have largely analogous dynamics. 4) In general, the schema grows\\nmore than it shrinks. This is in line with studies on relational schema evolution.\\n5) One observation in [13] was that the schema stabilizes early: There, for 7 out\\nof 10 projects, 60% of the maximum number of tables is reached in the ﬁrst 20%\\nof the commits. Interestingly, in our study, the number of entity-classes reaches\\nthe 60% in only 4 projects. 6) In [13], less than 2% of all commits contain valid\\nschema changes (across all ten projects analyzed there). In our study, the share\\nof commits with schema-relevant changes is between 2.8% and over 30%, with 4\\nprojects reaching over 20%. Clearly, we observe higher churn rates.\\n4.3 RQ3: How does the NoSQL schema evolve?\\nWe ﬁrst investigate how often entity-classes undergo schema changes when com-\\npared to others inside the same project, and how large they are in terms of our\\nproxy metric Schema-LoC. Figure 4 visualizes the entity classes making up the\\nten NoSQL schemas as a tree map. This ﬁgure is best viewed in color. Each\\ncolored area represents one project. Inside, each rectangle represents one entity-\\nclass, the area proportional to its Schema-LoC. Darker hue indicates that an\\nentity-class has undergone more schema changes than the other entity-classes\\nin the same project. For instance, for nomulus, the darkest area represents 12\\nschema changes against the same entity-class. Thus, some entity-classes change\\nquite more often than others. However, there are also projects where schema\\nchanges aﬀect entity-classes quite uniformly.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='10 Scherzinger and Sidortschuck\\n0% 20% 40% 60% 80% 100%\\nNekorp/Tikal-Technology\\nnareshPokhriyal86/testing\\ngoogle/nomulus\\nFraunhoferCESE/madcap\\nCryptonomica/cryptonomica\\nObjectify\\nAdd entity-class\\nRemove entity-class\\nAdd schema-relevant attribute\\nRemove schema-relevant attribute\\nChange schema-relevant attribute\\n0% 20% 40% 60% 80% 100%\\nMKLab-ITI/simmo\\nGBPeters/PubInt\\ncatedrasaes-umu/NoSQLD...\\nbujilvxing/QinShihuang\\naltiplanogao/tallyframework\\nMorphia\\n(a) By project.\\nEntity-class Schema-relevant attribute\\nadd remove add remove change\\nObjectify 56.3% 8.4% 24.7% 4.0% 6.5%\\nMorphia 34.5% 16.2% 21.6% 10.4% 17.2%\\nOverall 34.8% 15.1% 27.3% 7.4% 15.4 %\\n(b) Objectify-based vs. Morphia-based projects.\\nProject Type Initialization Annotations\\nObjectify\\nCryptonomica/cryptonomica 2 0 3\\nFraunhoferCESE/madcap 9 0 6\\ngoogle/nomulus 11 2 58\\nnareshPokhriyal86/testing 0 0 0\\nNekorp/Tikal-Technology 0 0 3\\nMorphia\\naltiplanogao/tallyframework 7 3 54\\nbujilvxing/QinShihuang 32 33 15\\ncatedrasaes-umu/NoSQLDataEngineering 43 0 13\\nGBPeters/PubInt 0 2 2\\nMKLab-ITI/simmo 7 5 18\\n(c) Drill-down into the remaining changes to schema-relevant attributes.\\nFig. 5. Distinguishing diﬀerent kinds of schema changes: (a) and (b): Relative shares of\\nschema changes (by project and by mapper library). (c) Zooming in on the remaining\\nchanges in schema-relevant attributes mentioned in (a), showing absolute values.\\nIn Figure 5, we capture the distribution of schema changes according to\\nthe kind of change. In Subﬁgure 5a (after Qiu et al. in [13]), we break down\\nthe distribution of changes by project. Note that when a new entity-class is\\nadded, we do not count this as adding attributes at the same time. Notably, the\\ndistributions are project-speciﬁc. We now discuss two projects that stand out.\\nIn the fourth Objectify-based project, adding an entity-class makes up for\\nnearly all changes. Considering the project characteristics in Table 1 reveals that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Design and Evolution of NoSQL Database Schemas 11\\nthis project is an outlier in several regards: With only 25 commits, it has barely\\nmade the bar for being considered in our analysis (see Section 3.1). At the same\\ntime, this project holds the second largest number of entity-classes in any project\\nconsidered in this analysis. Since the life cycle considered is only two months,\\nthis project is in a very early stage of development at the time of this analysis.\\nThus, it seems plausible that at this early phase, the developers kick start their\\ndata model by declaring the entity classes in bulk.\\nIn contrast, the second Morphia-based project stands out as the project with\\nthe least share of entity class additions. Since the git commit messages are in\\nChinese (which the authors of this paper do not master), we ﬁnd it diﬃcult to\\nretrace the developers’ motivation. What is noticeable in Subﬁgure 3b is that\\nwhile the number of entity classes increases in less than 10 distinct steps, the\\nproxy metric Schema-LoC changes in more ﬁne-granular steps. Thus, the entity-\\nclasses undergo more frequent changes. This matches the distribution plotted,\\nas then the share of entity-class creations is smaller by comparison. Subﬁgure 5b\\nsummarizes Subﬁgure 5a, and aggregates the changes by the mapper library.\\nWhile we see project-speciﬁc ﬂuctuations, when we group by mapper library, we\\nalso observe diﬀerences in the distribution. Overall, additions (whether of entity\\nclasses or of schema-relevant attributes) dominate.\\nIn Table 5c, we break down the schema-relevant attribute changes listed in\\nSubﬁgures 5a and 5b: (a) For some projects, types change. (b) For others, the\\ninitialization changes. A drill-down reveals that (as may be expected) adding\\nan initial value is the most frequent change, followed by changing the initializa-\\ntion value. (c) In other cases, mapper annotations that aﬀect the schema are\\nadded or removed. The most frequent annotations added are @PersistField\\nand @Reference. The ﬁrst is from a third-party framework. Since it is schema-\\nrelevant, we report it. The second supports referential constraints. Sporadically,\\nthird-party annotations are added to declare additional constraints, such as@Min.\\nResults. 1) We can conﬁrm the observations from related work on relational\\nschema evolution that schema changes are generally not distributed uniformly [13,\\n24]. 2) As already observed for RQ2, the trend is that entity-classes are added\\nmore frequently than they are removed. We see a similar pattern for schema-\\nrelevant attributes, in line with studies on relational schema evolution. Over-\\nall, in 9 out of 10 projects, additions collectively account for more than 50%\\nof the changes. In 5 projects, they even account for over 70% of the changes.\\n3) While additions are generally more frequent, there are also projects where\\nremovals of entity classes occur to a non-signiﬁcant degree. Related work on\\nrelational schema evolution has shown that there are what the authors call sur-\\nvivor tables [22], whereas there are that are more short-lived. The observation\\nthat entity-class removals are very project speciﬁc has also been made in [13].\\n4) Among all annotation changes, only 15 concern referential constraints (an-\\nnotation @Reference). The authors of two relate studies on relational schema\\nevolution, both [13] and [21], have observed that changes concerning referential\\nintegrity constraints are also rare in relational schema evolution. With NoSQL\\ndata stores, this is to be expected, as referential integrity is not supported to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='12 Scherzinger and Sidortschuck\\nthe same extent. 4) While Qiu et al. [13] found changes in attribute types to\\nbe the number one change for half of the projects analyzed (even outnumbering\\nadditions of either tables or columns), we do not see evidence of this eﬀect here.\\n5 Discussion\\nWe can reproduce the main results from related work on relational schema evo-\\nlution: There is strong evidence of NoSQL schema evolution, and additions are\\ndominant schema changes. However, we do not see the schema stabilizing in\\nthe early phases of all projects, which may partly be due to shorter project life\\nspans: The ten projects studied in [13] are PHP applications backed by rela-\\ntional databases, and have longer life cycles (two with ten years), more commits\\n(starting at nearly 5K), and more lines of code. This is to be expected with a\\nmuch older and thus more widely adopted stack.\\nStill, we do suspect that NoSQL developers evolve their schema more continu-\\nously. One indicator supporting this hypothesis is that we see higherchurn rates,\\nso a larger share of the commits contains code changes that aﬀect the schema.\\nThis calls for further study. Due to this churn, making sure that entity-class dec-\\nlarations are “backwards” compatible with legacy entities, persisted by earlier\\nversions of the application code, may become an overwhelming task. There are\\nﬁrst proposals for assisting tools, e.g., by type-checking versions of entity-class\\ndeclarations [3]. Clearly, more research is needed on systematic tool support.\\nThe fact that denormalization is common shows that solutions for managing\\nrelational schema evolution, managing ﬂat tuples, will not transfer immediately.\\nRather, when devising frameworks, we may want to turn to related work on\\nframeworks for handling schema evolution in XML (e.g. [9]) or object-oriented\\ndatabases (e.g. [26]) for inspiration on what has shown to be feasible.\\n6 Threats to Validity\\nConstruct validity. (1) With applications using older versions of Objectify and\\nMorphia, we cannot rely on the @Entity-annotation to identify entity-classes, so\\nwe also consider the @Id annotation. To be conﬁdent that this does not lead to\\nfalse positives, we performed manual checks. (With Objectify, we cross-checked\\nwhich entity-classes were registered with ObjectifyService, a mandatory pro-\\ngramming step.) (2) In static analysis, we encounter a limitation with attribute\\ntypes from third-party libraries. Tracking down these libraries is out of scope\\n(and not even possible in all cases). Thus, there are attributes that are not fully\\ncaptured by Schema-LoC. Yet as this is a proxy-metric to start with, we con-\\nsider this threat acceptable. Third-party libraries also aﬀect the recognition of\\nentity-classes as denormalized. Having sampled and inspected the entity-class\\ndeclarations, we are conﬁdent that – given the limitations of static code analysis\\n– the risk of false positives is acceptable. (3) We treat each single commit as\\ncontributing to a new version of the schema. There are software development\\nteams that operate by continuous deployment, so tested code is immediately'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Design and Evolution of NoSQL Database Schemas 13\\nand autonomously deployed to the production environment. There, in theory,\\neach commit containing a schema change comprises a new schema version. Yet\\nrather often, a release to production comprises more than one commit. Unfor-\\ntunately, we are not able to tell in static code analysis which commits where\\nreleased when. There are development teams that tag release commits, but this\\nis project-internal culture, and not consistently the practice across all ten stud-\\nied projects. Therefore, we must go by the simplifying assumption that each\\ncommit declares a new NoSQL database schema.\\nExternal validity. We next discuss threats in generalizing our results to other\\nsoftware stacks. (1) It would be desirable to search additional code repositories,\\nand extend to further NoSQL data stores, object mapper libraries, and program-\\nming languages. (2) Extending our analysis to projects that do not use object-\\nmappers requires a diﬀerent kind of static code analysis, and was implemented\\nin a related study that involved a single MongoDB project [12]. At the same\\ntime, object mappers are state-of-the art in modern application development,\\nand by now, Objectify and Morphia are actually part of oﬃcial Datastore and\\nMongoDB tutorials (even though they started as independent projects). Thus,\\nwe do analyze a highly relevant stack. (3) There is the fundamental question\\nwhether studies on open source projects generalize to commercial projects.\\n7 Related Work\\nDatabase schema evolution is a timeless research area, with various proposals\\nhow to systematically manage schema changes. Providing tool support, however,\\nis not the scope of this paper. In the following discussion of related work, we\\ntherefore focus on empirical studies on schema evolution in open source projects.\\nIt is only natural that the availability of public code repositories has en-\\nabled empirical studies on relational schema evolution [5,11,13,18–20,22,23,25].\\nAmong their key ﬁndings, these studies show that the schema evolves. They\\nconﬁrm that adding tables or columns are frequent changes. In these settings,\\nthe schema is speciﬁed declaratively (usually in SQL). Accordingly, the term\\nschema modiﬁcation operations (SMOs) [5] does not transfer well to our stack.\\nRather than declarative DDL statements, we need to parse raw Java code: While\\nthe authors of [25] also parse application code, they do so to extract declarative\\nstatements embedded in code.\\nSo far, there are only few empirical studies on schema evolution in NoSQL\\ndata stores. Our work builds on an earlier analysis [14] on the adoption of map-\\nper annotations for lazy schema evolution, which is a diﬀerent focus. The au-\\nthors in [12] present an approach for identifying a schema evolution history in\\nMongoDB-based Java applications. Diﬀerent from us, the authors do not assume\\nthat an object-NoSQL mapper is used to access the data store. Rather, they an-\\nalyze direct calls to the MongoDB API. The schema derived is similar to our\\nnotion of the NoSQL schema, since it captures the perspective of the application\\ncode. The authors evaluated their approach for a single open source project,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='14 Scherzinger and Sidortschuck\\nwhereas our study has a broader basis, considering ten projects. Moreover, their\\ncontribution is to derive a visualization of the schema evolution history.\\nMeanwhile, there is a growing body of work on extracting schema descrip-\\ntions [1, 4, 8] from large collections of JSON data. While this a bottom-up ap-\\nproach, starting from the data, we proceed top-down, analyzing application code.\\nIn capturing schema complexity based on Java class declarations, we could\\nhave resorted to software metrics [10]. However, it is not clear how metrics\\nindicating an overly complex object-oriented design (e.g. classes with many at-\\ntributes) transfer. The practice of building aggregate models in NoSQL schema\\ndesign may actually be orthogonal.\\nWe refer to [7] for a high-level discussion on schema variety versus code\\nvariety, as well as metrics for programmatic schema analysis.\\n8 Conclusion and Outlook\\nIn this paper, we present the study on NoSQL schema evolution with the largest\\ndata basis so far, analyzing ten real-world, open source projects. We track the\\nschema growth as well as the nature of changes to the NoSQL schema. We are\\nable to reproduce most of the insights of related studies on relational schema\\nevolution, but we have also identiﬁed subtle diﬀerences.\\nSince this is a ﬁrst systematic study, many interesting questions remain unan-\\nswered. We remark on two. (1) Originally, we set out to compile detailed statis-\\ntics on the structure of denormalized entity-classes, such as their nesting depth.\\nHowever, we found that Java code written by experienced developers (e.g., as is\\nthe case with Google’s nomulus project) is highly polymorphic. This makes it\\nimpossible to compute reliable statistics based on the static analysis of entity-\\nclass declarations. However, more holistic analysis techniques, such as data ﬂow\\nanalysis of the entire application code, might reveal further insights. (2) We see\\nevidence that the schema evolves, but we do not know the factors that inﬂuence\\nNoSQL schema evolution. This calls for follow-up work, where we take the git\\ncommit messages into account, which often comment the reason for a schema\\nchange. What is also needed are qualitative studies, surveying developers who\\nroutinely deal with NoSQL schema evolution.\\nAcknowledgements This project was funded by the Deutsche Forschungsgemein-\\nschaft (DFG, German Research Foundation), grant number #385808805.\\nReferences\\n1. Baazizi, M., Colazzo, D., Ghelli, G., Sartiani, C.: Parametric schema inference for\\nmassive JSON datasets. VLDB J. 28(4), 497–521 (2019)\\n2. Bellahsene, Z., Bonifati, A., Rahm, E.: Schema Matching and Mapping. Springer\\nPublishing Company, Incorporated, 1st edn. (2011)\\n3. Cerqueus, T., Cunha de Almeida, E., Scherzinger, S.: Safely Managing Data Vari-\\nety in Big Data Software Development. In: Proc. BIGDSE’15 (2015)\\n4. Chilln, A.H., Ruiz, D.S., Molina, J.G., Morales, S.F.: A Model-Driven Approach\\nto Generate Schemas for Object-Document Mappers. IEEE Access 7, 59126–59142\\n(2019)\\n5. Curino, C.A., Tanca, L., Moon, H.J., Zaniolo, C.: Schema evolution in Wikipedia:\\nToward a Web Information System Benchmark. In: Proc. ICEIS’08 (2008)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Design and Evolution of NoSQL Database Schemas 15\\n6. Fowler, M.: Patterns of Enterprise Application Architecture. Addison-Wesley Long-\\nman Publishing Co., Inc., Boston, MA, USA (2002)\\n7. Jain, S., Moritz, D., Howe, B.: High variety cloud databases. In: Proc. ICDE Work-\\nshops 2016 (2016)\\n8. Klettke, M., St¨ orl, U., Scherzinger, S.: Schema Extraction and Structural Outlier\\nDetection for JSON-based NoSQL Data Stores. In: Proc. BTW’15 (2015)\\n9. Kl´ ımek, J., Mal´ y, J., Necask´ y, M., Holubov´ a, I.: eXolutio: Methodology for Design\\nand Evolution of XML Schemas Using Conceptual Modeling. Informatica, Lith.\\nAcad. Sci. 26(3), 453–472 (2015)\\n10. Lanza, M., Marinescu, R.: Object-Oriented Metrics in Practice: Using Software\\nMetrics to Characterize, Evaluate, and Improve the Design of Object-Oriented\\nSystems. Springer Publishing Company, Incorporated, 1st edn. (2010)\\n11. Lin, D.Y., Neamtiu, I.: Collateral Evolution of Applications and Databases. In:\\nProc. IWPSE-Evol’09 (2009)\\n12. Meurice, L., Cleve, A.: Supporting schema evolution in schema-less NoSQL data\\nstores. In: Proc. SANER’17 (2017)\\n13. Qiu, D., Li, B., Su, Z.: An Empirical Analysis of the Co-evolution of Schema and\\nCode in Database Applications. In: Proc. ESEC/FSE’13 (2013)\\n14. Ringlstetter, A., Scherzinger, S., Bissyand´ e, T.F.: Data Model Evolution Using\\nobject-NoSQL Mappers: Folklore or State-of-the-art? In: Proc. BIGDSE’16 (2016)\\n15. Sadalage, P.J., Fowler, M.: NoSQL Distilled: A Brief Guide to the Emerging World\\nof Polyglot Persistence. Addison-Wesley Professional, 1st edn. (2012)\\n16. Sanderson, D.: Programming Google App Engine with Java: Build & Run Scalable\\nJava Applications on Google’s Infrastructure. O’Reilly Media, Inc., 1st edn. (2015)\\n17. Scherzinger, S., Cerqueus, T., Cunha de Almeida, E.: ControVol: A framework for\\ncontrolled schema evolution in NoSQL application development. In: ICDE 2015.\\npp. 1464–1467 (2015)\\n18. Sjøberg, D.: Quantifying schema evolution. Information & Software Technology\\n35(1), 35–44 (1993)\\n19. Skoulis, I., Vassiliadis, P., Zarras, A.V.: Open-Source Databases: Within, Outside,\\nor Beyond Lehman’s Laws of Software Evolution? In: Proc. CAiSE 2014. pp. 379–\\n393 (2014)\\n20. Skoulis, I., Vassiliadis, P., Zarras, A.V.: Growing Up with Stability. Inf. Syst.\\n53(C), 363–385 (Oct 2015)\\n21. Vassiliadis, P., Kolozoﬀ, M., Zerva, M., Zarras, A.V.: Schema evolution and foreign\\nkeys: a study on usage, heartbeat of change and relationship of foreign keys to table\\nactivity. Computing 101(10), 1431–1456 (2019)\\n22. Vassiliadis, P., Zarras, A.V.: Survival in Schema Evolution: Putting the Lives of\\nSurvivor and Dead Tables in Counterpoint. In: Proc. CAiSE 2017. pp. 333–347\\n(2017)\\n23. Vassiliadis, P., Zarras, A.V., Skoulis, I.: How is Life for a Table in an Evolving\\nRelational Schema? Birth, Death and Everything in Between. In: Proc. ER 2015.\\npp. 453–466 (2015)\\n24. Vassiliadis, P., Zarras, A.V., Skoulis, I.: Gravitating to rigidity: Patterns of schema\\nevolution - and its absence - in the lives of tables. Inf. Syst. 63, 24–46 (2017)\\n25. Wu, S., Neamtiu, I.: Schema Evolution Analysis for Embedded Databases. In: Proc.\\nICDEW’11 (2011)\\n26. Xue Li: A survey of schema evolution in object-oriented databases. In: Proceedings\\nTechnology of Object-Oriented Languages and Systems. pp. 362–371 (1999)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 0, 'page_label': '1'}, page_content='Author’s Accepted Manuscript\\nData Modeling in the NoSQL World\\nPaolo Atzeni, Francesca Bugiotti, Luca Cabibbo,\\nRiccardo Torlone\\nPII:\\nS0920-5489(16)30118-0\\nDOI:\\nhttp://dx.doi.org/10.1016/j.csi.2016.10.003\\nReference:\\nCSI3149\\nTo appear in:\\nComputer Standards & Interfaces\\nReceived date:\\n25 March 2016\\nRevised date:\\n30 September 2016\\nAccepted date:\\n6 October 2016\\nCite this article as: Paolo Atzeni, Francesca Bugiotti, Luca Cabibbo and Riccardo\\nTorlone, Data Modeling in the NoSQL World, \\nComputer Standards &\\nInterfaces, \\nhttp://dx.doi.org/10.1016/j.csi.2016.10.003\\nThis is a PDF file of an unedited manuscript that has been accepted for\\npublication. As a service to our customers we are providing this early version of\\nthe manuscript. The manuscript will undergo copyediting, typesetting, and\\nreview of the resulting galley proof before it is published in its final citable form.\\nPlease note that during the production process errors may be discovered which\\ncould affect the content, and all legal disclaimers that apply to the journal pertain.\\nwww.elsevier.com'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 1, 'page_label': '2'}, page_content='Data Modeling in the NoSQL World ✩\\nPaolo Atzenia, Francesca Bugiottib,L u c aC a b i b b oa, Riccardo Torlonea\\naUniversit`aR o m aT r e\\nbCentraleSup´elec\\nAbstract\\nNoSQL systems have gained their popularity for many reasons, including the\\nﬂexibility they provide in organizing data, as they relax the rigidity provided by\\nthe relational model and by the other structured models. This ﬂexibility and\\nthe heterogeneity that has emerged in the area have led to a little use of tradi-\\ntional modeling techniques, as opposed to what has happened with databases\\nfor decades.\\nIn this paper, we argue how traditional notions related to data modeling\\ncan be useful in this context as well. Speciﬁcally, we propose NoAM (NoSQL\\nAbstract Model), a novel abstract data model for NoSQL databases, which ex-\\nploits the commonalities of various NoSQLsystems. We also proposea database\\ndesign methodology for NoSQL systems based on NoAM, with initial activities\\nthat are independent of the speciﬁc target system. NoAM is used to specify a\\nsystem-independent representation of the application data and, then, this inter-\\nmediate representation can be implemented in target NoSQL databases, taking\\ninto account their speciﬁc features. Overall, the methodology aims at support-\\ning scalability, performance, and consistency, as needed by next-generation web\\napplications.\\nKeywords: Data models, database design, NoSQL systems\\n✩This paper extends a short article appeared in the Proceedings of the 33rd International\\nConference on Conceptual Modeling (ER 2014) with the title Database Design for NoSQL\\nSystems [1].\\nPreprint submitted to Journal of L ATEX Templates October 15, 2016'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}, page_content='1. Introduction\\nNoSQL database systems are today an eﬀective solution to manage large\\ndata sets distributed over many servers. A primary driver of interest in No-\\nSQL systems is their support for next-generation Web applications, for which\\nrelational DBMSs are not well suited. These are simple OLTP applications for5\\nwhich (i) data have a structure that does not ﬁt well in the rigid structure of\\nrelational tables, (ii) access to data is based on simple read-write operations,\\n(iii) relevant quality requirements include scalability and performance, as well\\nas a certain level of consistency [2, 3].\\nNoSQL technology is characterized bya high heterogeneity; indeed, more\\n10\\nthan ﬁfty NoSQL systems exist [4], each with diﬀerent characteristics. They can\\nbe classiﬁed into a few main categories [2], including key-value stores, document\\nstores, and extensible record stores. In any case, this heterogeneity is highly\\nproblematic to application developers [4], even within each category.\\nBeside the diﬀerences between the various systems, NoSQL datastores ex-\\n15\\nhibit an additional phenomenon: they usually support signiﬁcant ﬂexibility in\\ndata, with limited (if any) use of the notion of schema as it is common in\\ndatabases. So, the organization of data, and their regularity, is mainly hard-\\ncoded within individual applications and is not exposed, probably because there\\nis little need for sharing data between applications. Indeed, the notion of20\\nschema, and the need for a separation between data and programs, were moti-\\nvated in databases by the need for sharing data between applications. If this\\nrequirement does not hold any longer, many developers are led to believe that\\nthe importance of schemas gets reduced or even disappears.\\nAs the idea of data model is usually tightly related to that of schema, this\\n25\\n“schemaless” point view may lead to claim that the very notion of model and of\\nmodeling activities becomes irrelevant with respect to NoSQL databases. The\\ngoal of this paper is to argue that models and modeling do have an interesting\\nrole in this area. Indeed, modeling is an abstraction process, and this helps in\\ngeneral and probably even more in a world of diversity, as the analyst/designer30\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4'}, page_content='can reason at a high level, before delving into the details of the speciﬁc systems.\\nInstead, given the variety of systems, it is currently the case that the design\\nprocess for NoSQL applications is mainly based on best practices and guide-\\nlines [5], which are speciﬁcally related to the selected system [6, 7, 8], with no\\nsystematic methodology. Several authors have observed that the development35\\nof high-level methodologies and tools supporting NoSQL database design are\\nneeded [9, 10, 11], and models here are deﬁnitely needed, in order to achieve\\nsome level of generality.\\nLet us recall the various reasons for which modeling is considered important\\nin database design and development [12]. First of all, beside being crucial\\n40\\nin the conceptual and logical design phases, it oﬀers support throughout the\\nlifecycle, from requirement analysis, where it helps in giving a structure to the\\nprocess,tocodingandmaintenance, wher eit givesvaluabledocumentation. The\\nmain point to be mentioned is that modeling allows the specialist to describe\\nthe domain of interest and the application from various perspectives and at45\\nvarious levels of abstraction. Moreover, it provides support to communication\\n(and to individual comprehension). Finally, it provides support to performance\\nmanagement, as physical database design is also based on data structures, and\\nquery processing eﬃciency is often basedon reference to the regularity of data.\\nConceptual and logical modeling, as they are currently known, were devel-\\n50\\noped in the database world, with speciﬁc attention to relational systems, but\\nfound applications also in other contexts. Indeed, while the importance of rela-\\ntional databases was clear since the Eighties, it was soon understood that there\\nwere many “non-business” application domains for which other modeling fea-\\ntures were needed: the advocates of object-oriented databases observed, more\\n55\\nor less at the same time, that some requirements were not satisﬁed, such as\\nthose in CAD, CASE, and multimedia and text management [13]. This led\\nto the development of models with nested structures, more complex than the\\nrelational one, and less regular, and so more diﬃcult to manage.\\nFlexibility in structures was also required in another area, which emerged a60\\ndecade later, and has since been very important: the area of Web applications,\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5'}, page_content='where there were at least two kinds of developments concerned with models. On\\nthe one hand, work on complex object models for representing hypertexts [14,\\n15, 16], and on the other hand signiﬁcant development in semistructured data,\\nespecially with reference to XML [17].65\\nAnother recurring claim in the database world in the last ten or ﬁfteen\\nyears has been the fact that, while relational databases are ade factostandard,\\nit is not the case that there is one solution that works well for all kinds of\\napplications. As Stonebraker and C¸etintemel [18] argued, it is not the case that\\n“one size ﬁts all,” and diﬀerent engines and technologies are needed in diﬀerent\\n70\\ncontexts, for example OLAP and OLTP have diﬀerent requirements, but the\\nsame holds for other kinds of applications, such as stream processing, sensor\\nnetworks, or scientiﬁc databases.\\nThe NoSQL movement emerged for a number of motivations, including most\\nof the above, with the goal of supporting highly scalable systems, with speciﬁc\\n75\\nrequirements, usually with very simple operations over many nodes, on sets of\\ndata that have ﬂexible structure. Given that there are many diﬀerent appli-\\ncations and the speciﬁc requirementsvary, many systems have emerged, each\\noﬀeringadiﬀerentwayoforganizingdata andadiﬀerentprogramminginterface.\\nHeterogeneity can become a problem if migration or integration are needed, as\\n80\\nthis is often the case, in a world with changing requirements and new tech-\\nnological developments. Also, the availability of many diﬀerent systems, with\\ndiﬀerent implementations, has led to diﬀerent design techniques, usually related\\njust to individual systems or small families thereof.\\nIn this paper we argue that a model-based approach can be useful to tackle85\\nthe diﬃculties related to heterogeneity, and provide support in the form of\\nabstraction. In fact, modeling can be at the basis of a design process, at various\\nlevel; at a higher one to represent the features of interest for the application,\\nand at a lower one to describe some implementation features in a concrete but\\nsystem-independent way.\\n90\\nIndeed, we will present a high-level data model for NoSQL databases, called\\nNoAM (NoSQL Abstract Model) and show how it can be used as an interme-\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6'}, page_content='diate data representation in the context of a general design methodology for\\nNoSQL applications having initial steps that are independent of the individual\\ntarget system. We propose a design process that includes a conceptual phase,95\\nas common in traditional application, followed (and this is unconventional and\\noriginal) by a system-independent logical design phase, where the intermediate\\nrepresentation is used, as the basis for both modeling and performance aspects,\\nwith only a ﬁnal phase that takes into account the speciﬁc features of individual\\nsystems.\\n100\\nThe rest of the paper is organized as follows. In Section 2, we illustrate\\nthe features of the main categories of NoSQL systems arguing that, for each\\nof them, there exists a sort of data model. In Section 3 we present NoAM,\\nour system-independent data model for NoSQL databases, and in Section 4 we\\ndiscuss our design methodology for NoSQL databases. In Section 5 we brieﬂy\\n105\\nreview some related literature. Finally, in Section 6 we draw some conclusions.\\n2. NoSQL data models\\nIn this section we brieﬂy present and compare a number of representative\\nNoSQL systems, to make apparent the heterogeneity (as well as the similarities)\\nin the way they organize data and in their programming interfaces. We ﬁrst110\\nintroduce a sample application dataset, and then we show how to represent\\nthese data in the representative systems we consider.\\n2.1. Running example\\nLet us consider, as a running example, an application for an on-line social\\ngame. This is indeed a typical scenario in which the use of a NoSQL database\\n115\\nis suitable, that is, a simple next-generation Web application (as discussed in\\nthe Introduction).\\nThe application should manage various types of objects, including players,\\ngames, and rounds. A few representative objects are shown in Figure 1. The\\nﬁgure is a UML object diagram. Boxes and arrows denote objects and relation-120\\nships between them, respectively.\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 6, 'page_label': '7'}, page_content='mary : Player\\nusername = \"mary\"\\nfirstName = \"Mary\"\\nlastName = \"Wilson \"\\nrick : Player\\nusername = \"rick\"\\nfirstName = \"Ricky\"\\nlastName = \"Doe\"\\nscore = 42\\n2345 : Game\\nid = 2345\\nfirstPlayer secondPlayer\\n: GameInfo\\ngames [0]\\ngameopponent\\n: GameInfo\\ngames [0]\\ngame opponent\\n: Round : Round\\nrounds [0] rounds [1]\\n: Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 1: Sample application objects\\nmary : Player\\nusername = \"mary\"\\nfirstName = \"Mary\"\\nlastName = \"Wilson\"\\nrick : Player\\nusername = \"rick\"\\nfirstName = \"Ricky\"\\nlastName = \"Doe\"\\nscore = 42\\n2345 : Game\\nid = 2345\\nfirstPlayer secondPlayer\\n: GameInfo\\ngames [0]\\ngameopponent\\n: GameInfo\\ngames [0]\\ngame opponent\\n: Round : Round\\nrounds[0] rounds [1]\\n: Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 2: Sample aggregates (as groups of objects)\\nTo represent a dataset in a NoSQL database, it is often useful to arrange\\ndata in aggregates [19, 20]. Each aggregate is a group of related application\\nobjects, representing a unit of data access and atomic manipulation. In our\\nexample, relevant aggregates are players and games, as shown by closed curves125\\nin Figure 2. Note that the rounds of a game are grouped within the game itself.\\nIn general, aggregatescan be considered as complex-value objects [21], as shown\\nin Figure 3.\\nThedataaccessoperationsneededbyouro n-linesocialgamearesimpleread-\\nwrite operations on individual aggregates; for example, create a new player and130\\nretrieve a certain game. Other operations involve just a portion of an aggregate;\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 7, 'page_label': '8'}, page_content='Player:mary : ⟨\\nusername : ”mary”,\\nﬁrstName : ”Mary”,\\nlastName : ”Wilson”,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:rick ⟩,\\n⟨ game : Game:2611, opponent : Player:ann ⟩\\n}\\n⟩\\nPlayer:rick : ⟨\\nusername : ”rick”,\\nﬁrstName : ”Ricky”,\\nlastName : ”Doe”,\\nscore : 42,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:mary ⟩,\\n⟨ game : Game:7425, opponent : Player:ann ⟩,\\n⟨ game : Game:1241, opponent : Player:johnny ⟩\\n}\\n⟩\\nGame:2345 : ⟨\\nid : ”2345”,\\nﬁrstPlayer : Player:mary,\\ns\\necondPlayer : Player:rick,\\nrounds : {\\n⟨ moves : ... , comments : ... ⟩,\\n⟨ moves : ... , actions : ..., spell : ... ⟩\\n}\\n⟩\\nFigure 3: Sample aggregates (as complex values)\\nfor example, add a round to an existing game. In general, it is indeed the\\ncase that most real applications require only operations that access individual\\naggregates [2, 22].\\n2.2. NoSQL database models135\\nNoSQL database systems organize their data according to quite diﬀerent\\ndata models. They usually provide simple read-write data-access operations,\\nwhich also diﬀer from system to system. Despite this heterogeneity, a few main\\ncategories can be identiﬁed according to the modeling features of these sys-\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9'}, page_content='tems [2, 3]: key-value stores, document stores, extensible record stores, plus140\\nothers (e.g., graph databases) that are beyond the scope of this paper.\\n2.3. Key-value stores\\nIn general, in akey-value store, a database is a schemaless collection of key-\\nvalue pairs, with data access operations on either individual key-value pairs or\\ngroups of related pairs.145\\nAs a representative key-value store we consider hereOracle NoSQL[23]. In\\nthis system,keys are structured; they are composed of amajor keyand aminor\\nkey. The major key is a non-empty sequence of strings. The minor key is a\\nsequence of strings. Eachelement of a key is called acomponent of the key. On\\nthe other hand, eachvalue is an uninterpreted binary string.150\\nA sample key-value is the pair composed of key/Player/mary/-/username\\nand value ”mary”.I n t h e k e y , s y m b o l ‘/’ separates key components, while\\nsymbol ‘-’ separates the major key from the minor key. The distinction between\\nmajor key and minor is especially relevant to control data distribution and\\nsharding.155\\nIn a pair, the value can be either a simple value (such as the string”mary”)\\nor a complex value. In the former case, it is common to use some data inter-\\nchange format (such as XML, JSON, and Protocol Buﬀers [24]) to represent\\nsuch complex values.\\nOracle NoSQL oﬀers simple atomic access operations, to access and modify\\n160\\nindividual key-value pairs: put(key,value) to add or modify a key value pair\\nand get(key) to retrieve a value, given the key. Oracle NoSQL also provides\\nan atomic multiGet(majorKey) operation to access a group of related key-value\\npairs, and speciﬁcally the pairs having the same major key. Moreover, it oﬀers\\nan execute operation for executing multiple put operations in an atomic and\\n165\\neﬃcient way (provided that the keys speciﬁed in these operations all share a\\nsame major key).\\nThe data representation for a dataset in a key-value store can be based on\\naggregates. These are two common representations for aggregates:\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10'}, page_content='•Represent an aggregate using a single key-value pair. The key (major key)170\\nis the aggregateidentiﬁer. The value is the complex value ofthe aggregate.\\nSee Figure 4(a).\\n•Represent an aggregate using multiple key-value pairs. Speciﬁcally, the\\naggregate is split in parts that need to be accessed or modiﬁed separately,\\nand each part is represented by a distinct but related key-value pair. The175\\naggregateidentiﬁer is usedasmajorkeyfor allthese parts, while the minor\\nkey identiﬁes the part within the aggregate. See Figure 4(b).\\nThe data access operations provided by key-value stores usually enable an ef-\\nﬁcient and atomic data access to aggregates with respect to both data repre-\\nsentations. Indeed, all systems support the access to individual key-value pairs\\n180\\n(useful in the former case) and most of them (such as Oracle NoSQL) provide\\nalso the access to groups of related key-value pairs (required inthe latter case).\\n2.4. Document stores\\nIn adocument store, a database is a set of documents, each having a complex\\nstructure and value.\\n185\\nIn this category, a widely used system isMongoDB [25]. It is an open-source,\\ndocument-oriented data store that oﬀers a full-index support on any attribute,\\na rich document-based query API and Map-Reduce support.\\nIn MongoDB, adatabase comprises one or more collections. Eachcollection\\nis a named group of documents. Eachdocument is a structured document, that\\n190\\nis, a complex value, a set of attribute-value pairs, which can comprise simple\\nvalues, lists, and even nested documents. Thus, documents are neither freeform\\ntext documents nor Oﬃce documents. Documents are schemaless, that is, each\\ndocument can have its own attributes, deﬁned at runtime.\\nSpeciﬁcally, MongoDB documents are based on BSON (Binary JSON), a\\n195\\nvariant of the popular JSON format. Values constituting documents can be of\\nthe following types: (i) basic types, such strings numbers, dates, and boolean\\nvalues; (ii) arrays, i.e., ordered sequences of values; and (iii) documents (or\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11'}, page_content='key (/major/key/-) value\\n/Player/mary/- { username: ”mary”, ﬁrstName: ”Mary”, ... }\\n/Player/rick/- { username: ”rick”, ﬁrstName: ”Ricky”, ... }\\n/Game/2345/- { id: ”2345”, ﬁrstPlayer: ”Player:mary”, ... }\\n(a) Single key-value pair per aggregate\\nkey (/major/key/-/minor/key) value\\nPlayer/mary/-/username ”mary”\\nPlayer/mary/-/ﬁrstName ”Mary”\\nPlayer/mary/-/lastName ”Wilson”\\nPlayer/mary/-/games[0] {game: ”Game:2345”, opponent: ”Player:rick”}\\nPlayer/mary/-/games[1] {game: ”Game:2611”, opponent: ”Player:ann”}\\nPlayer/rick/-/username ”rick”\\nPlayer/rick/-/ﬁrstName ”Ricky”\\nPlayer/rick/-/lastName ”Doe”\\nPlayer/rick/-/score 42\\nPlayer/rick/-/games[0] {game: ”Game:2345”, opponent: ”Player:mary”}\\nPlayer/rick/-/games[1] {game: ”Game:7425”, opponent: ”Player:ann”}\\nPlayer/rick/-/games[2] {game: ”Game:1241”, opponent: ”Player:johnny”}\\nGame/2345/-/id 2345\\nGame/2345/-/ﬁrstPlayer ”Player:mary”\\nGame/2345/-/secondPlayer ”Player:rick”\\nGame/2345/-/rounds[0] {moves : ..., comments: ...}\\nGame/2345/-/rounds[1] {moves : ..., actions: ..., spell: ...}\\n(b) Multiple key-value pairs per aggregate\\nFigure 4: Representing aggregates in Oracle NoSQL\\nobjects): a document is a collection of zero or more key-value pairs, where each\\nkey is a plain string, while each value is of any of these types. Figure 5 shows a200\\nJSON representation of the complex value of a samplePlayer aggregate object\\nof Figures 2 and 3.\\nA main document is a top-level document with a unique identiﬁer, repre-\\nsented by a special attribute\\nid, associated to a value of a special typeObjectId.\\nData access operations are usually over individual documents, which are205\\nunits of data distribution and atomic data manipulation. The basic operations\\noﬀered by MongoDB are as follows:insert(coll,doc) adds a main documentdoc\\ninto collectioncoll;a n dﬁnd(coll,selector) retrieves from collectioncoll all main\\ndocuments matching document selector. The simplest selector is the empty\\ndocument {}, which matches with every document; it allows to retrieve all210\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 11, 'page_label': '12'}, page_content='[\\n\"username\" : \"mary\",\\n\"firstName\" : \"Mary\",\\n\"lastName\" : \"Wilson\",\\n\"games\" : {\\n[ \"id\" : \"Game:2345\", \"opponent\" : \"Player:rick\" ],\\n[ \"id\" : \"Game:2611\", \"opponent\" : \"Player:ann\"]\\n}\\n]\\nFigure 5: The JSON representation of the complex value of a sample Player object\\ncol lection\\n document id\\n document\\nPlayer\\n mary\\n {\"\\n id\":\"mary\", \"username\":\"mary\", \"firstName\":\"Mary\", ... }\\nPlayer\\n rick\\n {\"\\n id\":\"rick\", \"username\":\"rick\", \"firstName\":\"Rock\", ... }\\nGame\\n 2345\\n {\"\\n id\":\"2345\", \"firstPlayer\":\"Player:mary\", ... }\\nFigure 6: Representing aggregates in MongoDB\\ndocuments in a collection. Another useful selector is document{\\n id:ID},w h i c h\\nmatches with the document having identiﬁerID. There is also an operation to\\nupdate a document. Moreover, it is also possible to access or update just a\\nspeciﬁc portion of a document.\\nIn a document store, each aggregate is usually represented by a single main215\\ndocument. The document collection corresponds to the aggregate class (or\\ntype). The document identiﬁer ID is the aggregate identiﬁer. The content of\\nthe document is the complex-value of the aggregate, in JSON/BSON, including\\nalso an additional key-value pair{\\n id:ID} for the identiﬁer. See Figure 6.\\nAlso in this case, the data access operations oﬀered by document stores220\\n(such as MongoDB) provide an atomic and eﬃcient data access to aggregates.\\nSpeciﬁcally, they generally support both operations on individual aggregates, or\\nto speciﬁc portions of them, thereof.\\n2.5. Extensible record stores\\nIn an extensible record store, a database is a set of tables, each table is a225\\nset of rows, and each row contains a set of attributes (or columns), each with a\\nname and a value. Rows in a table are not required to have the same attributes.\\n11'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13'}, page_content='Data access operations are usually overindividual rows, which are units of data\\ndistribution and atomic data manipulation.\\nA representative extensible record store isAmazon DynamoDB [26], a No-230\\nSQL database service provided on the cloud by Amazon Web Services (AWS).\\nIn DynamoDB a database is organized in tables. Atable is a set of items. Each\\nitem contains one or moreattributes,e a c hw i t haname and a value (or a set\\nof values). Each table designates an attribute asprimary key. Items in a same\\ntable are not required to have the same set of attributes — apart from the pri-235\\nmary key, which is the only mandatory attribute of a table. Thus, DynamoDB\\ndatabases are mostly schemaless.\\nSpeciﬁcally, the primary key is composed of apartition keyand an optional\\nsort key. If the primary key of a table includes a sort key, then DynamoDB\\nstores together all the items having the same partition key, in such a way that240\\nthey can be accessed in an eﬃcient way.\\nDistribution is operated at the item level and, for each table, is controlled\\nby the partition key only.\\nSome operations oﬀered by DynamoDB are as follows:putItem(table,key,av)\\nadds (or modiﬁes) a new item in tabletable with primary keykey,u s i n gt h e245\\nset of attribute-value pairsav;a n dgetItem(table,key) retrieves the item of table\\ntable having primary keykey. It is also possible to access or update just a subset\\nof the attributes of an item. All these operations can be executed in an eﬃcient\\nway.\\nIn an extensible record store (such as DynamoDB), each aggregate can be250\\nrepresented by a record/row/item. The table corresponds to the aggregate class\\n(or type). The primary key (partition key) is the aggregate identiﬁer. Then,\\nthe item can have a distinct attribute-value pair for each top-level attribute of\\nthe complex value of the aggregate(or for each major part of the aggregatethat\\nneeds to be accessed separately). See Figure 7.\\n255\\nAgain, the data access operations provided by the systems in this category\\nsupport an eﬃcient data access to aggregates or to speciﬁc portions of them.\\n12'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14'}, page_content='table Player\\nusername ﬁrstName lastName score games[0] games[1] games[2]\\n”mary” ”Mary” ”Wilson” { game: ..., opponent: ... }{ ... }\\n”rick” ”Ricky” ”Doe” 42 { game: ..., opponent: ... }{ ... }{ ... }\\ntable Game\\nid ﬁrstPlayer secondPlayer rounds[0] rounds[1] rounds[2]\\n2345 Player:mary Player:rick { moves : ..., comments: ... }{ ... }\\nFigure 7: Representing aggregates in DynamoDB (abridged)\\n2.6. Comparison\\nTo summarize, it is possible to say that each NoSQL system provides a num-\\nber of “modeling elements” to organize data, which can be considered the “data260\\nmodel” of the system. Moreover, the various systems can be eﬀectively classiﬁed\\nin a few main categories, where each category is based on “data models” that,\\neven though not identical, do share some similarities. In the next section we\\nshow that it is possible to pursue these similarities, thus deﬁning an “abstract\\ndata model” for NoSQL databases.265\\n3. The NoAM data model\\nIn this section we presentNoAM (NoSQL Abstract Data Model), a system-\\nindependent data model for NoSQL databases. In the following section we will\\nalso discuss how this data model can be used to support the design of NoSQL\\ndatabases.270\\nIntuitively, the NoAM data model exploits the commonalities of the data\\nmodeling elements available in the various NoSQL systems and introduces ab-\\nstractions to balance their diﬀerences and variations.\\nA ﬁrst observation is that all NoSQL systems have a data modeling element\\nthat is a data access and distribution unit. By “data access unit” we mean\\n275\\nthat the system oﬀers operations to access and manipulate an individual unit\\nat a time, in an atomic, eﬃcient, and scalable way. By “distribution unit” we\\nmean that each unit is entirely stored in aserver of the cluster, whereas diﬀer-\\nent units are distributed among the various servers. With reference to major\\n13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15'}, page_content='NoSQL categories, this element is: (i) a group of related key-value pairs, in key-280\\nvalue stores; (ii) a document, in document stores; or (iii) a record/row/item, in\\nextensible record stores.\\nIn NoAM, a data access and distribution unit is modeled by ablock. Specif-\\nically, a block represents amaximal data unit for which atomic, eﬃcient, and\\nscalable access operations are provided. Indeed, while the access to an individ-285\\nual block can be performed in an eﬃcient way in the various systems, the access\\nto multiple blocks can be quite ineﬃcient. In particular, NoSQL systems do\\nnot usually provide an eﬃcient “join” operation. Moreover, most NoSQL sys-\\ntems provide atomic operations only over single blocks and do not support the\\natomic manipulation of a group of blocks. For example, MongoDB [25] provides\\n290\\nonly atomic operations over individual documents, whereas Bigtable does not\\nsupport transactions across rows [22].\\nA second common feature of NoSQL systems is the ability to access and\\nmanipulate just a component of a data access unit (i.e., of a block). This\\ncomponent is: (i) an individual key-value pair, in key-value stores; (ii) a ﬁeld,\\n295\\nin document stores; or (iii) a column, in extensible record stores. In NoAM,\\nsuch a smaller data access unit is called anentry.\\nFinally, most NoSQL databases providea notion of collection of data access\\nunits. For example, a table in extensiblerecord stores or a document collection\\nin document stores. In NoAM, a coll ection of data access units is called a300\\ncollection.\\nAccording to the above observations, the NoAM data model is deﬁned as\\nfollows.\\n•AN o A Mdatabase is a set ofcollections. Each collection has a distinct\\nname.305\\n•A collection is a set ofblocks. Each block in a collection is identiﬁed by a\\nblock key, which is unique within that collection.\\n•A block is a non-empty set ofentries. Each entry is a pair⟨ek,ev⟩,w h e r e\\nek is theentry key (which is unique within its block) andev is its value\\n14'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 15, 'page_label': '16'}, page_content='Player\\nmary\\nusername\\n ”mary”\\nﬁrstName\\n ”Mary”\\nlastName\\n ”Wilson”\\ngames[0]\\n ⟨ game : Game:2345, opponent : Player:rick ⟩\\ngames[1]\\n ⟨ game : Game:2611, opponent : Player:ann ⟩\\nrick\\nusername\\n ”rick”\\nﬁrstName\\n ”Ricky”\\nlastName\\n ”Doe”\\nscore\\n 42\\ngames[0]\\n ⟨ game : Game:2345, opponent : Player:mary ⟩\\ngames[1]\\n ⟨ game : Game:7425, opponent : Player:ann ⟩\\ngames[2]\\n ⟨ game : Game:1241, opponent : Player:johnny ⟩\\nGame\\n2345\\nid\\n 2345\\nﬁrstPlayer\\n Player:mary\\nsecondPlayer\\n Player:rick\\nrounds[0]\\n ⟨ moves : ..., comments : ... ⟩\\nrounds[1]\\n ⟨ moves : ..., actions : ..., spell : ... ⟩\\nFigure 8: A sample database in NoAM\\n(either complex or scalar), called theentry value.310\\nFor example, Figure 8 shows a possible representation of the aggregates\\nof Figures 2 and 3 in terms of the NoAM data model. There, outer boxes\\ndenote blocks representing aggregates, while inner boxes show entries. Note\\nthat entry values can be complex, being this another commonality of various\\nNoSQL systems.315\\nPlease note that the same data can be usually represented in diﬀerent ways.\\nCompare, for example, Figure 8 with Figure 9. We will discuss this possibility\\nin the next section.\\nIn summary, NoAM describes in a uniform way the features of many NoSQL\\nsystems, and so can be eﬀectively used,as we show in the next section, for an\\n320\\n15'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 16, 'page_label': '17'}, page_content='Player\\nmary\\n ϵ\\n⟨username:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:rick ⟩,\\n⟨ game : Game:2611, opponent : Player:ann ⟩\\n}⟩\\nrick\\n ϵ\\n⟨username:”rick”,\\nﬁrstName:”Ricky”,\\nlastName:”Doe”,\\nscore:42,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:mary ⟩,\\n⟨ game : Game:7425, opponent : Player:ann ⟩,\\n⟨ game : Game:1241, opponent : Player:johnny ⟩\\n}⟩\\nGame\\n2345\\n ϵ\\n⟨id : ”2345”,\\nﬁrstPlayer : Player:mary,\\nsecondPlayer : Player:rick,\\nrounds : {\\n⟨ moves :..., comments : ... ⟩,\\n⟨ moves :..., actions : ..., spell : ... ⟩\\n}⟩\\nFigure 9: Another NoAM sample database\\nintermediate representation in a NoSQL database design methodology.\\n4. System-independent design of NoSQL databases with NoAM\\nThe main goal of NoAM is to support a design methodology for NoSQL\\ndatabases that has initial activities that are independent of the speciﬁc tar-\\nget system. In particular, NoAM is used to specify an intermediate, system-325\\nindependent representation of the application data. The implementation in a\\ntarget NoSQL system is then a ﬁnal step, with a translation that takes into\\naccount its peculiarities.\\n16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18'}, page_content='The motivations to consider database design for NoSQL systems are as fol-\\nlows. It is important to notice that despite the fact that NoSQL databases330\\nare claimed to be “schemaless,” the data of interest for applications do show\\nsome structure, which should be mapped to the modeling elements (collections,\\ntables, documents, key-value pairs) available in the target system. Moreover,\\ndiﬀerent alternatives in the organization of data in a NoSQL database are usu-\\nally possible, but they are not equivalent in supporting qualities such as perfor-335\\nmance, scalability, and consistency (which are typically required when a NoSQL\\ndatabase is adopted). For example, a “wrong” database representation can lead\\nto performance that are worse by an order of magnitude as well as to the in-\\nability to guarantee atomicity of important operations.\\nSpeciﬁcally, our design methodology has the goal of designing a “good” rep-\\n340\\nresentation of the application data in a target NoSQL database, and is intended\\nto support major qualities such as performance, scalability, and consistency, as\\nneeded by next-generation Web applications.\\nThe NoAM approach is based on the following main activities:\\n•conceptual data modeling and aggregate design, to identify the various345\\nentities and relationships thereof needed in an application, and to group\\nrelated entities into aggregates;\\n•aggregate partitioning and high-level NoSQL database design, where ag-\\ngregates are partitioned into smaller data elements and then mapped to\\nthe NoAM intermediate data model;350\\n•implementation, to map the intermediate data representation to the spe-\\nciﬁc modeling elements of a target datastore.\\nIn this approach, only the implementation depends on the target datastore.\\nWe will discuss the various steps of this approach in the rest of this section.\\n4.1. Conceptual modeling and aggregate design355\\nThe methodology starts, as it is usual in database design, by building a con-\\nceptual representation of the data of interest, in terms of entities, relationships,\\n17'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19'}, page_content='and attributes. (This activity is discussed in most database textbooks, e.g.,\\n[12].) Following Domain-Driven Design (DDD [19]), which is a widely followed\\nobject-oriented methodology, we assume that the outcome of this activity is a360\\nconceptual UML class diagram deﬁning the entities, value objects, and relation-\\nships of the application. Anentity is a persistent object that has independent\\nexistence and is distinguished by a unique identiﬁer (e.g., a player or a game,\\nin our running example). Avalue objectis a persistent object which is mainly\\ncharacterized by its value, without an own identiﬁer (e.g., a round or a move).\\n365\\nThen, the methodology proceeds by identifying aggregates.\\nThe design of aggregates has the goal of identifying the classes of aggregates\\nfor an application, and various approaches are possible. After the preliminary\\nconceptual design phase, entities and value objects are grouped into aggregates.\\nEach aggregate has an entity as its root, and it can also contain many value370\\nobjects. Intuitively, an entity and a group of value objects are used to deﬁne an\\naggregate having a complex structure and value.\\nThe relevant decisions in aggregate design involve the choice of aggregates\\nand of their boundaries. This activity can be driven by the data access pat-\\nterns of the application operations, as well as by scalability and consistency\\n375\\nneeds [19]. Speciﬁcally, aggregates should be designed as the units on which\\natomicity must be guaranteed [20] (with eventual consistency for update op-\\nerations spanning multiple aggregates [27]). In general, it is indeed the case\\nthat most real applications require only operations that access individual aggre-\\ngates [2, 22]. Eachaggregateshould be large enough so as to include all the data380\\nrequired by a relevant data access operation. (Please note that NoSQL systems\\ndo not provide a “join” operation, and this is a main motivation for clustering\\neach group of related application objects into an aggregate.) Furthermore, to\\nsupport strong consistency (that is, atomicity) of update operations, each ag-\\ngregate should include all the data involved by some integrity constraints or\\n385\\nother forms of business rules [28]. On the other hand, aggregates should be as\\nsmall as possible; smallaggregates reduce concurrency collisions and support\\nperformance and scalability requirements [28].\\n18'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20'}, page_content='Thus, aggregate design is mainly driven by data access operations. In our\\nrunning example, the online game application needs to manage various collec-390\\ntions of objects, including players, games, and rounds. Figure 2 shows a few\\nrepresentativeapplication objects. (There, boxes and arrowsdenote objects and\\nlinks between them, respectively. An object having a colored top compartment\\nis an entity, otherwise it is a value object.) When a player connects to the\\napplication, all data on the player should be retrieved, including an overview395\\nof the games she is currently playing.Then, the player can select to continue\\na game, and data on the selected game should be retrieved. When a player\\ncompletes a round in a game she is playing, then the game should be updated.\\nThese operations suggest that the candidate aggregate classes are players and\\ngames. Figure 2 also shows how application objects can be grouped in aggre-400\\ngates. (There, a closed curve denotes the boundary of an aggregate.)\\nAs we mentioned above,aggregatedesign is alsodriven by consistency needs.\\nAssume that the application should enforce a rule specifying that a round can\\nbe added to a game only if some condition that involves the other rounds of the\\ngame is satisﬁed. An individual round cannot check, alone, the above condition;\\n405\\ntherefore, it cannot be an aggregate by itself. On the other hand, the above\\nbusiness rule can be supported by a game (comprising, as an aggregate, its\\nrounds).\\nIn conclusion, the aggregate classes for our sample application arePlayer\\nand Game, as shown in Figures 2 and 3.\\n410\\n4.2. Data representation in NoAM and aggregate partitioning\\nIn our approach, we use the NoAM data model (Section 3) as an intermedi-\\nate model between application aggregates (Section 4.1) and NoSQL databases\\n(Section 2). We represent each class of aggregates by means of a distinct col-\\nlection, and each individual aggregate by means of a block. We use the class415\\nname to name the collection, and the identiﬁer of the aggregate as block key.\\nThe complex value of each aggregate is represented by a set of entries in the\\ncorresponding block. For example, the aggregates of Figures 2 and 3 can be\\n19'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21'}, page_content='represented by the NoAM database shown in Figure 8. The representation of\\naggregates as blocks is motivated by the fact that both concepts represent a420\\nunit of data access and distribution, but at diﬀerent abstraction levels. Indeed,\\nNoSQL systems provide eﬃcient, scalable, and consistent (i.e., atomic) opera-\\ntions on blocks and, in turn, this choice propagates such qualities to operations\\non aggregates.\\nIn general, an application dataset of aggregates can be represented in NoAM425\\ndatabase in several diﬀerent ways. Eachdata representationfor a datasetδis a\\nNoAMdatabase Dδrepresentingδ. Speciﬁcally, thevariousdatarepresentations\\nfor a dataset diﬀer only in the choice of the entries used to represent the complex\\nvalue of each aggregate. We ﬁrst discuss basic data representation strategies,\\nwhich we illustrate with respect to the example described in Figure 3. We then\\n430\\nintroduce additional and more ﬂexible data representations.\\nA simple data representation strategy, called Entry per Aggregate Object\\n(EAO), represents each individual aggregate using a single entry. The entry\\nkey is empty. The entry value is the whole complex value of the aggregate. The\\ndata representation of the aggregates of Figure 3 according to the EAO strategy435\\nis shown in Figure 9.\\nAnotherdatarepresentationstrategy,called Entry per Top-level Field(ETF),\\nrepresents each aggregate by means of multiple entries, using a distinct entry\\nfor each top-level ﬁeld of the complex value of the aggregate. For each top-level\\nﬁeld f of an aggregateo, it employs an entry having as value the value of ﬁeldf440\\nin the complex value ofo (with values that can be complex themselves), and as\\nkey the ﬁeld namef. Figure 10 shows the data representation of the aggregates\\nof Figure 3 according to the ETF strategy.\\nAs a comparison, we can observe that the EAO data representation uses a\\nblock with a single entry to represent thePlayerobject having usernamemary,445\\nwhile the ETF representation needs a block with four entries, corresponding to\\nﬁelds username, ﬁrstName, lastName,a n dgames. Moreover, blocks in EAO\\ndo not depend on the structure of aggregates, while blocks in ETF depend on\\nthe top-level structure of aggregates (which can be “almost ﬁxed” within each\\n20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 21, 'page_label': '22'}, page_content='Player\\nmary\\nusername\\n ”mary”\\nﬁrstName\\n ”Mary”\\nlastName\\n ”Wilson”\\ngames\\n{⟨ game: Game:2345, opponent: Player:rick ⟩,\\n⟨ game: Game:2611, opponent: Player:ann ⟩}\\nrick\\nusername\\n ”rick”\\nﬁrstName\\n ”Ricky”\\nlastName\\n ”Doe”\\nscore\\n 42\\ngames\\n{⟨ game: Game:2345, opponent: Player:mary ⟩,\\n⟨ game: Game:7425, opponent: Player:ann ⟩,\\n⟨ game: Game:1241, opponent: Player:johnny ⟩}\\nGame\\n2345\\nid\\n 2345\\nﬁrstPlayer\\n Player:mary\\nsecondPlayer\\n Player:rick\\nrounds\\n{⟨ moves: ..., comments: ..., ⟩\\n⟨ moves: ..., actions: ..., spell: ... ⟩}\\nFigure 10: The ETF data representation\\nclass).450\\nThe general data representation strategies we just described can be suited in\\nsome cases, but they are often too rigid and limiting. For example, none of the\\nabove strategies leads to the data representation shown in Figure 8. The main\\nlimitation of such generaldata representations is that they refer only to the\\nstructure of aggregates, and do not take into account the data access patterns\\n455\\nof the application operations. Therefore, these strategies are not usually able to\\nsupport the performance of these operations. This motivates the introduction\\nof aggregate partitioning.\\nWe ﬁrst need to introduce a preliminary notion ofaccess path, to specify a\\n“location”in the structure ofa complexvalue. Intuitively, ifv is acomplex value460\\nand w is a value (possibly complex as well) occurring inv, then the access path\\n21'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23'}, page_content='ap for w in v represents the sequence of “steps” that should be taken to reach\\nthe component valuew in v. More precisely, an access pathap is a (possibly\\nempty) sequence ofaccess steps, ap = p1 p2 ...p n, where each steppi identiﬁes\\nac o m p o n e n tv a l u ei nas t r u c tured value. Furthermore, ifv is a complex value465\\nand ap is an access path, thenap(v) denotes the component value identiﬁed by\\nap in v.\\nFor example, consider the complex valuevmary of the Player aggregate\\nhaving username mary shown in Figure 3. Examples of access paths for this\\ncomplex value areﬁrstNameand games[0].opponent. If we apply these access470\\npaths tovmary, we access valuesMary and Player:rick, respectively.\\nA complex valuev can be represented using a set of entries, whose keys are\\naccess paths forv. Each entry is intended to represent a distinct portion of the\\ncomplex valuev, characterized by a location in its structure (the access path,\\nused as entry key) and a value (the entry value). Speciﬁcally, in NoAM we475\\nrepresent each aggregate by means of apartition of its complex valuev,t h a ti s ,\\nas e tE of entries that fully coverv, without redundancy. Consider again the\\ncomplex valuevmary shown in Figure 3; a possible entry forvmary is the pair\\n⟨games[0].opponent, Player:rick⟩. We have already applied the above intuition\\nearlier in this section. For example, the ETF data representation (shown in480\\nFigure 10) uses ﬁeld names as entry keys (which are indeed a case of access\\np a t h s )a n dﬁ e l dv a l u e sa se n t r yv a l u e s .\\nAggregate partitioning can be based on the following guidelines (which are a\\nvariant of guidelines proposed in [12] in the context of logical database design):\\n•If an aggregate is small in size, or all or most of its data are accessed or485\\nmodiﬁed together, then it should be represented by a single entry.\\n•Conversely, an aggregate should be partitioned in multiple entries if it is\\nlarge in size and there are operationsthat frequently access or modify only\\nspeciﬁc portions of the aggregate.\\n•T w oo rm o r ed a t ae l e m e n t ss h o u l db e l o n gt ot h es a m ee n t r yi ft h e ya r e490\\nfrequently accessed or modiﬁed together.\\n22'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24'}, page_content='Game\\n2345\\nϵ\\n⟨ id:2345,\\nﬁrstPlayer:Player:mary,\\nsecondPlayer:Player:rick ⟩\\nrounds[0]\\n ⟨ moves :. . . , comments :. . . ⟩\\nrounds[1]\\n ⟨ moves :. . . , actions :. . . , spell :. . . ⟩\\nFigure 11: An alternative data representation for games ( Rounds)\\n•Two or more data elements should belong to distinct entries if they are\\nusually accessed or modiﬁed separately.\\nThe applicationof the above guidelines suggests a partitioning of aggregates,\\nwhich we will use to guide the representation in the target database.495\\nFor example, in our sample application, consider the operations involving\\ngames and rounds. When a player selects to continue a game, data on the\\nselected game should be retrieved. When a player completes a round in a game\\nshe is playing, then the aggregate for the game should be updated. To support\\nperformance, it is desirable that this update is implemented in the database\\n500\\njust as an addition of a round to a game, rather than a complete rewrite of the\\nwhole game. Thus, data for each individual round is always read or written\\ntogether. Moreover, data for the various rounds of a game are read together,\\nbut each round is written separately. Therefore, each roundis a candidate to\\nbe represented by an autonomous entry. These observations lead to a data\\n505\\nrepresentation for games shown in Figure 8. However, apart from rounds, the\\nremaining data for each game comprises just a few ﬁelds, which can be therefore\\nrepresented together in a single entry. This further observation leads to an\\nalternative data representation for games, shown in Figure 11.\\n4.3. Implementation\\n510\\nWe now discuss how a NoAM data representation can be implemented in\\na target NoSQL database. Given that NoAM generalizes the features of the\\nvarious NoSQL systems, while keeping their major aspects, it is rather straight-\\nforward to perform this activity. We have implementations for various NoSQL\\n23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25'}, page_content='systems, including Cassandra, Couchbase, Amazon DynamoDB, HBase, Mon-515\\ngoDB, Oracle NoSQL, and Redis. For the sake of space, we discuss the im-\\nplementation only with respect to a single representative system for each main\\nNoSQL category. Moreover, with reference to the same aggregate objects of\\nFigures 2 and 3 we will sometimes show only the data for one aggregate. Sim-\\nilar representations can be obtained for the other aggregates of the running\\n520\\nexample.\\n4.3.1. Key-value store: Oracle NoSQL\\nIn the key-value store Oracle NoSQL [23] (Section 2.3), a data representa-\\ntion D for an application dataset can be implemented as follows. We use a\\nkey-value pair for each entry⟨ek,ev⟩ in D. The major key is composed of the525\\ncollection name C and the block keyid, while the minor key is a proper cod-\\ning of the entry keyek (recall that ek is an access path, which we represent\\nusing a distinct key component for each of its steps). An example of key is\\n/Player/mary/-/ﬁrstName,w h e r es y m b o l/ separates components, and symbol\\n- separates the major key from the minor key. The value associated with this530\\nkey is a representation of the entry valueev; for example,Mary.T h ev a l u ec a n\\nbe either simple or a serialization of a complex value, e.g., in JSON.\\nThe retrieval of a block can be implemented, in an eﬃcient and atomic way,\\nusing a singlemultiGet operation — this is possible because all the entries of a\\nblock share the same major key. The storage of a block can be implemented535\\nusing variousput operations. These multipleput operations can be executed in\\nan atomic way — since, again, all the entries of a block share the same major\\nkey.\\nFor example, Figure 4(b) shows the implementation in Oracle NoSQL of the\\ndata representation of Figure 8. Moreover, Figure 4(a) shows the implementa-\\n540\\ntion in Oracle NoSQL of the EAO data representation of Figure 9.\\nAnimplementationcanbeconsidered eﬀectiveifaggregatesareindeedturned\\ninto units of data access and distribution.The eﬀectiveness of our implementa-\\ntion is based on the use we make of Oracle NoSQL keys, where the major key\\n24'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26'}, page_content='controls distribution (sharding is based on it) and consistency (an operation in-545\\nvolving multiple key-value pairs can be executed atomically only if the various\\npairs are over a same major key).\\nMore precisely, a technical precaution is needed toguarantee atomic con-\\nsistency when the selected data representation uses more than one entry per\\nblock. Consider two separate operations that need to update just a subset of550\\nthe entries of the block for an aggregateobject. Since aggregatesshould be units\\nof atomicity and consistency, if these operations are requested concurrently on\\nthe same aggregate object, then the application would require that the NoSQL\\nsystem identiﬁes a concurrency collision, commits only one of the operations,\\nand aborts the other. However, if the operations update twodisjoint subsets\\n555\\nof entries, then Oracle NoSQL is unable to identify the collision, since it has\\nno notion of block. We support this requirement, thus providing atomicity and\\nconsistency over aggregates, by always including in each update operation the\\naccess to the entry that includes the identiﬁer of the aggregate (or some other\\ndistinguished entry of the block).\\n560\\n4.3.2. Extensible record store: DynamoDB\\nIn the extensible record store Amazon DynamoDB ([26], Section 2.5), the\\nimplementation of a NoAM database can be based on a distinct table for each\\ncollection, and a single item for each block. The item contains a number of\\nattributes, which can be deﬁned from the entries of the block for the item.565\\nA NoAM datarepresentationD can be representedin DynamoDB as follows.\\nConsider a blockb in a collectionC having block keyid. According toD,o n e\\nor multiple entries are used within each block. We use all the entries of a block\\nb to create a new item in a table forb. Speciﬁcally, we proceed as follows: (i)\\nthe collection nameC is used as a DynamoBD table name; (ii) the block key\\n570\\nid is used as a DynamoBD primary key in that table; (iii) the set of entries\\n(key-value pairs) of a blockb is used as the set of attribute name-value pairs\\nin the item forb (a serialization of the values is used, if needed). For example,\\nFigure 7 shows the implementation of the NoAM database of Figure 8.\\n25'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 26, 'page_label': '27'}, page_content='col lection Player\\nid\\n document\\nmary\\n{\\nid:”mary”,\\nusername:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames:\\n[ { game:”Game:2345”, opponent: ”Player:rick”},\\n{ game:”Game:2611”, opponent: ”Player:ann”} ]\\n}\\nFigure 12: Implementation in MongoDB\\nThe retrieval ofa block, given its collectionC and block keyid,c a nb ei m p l e -575\\nmented by performing a singlegetItem operation, which retrieves the item that\\ncontains all the entries of the block. The storage of a block can be implemented\\nusing a putItem operation, to save all the entries of the block, in an atomic way.\\nIt is worth noting that, using operationgetItem,i ti sa l s op o s s i b l et or e t r i e v ea\\nsubset of the entries of a block. Similarly, using operationupdateItem, it is also580\\npossible to update just a subset of the entries of a block, in an atomic way.\\nThis implementation is also eﬀective, since DynamoDB controls distribution\\nand atomicity with reference to items.\\n4.3.3. Document store: MongoDB\\nIn MongoDB ([29], Section 2.4), which is a document store, a natural imple-585\\nmentation for a NoAM database can be based on a distinct MongoDB collection\\nfor each collection of blocks, and a single main document for each block. The\\ndocument for a blockb can be deﬁned as a suitable JSON/BSON serialization\\nof the complex value of the entries inb, plus a special ﬁeld to store the block\\nkey id of b, as required by MongoDB,{\\n id:id}.590\\nWith reference to a NoAM data representationD, consider a blockb in a\\ncollectionC havingblockkey id.I fbcontainsjust anentry e, then the document\\nfor b is just a serialization ofe.O t h e r w i s e ,i fb contains multiple entries, we use\\nall the entries in blockb to create a new document. Speciﬁcally, we proceed by\\n26'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28'}, page_content='col lection Player\\nid\\n document\\nmary\\n{\\nid:”mary”,\\nusername:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames[0]: { game:”Game:2345”, opponent: ”Player:rick” },\\ngames[1]: { game:”Game:2611”, opponent: ”Player:ann” }\\n}\\nFigure 13: Alternative implementation in MongoDB\\nbuilding a documentd for b as follows: (i) the collection nameC is used as the595\\nMongoDB collection name; (ii) the block keyid is used for the special top-level\\nid ﬁeld{\\n id:id} of d; (iii) then, each entry in the blockb is used to ﬁll a (possibly\\nnested) ﬁeld of documentd. See Figure 12.\\nTheretrievalofablock, givenitscollection C andkey id, canbeimplemented\\nby performing aﬁnd operation, to retrieve the main document that represents600\\nall the block (with its entries). The storage of a block can be implemented using\\nan insert operation, which saves the whole block (with its entries), in an atomic\\nway. It is worth noting that, using other MongoDB operations, it is also possible\\nto access and update just a subset of the entries of a block, in an atomic way.\\nAn alternative implementation for MongoDB is as follows. Each block b\\n605\\nis represented, again, as a main document forb, but using a distinct top-level\\nﬁeld-value pair for each entry in the NoAM data representation. In particular,\\nfor each entry (ek,ev), the document forb contains a top-level ﬁeld whose name\\nis a coding for the entry key (access path)ek, and whose value is either an\\natomic value or an embedded document that serializes the entry valueev.F o r610\\nexample, according to this implementation, the data representation of Figure 8\\nleads to the result shown in Figure 13.\\n4.4. Experiments\\nWe will now discuss a case study of NoSQL database design, with refer-\\nence to our running example. For the sake of simplicity, we just focus on the\\n615\\n27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29'}, page_content='representation and management of aggregates for games.\\nData for each game include a few scalar ﬁelds and a collection of rounds.\\nThe important operations over games are: (1) the retrieval of a game, which\\nshould read all the data concerning the game; and (2) the addition of a round\\nto a game.620\\nAssume that, to manage games, we have chosen a key-value store as the\\ntarget system. The candidate data representations are: (i) using a single entry\\nfor each game (as shown in Figure 9, in the following calledEAO); (ii) splitting\\nthe data for each game in a group of entries, one for each round, and including\\nall the remaining scalar ﬁelds in a separate entry (as shown in Figure 11, called\\n625\\nRounds).\\nWe expect that the ﬁrst operation (retrieval of a game) performs better in\\nEAO, since it needs to read just a key-valuepair, while the second one (addition\\nof a round to a game) is favored byRounds, which does not require to rewrite\\nthe whole game.630\\nWe ran a number of experiments to compare the above data representations\\nin situations of diﬀerent application workloads. Each game has, on average, a\\ndozen rounds, for a total of about 8KB per game. At each run, we simulated\\nthe following workloads: (a) game retrievals only (in random order); (b) round\\nadditions only (to random games); and (c) a mixed workload, with game re-\\n635\\ntrieval and round addition operations, with a read/write ratio of 50/50. We ran\\nthe experiments using diﬀerent database sizes, and measured the running time\\nrequiredby the workloads. The targetsystem was Oracle NoSQL,deployed over\\nAmazon AWS on a cluster of four EC2 servers.\\n1\\nThe results are shown in Figure 14. Database sizes are in gigabytes, timings640\\nare in milliseconds, and points denote the average running time of a single op-\\neration. The experiments conﬁrm the intuition that the retrieval of games (Fig-\\nure 14(a)) is always favored by theEAO data representation, for any database\\nsize. On the other hand, the addition of a round to an existing game (Fig-\\n1 This activity was supported by A WS in Education Grant award.\\n28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 29, 'page_label': '30'}, page_content='0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nGame Retrieval\\nEAO Rounds\\n(a) Game Retrieval\\n0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nRound Addition\\nEAO Rounds\\n(b) Round Addition\\n0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nMixed Load (50/50)\\nEAO Rounds\\n(c) Mixed Load\\nFigure 14: Experimental results\\nure 14(b)) is favored by theRounds data representation. Finally, the exper-645\\niments over the mixed workload (Figure 14(c)) show a general advantage of\\nRounds over EAO, which however decreases as the database size increases.\\nOverall, it turns out that theRounds data representation is preferable.\\nWe also performed other experiments on a data representation that does\\nnot conform to the design guidelines proposed in this paper. Speciﬁcally, a data650\\nrepresentation that divides the rounds of a game into independent key-value\\npairs, rather than keeping them together in a same block, as suggested by our\\napproach. In this case, the performance of the various operations worsens by at\\nleast an order of magnitude. Moreover, with this data representation it is not\\npossible to update a game in an atomic way.\\n655\\nOverall, these experiments show that: (i) the design of NoSQL databases\\nshould be done with care as it aﬀects considerably the performance and consis-\\n29'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31'}, page_content='tency of data access operations, and (ii) our methodology provides an eﬀective\\ntool for choosing among diﬀerent alternatives.\\n5. Related works660\\nAlthough several authors have observed that there is a need for data-model\\napproaches to the design and management of NoSQL databases [9, 10, 11],\\nvery few works have addressed this issue,especially from a general and system-\\nindependent point of view. Indeed, most of them propose a solution to a speciﬁc\\nproblem in a limited scenario.\\n665\\nFor instance, Pasqualin et al. have recently shown how a document-oriented\\nmodel can be eﬃciently implemented in a NoSQL document store [30]. Sim-\\nilarly, Olivera et al. [31] and de Lima and Mello [32] have proposed a data-\\nmodel based methodology for the design of NoSQL document database [32],\\nwhereas Chevalier et al. have addressed the speciﬁc problem of leveraging on\\n670\\na document-oriented model for implementing a multidimensional database in a\\nNoSQL document store [33] and in a column-oriented NoSQL database [34].\\nMost of the other contributions to data modeling for NoSQL systems come\\nfrom on-line papers, usually published in blogs of practitioners, that discuss\\nbest practices and guidelines for modeling NoSQL databases, most of which\\n675\\nare suited only for speciﬁc systems. For instance, [5] lists some techniques for\\nimplementing and managing data stored in diﬀerent types of NoSQL systems,\\nwhile [35] discusses design issues for the speciﬁc case of key-value datastores.\\nSimilarly, Mior et al. [36] have recently proposed an approach to the problem of\\nschema design for the speciﬁc class of extensible record stores. On the system-\\n680\\noriented side, [6, 7, 8] illustrate design principles for the speciﬁc cases of HBase,\\nMongoDB, and Cassandra, respectively. However, none of them tackles the\\nproblem from a general perspective, as we advocate in this paper.\\nRecently, Ruiz et al. have proposed a reverse engineering strategy aimed at\\ninferring the implicit schema of NoSQL databases [37]. This approach supports685\\nthe idea that, even in this context, a model-baseddescriptionofthe organization\\n30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32'}, page_content='of data is very useful during the entire life-cycle of a data set.\\nTo the best of our knowledge, this paper presents the ﬁrst general design\\nmethodology for NoSQL systems with initial activities that are independent of\\nthe speciﬁc target system. Our approach to data modeling is based on data690\\naggregates, a notion that is central in NoSQL databases where application data\\nare grouped in atomic units that are accessed and manipulated together [3].\\nThe notion of aggregate also occurs in other contexts with a similar meaning.\\nFor example, in Domain Driven Design [19], a widely followed object-oriented\\nsoftware development approach, an aggregate is a group of related application\\n695\\nobjects, used to govern transactions and distribution. Also Helland [20] advo-\\ncates the use of aggregates (there called entities) as units of distribution and\\nconsistency. In this framework, Baker et al. [38] propose the notion of entity\\ngroups, a set of entities that can be manipulated in an atomic way. They also\\ndescribe a speciﬁc mapping of entity groups to Bigtable [22], which however700\\nmakes the approach targeted only to a speciﬁc NoSQL system. Our approach is\\nbased on a more abstract database model, NoAM, and is system independent,\\nas it is targeted to a wide class of NoSQL systems.\\nThe issue of identifying data accessunits in database design shows some\\nsimilarities with problems studied in the past, such as: (i) the early works\\n705\\non vertical partitioning and clustering [39], with the idea to put together the\\nattributes that are accessed together and to separate those that are visited\\nindependently, and (ii) the more recent approaches to relational (or object-\\nrelational) storage of XML documents [40], where various alternatives obviously\\nexist, with tables that can be very small and handle individual edges, or very710\\nwide and handle entire paths, and many alternatives in between.\\nA major observation from [9] is that the availability of a high-level represen-\\ntation of the data remains a fundamental tool for developers and users, since it\\nmakes understanding, managing, accessing, and integrating information sources\\nmuch easier, independently of the technologies used. We have addressed this715\\nissue by proposing NoAM, an abstract data model that makes it possible to\\ndevise an initial phase of the design process that is independent of any speciﬁc\\n31'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 32, 'page_label': '33'}, page_content='system but suitable for each.\\nAlong this line, SOS [41] is a tool that provides a common programming\\ninterface towards diﬀerent NoSQL systems, to access them in a uniﬁed way.720\\nThe interface is based on a simple, high-level common data model which is\\ninspired by those of non-relational systems and provides simple operations for\\ninserting, deleting, and retrieving database objects. However, the deﬁnition of\\ntools for data access is complementary to data models and design issues.\\nFinally, Jain et al. discusses the potential mismatch between the require-725\\nments of scientiﬁc data analysis and the models and languages of relational\\ndatabase systems [42], whereas Alagiannis et al. [43] advocate a new database\\ndesign philosophy for emerging applications. This paper tries to provide a con-\\ntribution to these problems.\\n6. Conclusion\\n730\\nIn this paper we have argued how data modeling can be useful in the No-\\nSQL arena. Speciﬁcally, we have proposed a comprehensive methodology for\\nthe design of NoSQL databases, which relies on an aggregate-oriented view of\\napplication data, an intermediate system-independent data model for NoSQL\\ndatastores, and ﬁnally an implementation activity that takes into account the735\\nfeatures of speciﬁc systems.\\nReferences\\n[1] F. Bugiotti, L. Cabibbo, P. Atzeni, R. Torlone, Database design for NoSQL\\nsystems, in: Conceptual Modeling - 33rd International Conference, ER\\n2014, Atlanta, GA, USA, October 27-29, 2014. Proceedings, 2014, pp. 223–740\\n231.\\n[2] R. Cattell, Scalable SQL and NoSQL data stores, SIGMOD Record 39 (4)\\n(2010) 12–27.\\n[3] P. J. Sadalage, M. J. Fowler, NoSQL Distilled, Addison-Wesley, 2012.\\n32'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 33, 'page_label': '34'}, page_content='[4] M. Stonebraker, Stonebraker on NoSQL and enterprises, Comm. ACM745\\n54 (8) (2011) 10–11.\\n[5] I. Katsov, NoSQL data modeling techniques, Highly Scalable Blog,\\nhttps://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/,\\naccessed February 2016 (2012).\\n[6] A. Khurana, Introduction to HBase Schema Design, ;login: The Usenix750\\nmagazine 37 (5) (2012) 29–36.\\n[7] M. Hamrah, Data Modeling at Scale: MongoDB + Mon-\\ngoid, Callbacks, and Denormalizing Data for Eﬃciency,\\nhttp://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks -\\n(Accessed February, 2016) (2011).755\\n[8] A. Chebotko, A. Kashlev, S. Lu, A Big Data Modeling Methodology for\\nApache Cassandra, in: IEEE International Congress on Big Data, 2015,\\npp. 238–245.\\n[9] P. Atzeni, C. S. Jensen, G. Orsi, S. Ram, L. Tanca, R. Torlone, The rela-\\ntional model is dead, SQLis dead, and I don’t feel sogoodmyself, SIGMOD\\n760\\nRecord 42 (2) (2013) 64–68.\\n[10] A. Badia, D. Lemire, A call to arms: revisiting database design, SIGMOD\\nRecord 40 (3) (2011) 61–69.\\n[11] C. Mohan, History repeats itself: sensible and NonsenSQL aspects of the\\nNoSQL hoopla, in: EDBT, 2013, pp. 11–16.765\\n[12] C. Batini, S. Ceri, S. B. Navathe, Conceptual Database Design: An Entity-\\nRelationship Approach, Benjamin/Cummings, 1992.\\n[13] F. Bancilhon, Object-oriented database systems, in: Proceedings of the\\nSeventh ACM SIGACT-SIGMOD-SIGART Symposium on Principles of\\nDatabase Systems, March 21-23, 1988, Austin, Texas, USA, 1988, pp. 152–770\\n162.\\n33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 34, 'page_label': '35'}, page_content='[14] P. Atzeni, P. Merialdo, G. Mecca, Data-intensive web sites: Design and\\nmaintenance, World Wide Web 4 (1-2) (2001) 21–47.\\n[15] S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera,\\nDesigning Data-Intensive Web Applications, Morgan Kaufmann, 2003.775\\n[16] G. Mecca, A. O. Mendelzon, P. Merialdo, Eﬃcient queries over web views,\\nIEEE Trans. Knowl. Data Eng. 14 (6) (2002) 1280–1298.\\n[17] S. Abiteboul, P. Buneman, D. Suciu, Data on the Web: From Relations to\\nSemistructured Data and XML, Morgan Kaufmann, 1999.\\n[18] M. Stonebraker, U. C¸etintemel, “one size ﬁts all”: An idea whose time780\\nhas come and gone (abstract), in: Proceedings of the 21st International\\nConferenceonDataEngineering,ICDE2005,5-8April2005,Tokyo,Japan,\\n2005, pp. 2–11.\\n[19] E. Evans, Domain-Driven Design, Addison-Wesley, 2003.\\n[20] P. Helland, Life beyond distributed transactions: an apostate’s opinion, in:785\\nCIDR 2007, 2007, pp. 132–141.\\n[21] S. Abiteboul, R. Hull, V. Vianu, Foundations of Databases, Addison-\\nWesley, 1995.\\n[22] F. Chang, et al., Bigtable: A distributed storage system for structured\\ndata, ACM Trans. Comput. Syst. 26 (2).790\\n[23] Oracle,OracleNoSQLDatabase, http://www.oracle.com/us/products/database/nosql/,\\naccessed February 2016.\\n[24] J. Shute, et al., F1: A distributed SQL database that scales, PVLDB 6 (11)\\n(2013) 1068–1079.\\n[25] MongoDB Inc., MongoDB, http://www.mongodb.org, accessed February795\\n2016.\\n34'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 35, 'page_label': '36'}, page_content='[26] Amazon Web Services, DynamoDB, http://aws.amazon.com/dynamodb,\\naccessed February 2016.\\n[27] D. Pritchett, BASE: An ACID alternative, ACM Queue 6 (3) (2008)48–55.\\n[28] V. Vernon, Implementing Domain-Driven Design, Addison-Wesley, 2013.800\\n[29] K. Chodorow, MongoDB: The Deﬁnitive Guide, O’Reilly Media, 2013.\\n[ 3 0 ]D .P a s q u a l i n ,G .S o u z a ,E .L .B u r a t t i ,E .C .d eA l m e i d a ,M .D .D e lF a b r o ,\\nD. Weingaertner, A case study of the aggregation query model in read-\\nmostly NoSQL document stores, in: 20th Int. Database Engineering &\\nApplications Symposium (IDEAS ’16), IDEAS ’16, ACM, New York, NY,805\\nUSA, 2016, pp. 224–229.\\n[31] H. V. Olivera, M. Holanda, V. Guimarˆaes, F. Hondo, W. Boaventura, Data\\nmodeling for NoSQL document-oriented databases, in: 2nd Annual Int.\\nSymposium on Information Management and Big Data (SIMBig 2015),\\nVol. 1478 of CEUR Workshop Proceedings, 2015, pp. 129–135.\\n810\\n[32] C.de Lima, R.dos SantosMello, Aworkload-drivenlogicaldesignapproach\\nfor NoSQL document databases, in: 17th Int. Conference on Information\\nIntegration and Web-based Applications & Services (iiWAS ’15), iiWAS\\n’15, ACM, New York, NY, USA, 2015, pp. 73:1–73:10.\\n[33] M. Chevalier, M. E. Malki, A. Kopliku, O. Teste, R. Tournier, Implemen-\\n815\\ntation of multidimensional databases with document-oriented NoSQL, in:\\n17th International Conference on Big Data Analytics and Knowledge Dis-\\ncovery, (DaWaK 2015), Vol. 9263 of Lecture Notes in Computer Science,\\nSpringer, 2015, pp. 379–390.\\n[34] M. Chevalier, M. E. Malki, A. Kopliku, O. Teste, R. Tournier, Implementa-820\\ntion of multidimensional databases in column-oriented NoSQL systems, in:\\n19th EastEuropeanConference on Advances in Databases and Information\\nSystems (ADBIS 2015), 2015, pp. 79–91.\\n35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 36, 'page_label': '37'}, page_content='[35] T. Olier, Database design using key-value tables,\\nhttp://www.devshed.com/c/a/mysql/database-design-using-key-value-tables/,825\\naccessed February 2016 (2006).\\n[36] M. J. Mior, K. Salem, A. Aboulnaga, R. Liu, Nose: Schema designfor nosql\\napplications, in: 32ndIEEEInternationalConferenceonData Engineering,\\nICDE 2016, Helsinki, Finland, May 16-20, 2016, 2016, pp. 181–192.\\n[37] D. S. Ruiz, S. F. Morales, J. G. Molina, Inferring Versioned Schemas from830\\nNoSQL Databases and Its Applications, in: 34th International Conference\\non Conceptual Modeling (ER 2015), 2015, pp. 467–480.\\n[38] J. Baker, et al., Megastore: Providing scalable, highly available storage for\\ninteractive services, in: CIDR 2011, 2011, pp. 223–234.\\n[39] T. J. Teorey, J. P. Fry, The logical record access approach to database835\\ndesign, ACM Comput. Surv. 12 (2) (1980) 179–211.\\n[40] D. Florescu, D. Kossmann, Storing and querying XML data using an\\nRDMBS, IEEE Data Eng. Bull. 22 (3) (1999) 27–34.\\n[41] P. Atzeni, F. Bugiotti, L. Rossi, Uniform access to NoSQL systems, Inf.\\nSyst. 43 (2014) 117–133.840\\n[42] S. Jain, D. Moritz, D. Halperin, B. Howe, E. Lazowska, SQLShare: Results\\nfromamulti-yearSQL-as-a-Servicee xperiment, in: Proceedingsofthe 2016\\nInternational Conference on Management of Data, SIGMOD Conference\\n2016, San Francisco, CA, USA, June 26 - July 01, 2016, 2016, pp. 281–293.\\n[43] I. Alagiannis, R.Borovica-Gajic,M.Branco, S.Idreos, A. Ailamaki, NoDB:845\\neﬃcient query execution on raw data ﬁles, Commun. ACM 58 (12) (2015)\\n112–121.\\n36'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nIEEE TRANSACTIONS ON Knowledge and Data Engineering,  manuscript ID 1 \\n \\nHaery: a Hadoop based Query System on Accumulative and  \\nHigh-dimensional Data Model for Big Data \\nJie SONG 1, Hongyan HE 1, Richard THOMAS 2, Yubin BAO 3, Ge YU 3 \\nAbstract——Column-oriented stores, known for their scalability and flexibility, are a common NoSQL database implementation \\nand are increasingly used in big data management. In column -oriented stores, a “full -scan” query strategy is inefficient and the \\nsearch space ca n be reduced if data is well partitioned or indexed, however there is no pre -defined schema for building and \\nmaintaining partitions and indexes at lower cost. We leverage an accumulative and high -dimensional data model, a \\nsophisticated linearization algori thm, and an efficient query algorithm, to solve the challenge of how a pre -defined and well -\\npartitioned data model can be applied to flexible and time -varied key-value data. We adapt a high-dimensional array as the data \\nmodel to partition the key -value dat a without additional storage and massive calculation; improve the Z -order linearization \\nalgorithm, which map multidimensional data to one dimension while preserving locality of the data points , for flexibility; efficiently \\nbuild a n expansion mechanism for the data model to support  time-varied data. The result is Haery, a column -oriented store, \\nbased on a distributed file system and computing framework. In experiments, Haery is compared with Hive, HBase, Cassandra, \\nMongoDB, PostgresXL and HyperDex in terms of query performance. With results indicat ing Haery on average performs 4.57x, \\n4.23x, 3.55x, 1.79x, 1.82x and 120.6x faster, respectively. \\nIndex Terms—Key-value data, Column-oriented store, Multi-dimensional data model, Linearization, Accumulation  \\n——————————   \\uf075   —————————— \\n1 INTRODUCTION\\nIn the big data era, traditional relational databases can no \\nlonger meet the requirements of query performance and \\nscalability [1]. Researchers are eager to find an effective way \\nto manage massive data. Column\\n-oriented stores, which are \\nnewly emerged NoSQL databases, are wildly accepted by \\nboth indu stry and academia. Column -oriented stores use \\ntables, wide columns or column families as their fundamen-\\ntal data models. In these models, a record is represented as a \\ncollection of key -value pairs (the column name is the key) \\nsuch that each possible key app ears at most once in the col-\\nlection. [2]. \\nIn this paper, we focus on partition based query optimiza-\\ntions of column -oriented stores. Generally, queries can be \\noptimized by partition pruning, i.e, if data is well partitioned \\nby keys, then the query can be op timized by only scanning \\nthe matched partitions. The advantage of a column -oriented \\nstore is flexibility; it relies on a free -formed and soft schema \\nso that arbitrary key -value pairs can be stored. Unfortunate-\\nly, because of this flexibility, column-oriented stores lack a \\npre-defined schema so key based partitions are difficult to \\nmaintain. For example  of composite partitioning, rows (rec-\\nords) do not contain same keys and new keys are freely in-\\ntroduced. Conversely, in a traditional database, the data fol-\\nlows a pre-defined and rigid schema so that it is well parti-\\ntioned, the cost of which is flexibility. Consequently a \\ntradeoff is made between normalization and flexibility of the \\nschema. \\nWe have built key -based partitions on massive key -value \\ndata without redu cing flexibility. These partitions improve \\nquery performance by greatly reducing the search space. In \\ncolumn-oriented stores, the key -value data is organized in \\ntables. Given two related key -value pairs, the relationships \\nbetween them are “different keys (columns) of the same rec-\\nord (row)” or “same key (column) of different records \\n(rows)”. If we segment each key by its value, and let keys be \\ndimensions, then rows can be categorized into a high -\\ndimensional data model (HDM for short). The HDM is a \\nlogical data model, by which the search space can be reduced \\nto some cells of the HDM when a query is performed. This \\ninstinctive solution challenges the flexibility of a key -value \\ndata model. The following issues need to be solved: \\n(1\\n) How to ensure the efficiency of query and storage on \\nHDM, especially given the sparsity of HDM due to the fact \\nthat keys in different rows are diverse; \\n(2\\n) Given the lack of well-formed schema, how to fit the \\nnewly imported keys; \\n(3) Considering the extremely large address space of \\nHDM, how to design the mapping mechanism, to linearize \\nthe HDM to storage, and to expand without changing the \\nexisting storage; \\n(4) The query algorithm and implementation. \\nIn this paper we propose Haery ( Hadoop query), a Ha-\\ndoop based query system that uses  an accumulative HDM \\nfor big data. As a query system, it highlights the query opti-\\nmization of column-oriented stores, and is based on Hadoop \\nHDFS as the storage and Hadoop MapReduce as the compu-\\nting framework  (it also support s other computing frame-\\nworks). Our contributions are as follows: \\n(1\\n) Drawing on the experience of partitions in relational \\ndatabase, we propose an accumulative HDM to partition \\nkey-value data. The HDM is \"accumulative\" in the sense that \\nnew keys and values can be dynamically introduced. \\n(2) We propose a linearization algorithm, as an extensible \\nand flexible improvement of the Z -order curve [3] and pro-\\nxxxx-xxxx/0x/$xx.00 © 200x IEEE        Published by the IEEE Computer Society \\n———————————————— \\n\\uf0b7 Jie SONG and Hongyan HE are with the Software College, Northeastern \\nUniversity, Shenyang, 110819, China. E -mail: songjie@mail.neu.edu.cn. \\nand  2322710332@qq.com. \\n\\uf0b7 Richard THOMAS is with School of Information Technology and Electrical \\nEngineering, The University of Queensland, Queensland, Australia  \\n\\uf0b7 Yunbing Bao and Ge YU are with the School of Computer Science and \\nEngineering, Northeastern University, Shenyang, 110819, China. E -mail: \\nbaoyb@mail.neu.edu.cn and yuge@mail.neu.edu.cn.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content=\"1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n2 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\npose an address tree, to map the logical data model to physi-\\ncal addresses. \\n (3) Haery, as a system, contributes to the implementation \\nexperience of NoSQL databases. \\nFrom the aspect of query optimization, the differences be-\\ntween Haery and other partitioned or indexed NoSQL data-\\nbases are as follows: \\n(1) Partitioning techniques face challenges of maintaining \\nflexibility and extensibility. Haery adopts a schema -\\ndependent partition approach which has a finer granularity \\nthan sharding techniques  [4], and is easy to maintain and \\nextend. \\n(2) Indexing techniques face challenges of large storage \\nand search cost in a big data environment. Compared with \\nindexing, Haery does not depend on any pre -computation \\nand materialization techniques, so the storage cost is very \\nlow. Haery also prefers calculation to sea rching, avoiding \\ncomplexity explosion in a big data environment.  \\n(3) HDM in Haery is easier to maintain than partitions \\nand indexes \\nThis paper evaluates the core algorithm of Haery, com-\\nparing query performance, loading performance and storage \\ncost of Haery with t hose of other data stores, such as Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nThe results show that Haery has a marked advantage in que-\\nry performance regardless of the data size and query com-\\nplexity. The extra calculation required during data loading is \\nless than these other stores, and the additional storage cost is \\nnegligible. \\nThe rest of this paper is organized as follows. Section 2 \\noverviews related work. Section 3 provides a detailed de-\\nscription of the HDM and section 4 explains the flat  Z-order \\nlinearization and addressing algorithms. Section 5 describes \\nthe accumulation strategy and section 6\\n describes the query \\napproaches. Section 7 briefly introduces the system architec-\\nture of Haery, explaining each component of the system. \\nSection 8 evaluates the loading and querying performance of \\nHaery, comparing these with of the performance of Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nFinally, conclusions and future works are summarized in \\nsection 9. \\n2 RELATED WORK \\nIn recent years, column-oriented stores which store arbitrary \\nkey-value pairs, ha ve attracted much attention in big data \\nresearch. Research on indexing techniques and on schema or \\nlogical data models of column -oriented stores are related to \\nHaery.  \\nWu et al. [ 5] proposes a two -level indexing framework \\nwhich contains local and global indexes. The local index \\nmaintains information on each node. The global index con-\\nsists of selected local indexes in order to save storage and \\nimprove query efficiency. The proposed Ef ficient B -tree \\nadopts a B+ tree to build the local index and a cost model -\\nbased adaptive strategy to select local indexes. The global \\nindex is organized as a BATON network [6]. However, it is \\noptimized for only querying single attributes, not for multi -\\ndimensional queries. Similar to Efficient B -tree, RT-CAN [7] \\nadopts an R-tree to build a local index and a CAN network to \\nbuild a global index. These optimizations are based on a \\npeer-to-peer structure. Centralized EMINC [8] and A-tree [9] \\nindexes are propose d in a master -sla\\nve structure which is \\nalso widely accepted in big data environments. While \\nproviding query performance optimization, the cost is huge \\nto maintain and rebuild indexes. \\nITHBase [10] and IHBase [11] employ secondary indexes. \\nBoth build an index table based on index keys. The key-value \\npairs in the index table are retrieved from the original data \\nset. When a query is performed, the index table is accessed \\nfirst, then the matched data is located by the mappings be-\\ntween the index table and indexe d keys. Secondary indexes \\nbenefit query performance when indexed keys are contained \\nin the query conditions. To reduce the cost of random query-\\ning, Zou et al. [12] proposed CCIndex which stores original \\ndata in an index table. When a query is performed, it  only \\nscans the index table without looking up the mappings be-\\ntween the index table and indexed keys. However, it is diffi-\\ncult to guarantee fault tolerance and consistency. A second-\\nary index is easier to maintain than two -level indexes. Both \\ntwo-level indexes and secondary indexes are based on specif-\\nic index structures, which are difficult to maintain when the \\ndata set is updated frequently. They are also not efficient \\nwhen querying high-dimensional data due to the indexes on \\nmany columns are extreme huge.  \\nMulti-dimensional indexes have been studied extensively. \\nCommon multi-dimensional indexes include Grid Index [13], \\nKD-trees [14] and R-Trees [15]. They divide the space into a \\nk-dimensional space which consists of grids, and allocate \\naddresses for each grid. Query results can be found by locat-\\ning specific grids, which is faster than the original query \\nstrategy. Based on this idea, MD- HBase [16] and SHG -tree \\n[17] were proposed. The storage of SHG\\n-tree is inefficient \\ndue to the sparsity of high-dimensional data, and MD-HBase \\nhas weaknesses in data consistency, and brings extra costs \\nand latency when data is unevenly distributed. Many re-\\nsearchers have come up with new index structures to satisfy \\ndifferent requirements. VAR-tree [18] aims to improve query \\nperformance by compression; CSA -tree [19] focuses on \\nmemory search; NV-tree [20] facilitates near inquiries, and so \\non. \\nThe optimization of schemas of non -relational databases \\nalso contributes to the query performance. Mior et al. [2 1\\n] \\npresented a system for re commending database schemas for \\nNoSQL applications. Automating the design process allows \\nthe proposed prototype, NoSQL Schema Evaluator (NoSE), \\nto produce efficient schemas and to examine more alterna-\\ntives than would be possible with a manual rule -based ap-\\nproach. They proposed a cost -based approach by a novel \\nbinary integer programming formulation to guide the map-\\nping from the application's conceptual data model to a data-\\nbase schema, while we propose  the logic data model map-\\nping the database schema  to the physical dataset for query \\noptimization.Vajk et al. [2 2] defined some problems of sche-\\nma design in NoSQL database by compared with material-\\nized views in relational databases, and proposed a cost driv-\\nen model, which can optimize the mapping from the applica-\\ntion data model to a physical schema. They start with a nor-\\nmalized data schema, then identified a column store schema \\nwhich can serve the queries with minimal cost. However, it\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 3 \\n \\nis to optimize for monetary cost (storage and query) only \\ninstead of co nsidering performance, also the cost model is \\nusually not the case, which maybe overvalue the inexpensive \\nschema options with poor performance. And the solution is \\nnot general enough, it is only efficient for predefined queries.  \\nHyperDex [23,24] is a cloud database. The key insight be-\\nhind HyperDex is the concept of hyperspace hashing in \\nwhich objects with multiple attributes are mapped into a \\nmultidimensional hyperspace. Haery’s initial idea is close to \\nHyperDex, however, the two systems have essential dif fer-\\nences. HyperDex relays on a predefined schema  of objects, \\nmodels the objects with same attributes into tables, divides \\nattributes of the table into different partitions, duplicat es \\nobjects of the table to every partition, builds the hyperspace \\nof each partition according to order ed hash values of its at-\\ntributes, tessellats the hyperspace into regions, assigns the \\nregions to servers, and stores the mappings as data in the \\ncorrdinator server. As the comparsion, Haery relays on a \\nflexible and accumulative schema of rows, models the rows \\nwith different keys into a table, builds the key-cube accord-\\ning to the segments on value range of each key, expands key-\\ncube to support new keys, divideds rows into cells, and cal-\\nculates addresses of cells. We compare the two systems in \\nthe Table 1. \\nIn Table 1, some concepts will be explained in \\nthe rest paper. \\nTABLE 1 DIFFERENCES BETWEEN HYPERDEX AND HAERY \\n \\nFacing the “volume”, “variety” and “variability” charac-\\nteristics of big data, index based solutions have three weak-\\nnesses: indexes require a large amount of storage, it is costly \\nto build indexes for various keys, and it is too complex to \\nmaintain and rebuild indexes when dealing with large scale \\nand frequently updated data sets. Haery solves these prob-\\nlems by an accumu lative and high-dimensional data model. \\nThere are three categories of multidimensional data models \\nfor NoSQL databases. \\n(1\\n) Basic multidimensional data models, which are a sim-\\nple technique, take the data sets as points in multidimen-\\nsional space, then divide data attributes (keys) as dimensions. \\nFor example, Chevalier et al. [25] propose an implementation \\nof traditional OLAP (On-Line Analytical Processing) systems \\nby column-oriented and document-oriented database;  \\n(2) Statistical multidimensional models, which are more \\ncomplicated than basic models, leverage specific aggregation \\nfunctions to achieve hierarchies of dimensions. For example, \\nIqbal et al. [26] propose a histogram method, which iterative-\\nly splits histogram buckets until the given space limit is \\nreached, to summarize multidimensional data represented as \\na tuple -independent probabilistic model. The proposed \\nmethod. \\n(3) Structured multidimensional data model s, which are \\nmore powerful to support hierarchies of dimensions. For \\nexample, we have prop osed a paper proposes a structured \\nand distributed MOLAP technique, named DOLAP (distrib-\\nuted OLAP), based on Hadoop distributed file system (HDFS) \\nand MapReduce program model, it can also support complex \\naggregation operations between hierarchical structures [27\\n]. \\nHaery adopts an improved basic multidimensional data \\nmodel. On one hand, all the above data models would be \\nrefactored when a new dimension is introduced, or current \\ndimension hierarchies are updated. For basic model s, new \\ndimensions cause updating index spaces and remapping \\nindexes to addresses. For statistical\\n models, new data causes \\nre-calculation of the existing aggregation. For structured \\nmodels, updating dimension hierarchies causes the rebuild-\\ning of the complex data structure. The cost of this refactoring \\nis unaffordable in a big data environment. Haery treats refac-\\ntoring in a distinct way, creating a new data structure instead \\nof modifying the existing one. In most column -oriented \\nstores, to reduce the cost of modification, data items  are not \\ndeleted or modified but replaced. Following this idea, the \\ndata model may also be replaced instead of being modified. \\nThe previously mentioned works aim to solve a specific \\nproblem to meet some specific requirements. Consequently \\nthey concentrate on query performance optimization. Haery \\naims to solve a broader range of problems and is a general -\\npurpose, high-dimensional data supported, query optimized, \\nflexible and extendable data store. \\n3 MODEL \\nIn this section, the critical definitions for key-cube, which are \\nfundamental to linearization, accumulation and query in the \\nrest of paper, are explained. The main symbols appearing in \\nthis paper are listed in Table 2. \\nTABLE 2 MAIN SYMBOLS IN THIS PAPER \\n \\n3.1 Key-values \\nIn column-oriented stores, data is accessed with a key -value \\ndata model. A table consists of rows, and a row consists of a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n4 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nunique row key and many objects. An object has key -value \\npairs associated with different timestamps, in which keys \\nwith different timestamps are the same but valu es with dif-\\nferent timestamps are different. Referred to by an entity -\\nrelationship conceptual model, a row stands for an entity \\nand objects are attributes of the entity, in which keys are at-\\ntribute names, and values are attribute values, while \\ntimestamps represent different versions of attributes. \\n3.2 Segments \\nIn a column -oriented store, the values of a key may be nu-\\nmerical or textual data, and values may belong to a certain \\nrange. To partition these values, we define the segment as a \\nspecial set of values which belongs to the same range. \\nDefinition 1 Segment. A segment s represents a range of values, \\ns is a two -tuple <α, β> representing the lower boundary and \\nupper boundary of the range (α and β are numbers) \\nThere are three special forms of segment. \\n(1\\n) s0 =<-∞,+∞>, also named ALL, represents all values; \\n(2) s1=<-∞, α> and sw=<β, +∞>, named lower overflow \\nsegment and upper overflow segment; \\n(3) se=null, also named NULL, represents no value.  \\nGiven a value u of the key, we define u belongs to the \\nsegment s, denoted as segment u≼s. s and ≼ may have fol-\\nlowing four forms: \\n(1) u≼s0 \\n(2) u≼se if u is null.  \\n(3) u is a numerical value, namely u≼s iff u∈(a, β]. \\n(4) u is textual value, namely u≼s iff hash(u)∈(a, β]. Func-\\ntion hash() is the hash value (integer) of a string. The func-\\ntions hash-1(-∞) and hash-1(+∞) are invalid. \\nFor segment sx and sy , their operations are defined as fol-\\nlows. \\n(1) Union of two segments, as sx∪sy ={u| u≼ sx or u≼ sy }, \\nand intersection of two segments, as sx∩sy ={u| u≼ sx and u≼ \\nsy } \\n(2) One segment is the sub-segment (proper subset) of the \\nother, as sx \\uf0cc sy iff ax <ay and βx >βy \\n(3) The special operations on se, such as sx ⊄se, se ⊄ sx, se \\uf0cc \\nse; and sx∩se =∅, se∩se = se \\n(4) The comparison between segments, as sx ≤ sy iff βx ≤ \\nay, we know \"≤\" is total order relation on {s} \\n(5) sub(s) are the non-overlapping and non-empty subsets \\nof s, which are in ascending order. \\nPer the function sub(s), a segment s can be divided into \\nsub-segments. We call this segment refinement, and sub -\\nsegments are in ascending order according to the \" ≤\" opera-\\ntor. \\nDefinition 2 Segment Refinement. Segment refinement means \\nsegment s is divided into w sub-segments, which are in ascend-\\ning order, and are non -overlapping and non-empty. Segment s \\nis refined to sub(s) according to the definition of function sub() \\nin definition 1. The number of sub -segments is a fixed value, \\ncalled the refinement factor and denoted as w (w>3). \\nThe segmentation algorithm divides a segment into sub -\\nsegments with an equal interval which is defined as Δ=(β- \\na)/w. It is a simple algorithm and is abbreviated here. \\nAmong the sub(s), lower overflow sub-segment is included if \\ns is also a lower overflow su b-segment, and the same for \\nupper overflow segments. For example: \\nFor the initial state: s0=<-∞,+∞>,Δ=(β- a)/w, w=3 \\nThe sub-segment of s0 is: sub(s0)={ s1, s2, s3,..., sw}= {<-∞, a \\n>, < a, a +Δ>, < a +Δ, a +2Δ>,...,<β-Δ, β>, <β, +∞> } \\nThe sub-segment of s1 is: sub(s1)={<-∞, a -wΔ>,...,< a -2Δ,  a \\n-Δ>, < a -Δ, a >} \\nThe sub -segment of sw is: sub(sw)={ < β, β+Δ>, < β+Δ, \\nβ+2Δ>,...,<β+wΔ, +∞>,} \\nSegment refinement disperses a massive amount of  data \\non the sub -segments, and because of equal intervals, seg-\\nments may map different volumes of data. But, as is ex-\\nplained later, the data volume of segments tends to be equal \\n(equal frequency) after many iterations of segment refine-\\nment. \\nDefinition 3 Segment Hierarchy and Levels. A segment and \\nits sub-segments are organized as a hierarchy. Segment hierar-\\nchy is initiated as two levels at least, then expend to τ\\n levels. \\nThe root level is level 0 with a single segment s 0 (ALL). The \\nnext level contains predef ined w segments, which are sub(s0), \\nincluding upper and lower overflow segments. A segment in \\nlevel 1 may expanded to w sub -segments, with each level in \\nturn potentially expanding further. Or, a level may not expand \\nat all. The segment hierarchy is a “w -ary” ordered tree because \\nsub(s) is in ascending order. \\nA consequence of definition 3 is that the segment hierar-\\nchy expands gradually. We introduce the concept of version-\\ning to distinguish the segment hierarchies of different ex-\\npanded stages in section 5. \\nDefinition 4\\n Segment Index. Segment index is the identity of \\neach segment in a segment hierarchy, and is denoted as the sub-\\nscript \"j\" of segment s j. The segment hierarchy which contains \\nτ+1 levels may have at most  segments, so the index of the root \\nsegment is 0 and the in dexes of th e rest are coded \\nfrom 1 to n level -wise, and from left to right in a level. Since \\nlevel l contains at most power(w, l) segments, some segments \\nmay be absent because the refinement has not happened yet, but \\ntheir corresponding indexes are reserved. \\nFunction f. According to decimal coding, a segment range \\ncan be calculated by giving a segment index j. Given s1=<-∞, \\nα> and sw=<β, +∞>, a function f(j) returns the segment by its \\nindex, that is,  sj=f(j) and sj =<aj, bj>=<fa(j), fb(j)>. Meanwhile \\nj= f-1(\\nsj)=f-1(<aj, bj>). If it exists, sj is the x-th ordered segment \\nin the level y of segment hierarchy, x, y, aj and bj are as fol-\\nlows (in the equation wy is “y-th power of w”, but not super-\\nscript): \\ny = logww+(w-1)j-1,              x=j- (wy-w)/(w-1) ; \\nwhen  j=(wy-1)/(w-1),         aj=-∞,  βj=2a-β+x·w1-y·(β-a) ; \\nwhen j=(wy+1-w)/(w-1),aj=2β-a+(x-1-wy)w1-y·(β-a), βj=+∞ ; \\notherwise aj=2β-a +(x-1-wy)·w1-y·(β-a), βj=2a-β+x·w1-y·(β-a) . \\nDefinition 5 Active Segment and Dormant Segment.  Given \\na segment hierarchy, a segment which has no sub -segments (a \\nleaf node) is an active segment; and segments which have sub -\\nsegments (non-leaf nodes) are dormant segments. \\n3.3 Key-cube \\nKey-cube, which is a replacement for the key -value data \\nmodel, is a high -dimensional data model whose dimensions \\nare keys and cells are links to the data files. The key, which \\n \\n\\uf0e5\\n\\uf03d\\n\\uf03d\\n\\uf074\\n0l\\nlw n'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 5 \\n \\nrepresents a dimension, is divided into segments according \\nto their values. Key-cube is not a fixed data model but accu-\\nmulatively changes according to the keys and their values. \\nDefinition 6 Dimension and Dimension Index . In a key -\\ncube, a key is modeled as a dimension which categorizes values \\nof the key into non-overlapping active segments and se, in order \\nto provide filtering, grouping and labeling. A dimension con-\\ntains segments of the expandable segment hierarchy. se is the \\nfirst segment of a dimension, and the rest of the segments are \\nactive segments of the corresponding segment hierarchy from \\nleft to right. Thus a dimension d contains at least w+1 segments \\nbecause the segment hierarchy is initiated with at least two lev-\\nels (see definition 3). A dimension index is the identity number \\nof the dimension. Let subscript i (1≤i≤n) represent the index of \\nd. And a table of dimension index, named as DTable, is used to \\nmap the keys (names) to the dimension indexes. \\nDefinition 7\\n Cell and Cell Index . A cell is specified by active \\nsegments (leaves) or  the null segment ( se) of a dimension. All \\nrows are partitioned into the cells by the following rules: Let a \\nrow contains a key -value pair <k i, ui>, and for the correspond-\\ning dimension di, if the value u i≼sij, then the row is mapped to \\nsij. Rows in column -oriented store are organized as data files, \\nthus a cell contains physical directories of the data files where \\nthe corresponding rows are stored. Cell index is calculated by \\nthe linearization algorithm and the mapped directories are re-\\ntrieved by the address tree (see section 4). \\nDefinition 8 Segment Map . A segment map is a HashMap \\nwhose keys are integers, representing dimension indexes, the \\ncorresponding values of a key are also integers, representing \\nsegment indexes. Both keys and values are sorted. Let m be a \\nsegment map, the array m.keys are dimension indexes, and the \\narray m[i].values are segment indexes of dimension di. \\nNormally, key value pairs are sparse, thus not all keys are \\nmodeled as dimensions, and a key -cube contains a large \\nnumber of empty cells. But the key -cube is a logical data \\nmodel, and as is explained in sections 4, the sparsity does not \\nwaste storage. A high -dimensional key-cube does not have \\nthe problem of storage explosion because most information \\nis calculated rather than stored. \\nDefinition 9 Dimensional Key and Trivial Key . Keys which \\nare selected as dimensions are dimensional keys, otherwise they \\nare trivial keys. Most keys are dimensional keys in a key -cube. \\nBy default, new keys are trivial keys, which may be upgraded to \\ndimensional keys. \\nDifferent from the selection of keys when building  an in-\\ndex, dimensional keys not only frequently occur in query \\nconditions, but also in query results. On the contrary, trivial \\nkeys are keys whose values are not queried often, neither in \\nquery conditions nor in query results. There are a number of \\nkeys in the key-value data model, and among which dimen-\\nsional keys dominate, so that the key -cube has a high num-\\nber of dimensions. When a key is selected as a dimension, \\nand its range is determined according to the context, then s0 \\nto sw, which are levels 0 and 1  of the segment hierarchy, are \\ninitialized first.  \\n4 LINEARIZATION \\nA key -cube is a high -dimensional data model. There are a \\nlarge number of cells, even though most of them are empty, \\nbut the large address space of cell indexes is reserved. Haery \\nneeds a sophisticated linearization algorithm and addressing \\nmechanism to encode and map the cells indexes to the file \\naddresses. A linearization algorithm is an algorithm which \\nmaps multi-dimensional data to one dimension. This section \\ndescribes a linearization algorithm that encodes the cells of a \\nkey-cube into a sequence. A space -filling curve (SFC) is a \\ncurve whose range contains the entire n -dimensional hyper-\\ncube, so that a curve is selected to design a linearization al-\\ngorithm. The cell index, which is the resul t of linearization, \\nmaps to physical directories by querying a tree -structure, \\ncalled an address tree.  \\nIn a key-cube, the adjacency of cells benefits the address-\\ning process because the matched cells for a query are normal-\\nly adjacent, thus they can be addr essed together. The ad-\\ndressing of one cell could reuse the addressing information \\nof its adjacent cells, therefore, the indexes of neighbor cells \\nshould be continuous. Based on the classification of Asano T. \\n[28], the most popular SFCs are recursive SFCs, such as Z -\\norder, Gray and Hilbert, and non -recursive SFCs, such as \\nSweep and Scan. Sweep and Scan are simple, their time \\ncomplexities and flexibilities are better than the others, but \\nthey only maintain the continuity of one dimension and \\nbreak that of the  other dimensions. Meanwhile, Z -order is \\nfair and scalable on each dimension, and is relatively simpler \\nthan Gray and Hilbert  [29]. By Z -order, cells, especially \\nneighboring cells are encoded continuously. Z-order curve is \\nmore suitable choice for its fairness in all dimensions. \\nHowever, Z-order can only expand synchronously in all \\ndimensions. It is too rigid and requires the same number of \\nactive segments in each dimension. To solve this problem, a \\nflat Z-order algorithm is proposed, which only requires th at \\nthe number of active segments in dimensions are integral \\nmultiples of each other. If the conditions are not satisfied \\nnaturally, some null \\nsegments (se) are artificially added to the \\ndimensions. \\n \\nFig. 1. Six space curves in two-dimensional space \\nFigure 1-(e) and (f) shows the flat Z -order in comparison \\nwith the original Z -order in two dimensional space. Let the \\ninterleaving ratio of two dimensions be 1:2. Thus, in Figure \\n1\\n-(f), when the curve extends one dimension for one cell, it \\nextends the other dimension for two cells, while the original \\nZ-order can expand all the dimensions by the same number \\n(a) Sweep (b) Scan (c) Gray (d) Hilbert\\n(e) Z-order (f) Flat-zorder'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n6 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nof cells (interleaving ratio is 1:1). \\nDefinition 10 Binary length. Binary length θi of dimension i is \\n⌊log2max(sij)⌋ in which max(s ij) is the maximum index of a seg-\\nment in dimension i. Without loss of generality, let θ be the \\nminimum binary length among those of all dimensions, then \\nθi/θ is a positive integer. \\nGiven the active segments of dimension 1 to n, the index \\nof a corresponding cell is calculated as the following three \\nsteps in which arrays are indexed starting with 1. For clarity, \\nsij is the j-th segment in i-th dimension (see Table 1), and Fig-\\nure 2 is an example of these steps. \\n(1) Binarization: Convert index (j) of segment sij into a se-\\nquence of binary numbers. Let the sequence be stored in ar-\\nray xij[]. If necessary, several “\\n0”s are complemented on to \\nthe head of the sequence to ensure that the length of xij[] is θi. \\nThen xij[p] is 0 or 1 ( pi ∈ [1, θi]). For example, θ1=2, x12=[1,0]; \\nθ2=8, x25=[0,0,0,0,0,1,0,1] . \\n(2) Grouping: Convert xij[] into yij[] whose length is θ. Let \\n“+” be the string concatenation operator, then ∀ p ∈ [1,θ], yij[p] \\n= xij[(p·θi /θ)] + xij[(p·θi /θ)+1]+ xij[(p·θi /θ)+2]+...+ xij[(p·θi \\n/θ)+ θi /θ], For example, let θ2= 8 and θ=2, then y25=[0000, \\n0101]. \\n(3) Interleaving: Interleave the segments of each dimen-\\nsion. Let array z[] contains n selected segments indexes for \\ndimension, respectively, then the index of a cell determined \\nby z[n] is { y1(z[1])[1]+ y2(z[2])[1]+…+ yn(z[n])[1]}+ {  y1(z[1])[2]+ \\ny2(z[2])[2]+… + yn(z[n])[2]}+…+{ y1(z[1])[θ]+ y2(z[2])[θ]+…+ y \\nn(z[n])[θ]}. Therefore： cell_index=  . \\nθ=2\\nθ1=2\\nθ2=8\\nS12\\nS25\\nx12= [1,0]\\nx25=[0,0,0,0,\\n        0,1,0,1]\\n(1)\\n(1)\\ny12=[1,0]\\ny25=[0000,0101]\\nz=[2,5]\\n(3)\\n(3)\\nindex=1000000101]\\n5 is 101\\n2 is 10\\n(1) Binarization (2 ) Grouping (3 ) Interleaving\\n(2)\\n(2)\\n \\nFig. 2. Example of (1) Binarization, (2) Grouping and (3) Interleaving \\nWe introduce the concept of b inary length  for dealing \\nwith an irregular n-dimensional key- cube, otherwise each \\ndimension should contain the same number of active seg-\\nments, and has to expand all the dimensions synchronously. \\nThe complemented “0” in the head of  sequence only con-\\nsumes the address space but not storage.  \\n \\nFig. 3. Example of flat Z-order \\nThe cell indexes for a 4× 8 key-cube (both in decimal and \\nbinary) are shown in Figure 3 as an example. A nd in this \\ncase, the interleaving ratio is 1:2. Interleaving the binary co-\\nordinate values yields binary Z-values, and connecting the Z-\\nvalues in their numerical order produces the recursively flat \\nZ-shaped curve. \\nFunction g(): Specifies segments of each dimension, then \\none or more cells are determined.  Let m be a segment map \\ncontaining active segment indexes for n dimensions. A func-\\ntion g(m) returns cell indexes c[] mapped by m. Function g() \\nis implemented according to the flat Z -order. The size of c[] \\nis \\nvalues i m\\nn\\ni\\n].[\\n1\\n0\\n\\uf02d\\n\\uf03d\\n\\uf050 . \\nDefinition 11 Cell Address and Address Tree . Given a cell, \\nits cell address is a group of physical directories in which the \\ncorresponding data is contained. An address tree is an ordered-\\ntree associated with a key-cube. It leverages the longest common \\nprefix naming scheme to map a cell index to a cell address. \\nA quad-tree is a tree data structure in which each internal \\nnode has exactly four children. An address tree has a maxi-\\nmum of θ levels, from level 0 to level θ-1. A θ-levels address \\ntree is similar to a quad -tree e xcept that each node in the \\nlevel (θ-2) has \\n) (\\n1\\n\\uf071 \\uf071i\\nn\\ni \\uf03d\\n\\uf050  children; then an internal node rep-\\nresents the longest prefix of its children’s indexes, and a leaf \\nrepresents a cell index. So that for an arbitrary address tree, \\nan internal node maps to the parent directories of the cell \\naddress, and a leaf maps to the cell address. An address tree \\nis initialized as θ levels at most, or less levels for reducing its \\nsize, according to the scale of key-cube. \\nFunction r(): Given an address tree and cell indexes, de-\\nnoted as array c[], function r(c[]) returns physical directories \\nthat contain the data files of the cells in c[]. In r(), the directo-\\nries are addressed by comparing the prefix of the cell index \\nwith nodes. Since the indexes of adjacent cells are continu-\\nous, they are addressed together. Assuming the indexes in c[] \\na\\nre sorted, then when c[i] is addressed, the traversal path \\n(node list from the root to the matched node) is cached, then \\nc[i+1] is addressed along the path inversely. The address tree \\ncontains θ levels at most, so if n cells are addressed one by \\none, the time complexity is O(θn), and the time complexity of \\nr() is no greater than O(θn). The algorithm is as follows:  \\n \\n\\uf0e5\\uf0e5\\n\\uf03d \\uf03d\\n\\uf071\\n1 1\\n])[ ( ] [\\np\\nn\\ni\\ni z i p y'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 7 \\n \\n \\n \\nFigure 4 shows the example of address tree of Figure 3. In \\nFigure 4, the selected cells in the left ma p to the node of ad-\\ndress tree in the center, and the node maps to the directories \\nin the right. I n Figure 4, a cell address contains only one di-\\nrectory, but in practice it would usually contain more. All the \\ndescendants of a node have a common prefix that is associat-\\ned with the node, and the root is associated with a computer.  \\n[0000]\\n...\\n[11]\\n[01]\\n...\\nPhysical Directories \\n(partly)\\n4\\\\\\nNode n\\nds\\\\\\nts\\\\\\n4\\\\\\n0\\\\\\n1\\\\\\n... ds\\\\\\nts\\\\\\n[10][00]\\n[0001]\\n[0010]\\n[0011]\\n[1000]\\n[1011]\\n[1100]\\n[1111]\\n...\\n...\\n[00001][00000] [10111][10110] [11111][11110]\\n...\\nCell with indexes\\nAddress tree (partly)\\nFig. 4. Example of address tree \\n \\nFig. 5. Example of segment refinement and key cube expansion  \\n5 ACCUMULATION  \\nHaery improves query performance by providing the pre -\\ndefined, well -formed and well -partitioned key -cube. Key -\\ncube has two features: high-dimensions for supporting a vari-\\nety of keys (section 4 explains why the high -dimensions do \\nnot cause performance problems); accumulation for perfor-\\nmance and flexibility. The later one is called key -cube expan-\\nsion and explained in this section. Figure 5 shows the accu-\\nmulation of key -cube expansion by segment refinement and \\ndimension introduction. \\nOn one hand, as defined in definition 2 , segment refine-\\nment means segment s is divided into w sub-segments. Obvi-\\nously,  a dimension initialized as a root with only w active \\nsegments is too coarse to categorize massive data, so that, the \\ndata volume in a segment gets larger and larger when new \\ndata is imported, and th en, the segment is refined to sub -\\nsegments for reducing data volume in a segment. By this \\nmeans, the key-cube is expanded by segment refinement, for \\nexample the step (1) and (3) in Figure 5, s22 is refined to s27, s28, \\ns29 in step (1) and s33 is refined to s311 and s312  in step (3). The \\ntrigger of segment refinement is that both the data volume \\nand data dispersion are larger than given thresholds. Data \\ndistribution is measured by a sampling approach and five -\\nnumber summary (minimum, Q1, median, Q3, maximum). \\nOn the other hand, new keys (by default trivial keys) are \\nintroduced with new data meanwhile the trivial keys may no \\nlonger be “trivial”, and new a dimension is accordingly add-\\ned to the key -cube. So that the key cube is expanded by di-\\nmension introduction, for example the step (2) and (4) in Fig-\\nure 5. Dimension introduction is triggered according to the \\nfrequency of a trivial key occurring in rows. \\nDefinition 12 Key-cube Expansion. A key-cube expansion is a \\nprocess of creating a new key\\n-cube based on a exsiting one, by in-\\ntroducing dimensions (new keys or trivial keys are upgraded as \\ndimensional keys), or by refining active segments of dimensions \\n(one or more active segments refine to w sub -segments), or by \\nboth. Expansion is the key feature of the accumu lative key-cube \\nwhich improves both flexibility and query efficiency. \\nDefinition 13 Key-cube Version. The version of a key-cube is for \\ndistinguishing different key -cubes by their historical features. \\nEvery time a key -cube is expended, a new one is defined.  These \\nkey-cubes are numbered in a sequence order from  earliest to lat-\\nest, and their orders are their versions. The version of the initial \\nkey-cube is 0. \\nDefinition 14 Key-cube Backtrace. A key-cube backtrace is the \\nprocess of traversing from a later versioned key-cube to an earlier \\nversioned one, for comprehensive query on data partitioned by \\ndifferent key-cubes. The key-cube backtrace can be treated as the \\ninverse operation of key -cube expansion, by dimensio n reduc-\\ntion, or by segments consolidation (replacing the active segments \\nwith its parent), or by both. \\nFor example in Figure 5, steps (6) and (8) are segment con-\\nsolidation, and step (5) and (7) are dimension reduction. \\nFunction h. The function h(cube) returns the previous  ver-\\nsioned key-cube of the current one by querying the key -cube \\nmetadata. Notice that many concepts defined in the previous \\nsections associated with the key -cube, such as dimensions, \\nsegments, function f(), g\\n() and r(), linearization algorithm and \\naddress tree, these concepts of different versioned key -cube \\nare different too. Especially the linearization algorithm, when \\na new key- cube is expanded, the linearization algorithm and \\naddress tree are all changed, mapping new data to different \\ncell addresses. \\nAs far as the query is concerned, every key-cube expansion \\nis a segmentation on the new data in the future, however, \\nexpansion does not affect the existing data. When a query is \\nissued, the targets are both the current data managed by the \\ncurrent key -cube, and “historical” data managed by earlier \\nversioned key- cubes. In these cases, Haery adopts a two -\\n \\n1\\nd1\\nd2\\ns11 s12 s13 s1e\\ns21\\ns22\\ns23\\ns2e\\nd1\\nd2\\nd3\\ns1es11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns31\\ns32\\ns33\\ns3e\\n78\\n3\\n5\\n4\\n6\\nd3\\nd1\\nd2\\ns1es11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns31\\ns32\\ns33 s3e\\ns310\\ns311\\ns312\\n2\\ns1e\\nd2\\nd1\\ns11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns22\\nσ0 σ1 σ2 σ3 σ4'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n8 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nphase query strategy: first query all key -cubes and locate all \\ndata files as the search space; second query data in these files. \\nThe key-cube backtrace ensures the data files containing both \\ncurrent data and historical data are selected. The query per-\\nformance is dominated by the size of the search space, which \\ncan be greatly reduced by querying key -cubes. Key-cube ex-\\npansion ensures that the optimization effect of later versioned \\nkey-cube is the same, or even better, than that of earlier ver-\\nsioned ones, and the optimization effect of earlier versioned \\nkey-cubes remains unchanged. Query performance is im-\\nproved by the accumulation strategy.  \\n6 QUERY \\nThe key-cube not only benefits queries on values (conditions) \\nby reducing the search space, but also benefits queries on \\nkeys (targets) by well -managed dimensional keys. And, due \\nto the accumulative key -cube, performance optimization is \\nstill in effect whe n new data is imported. In this section the \\nquery process is introduced. The query process of Haery in-\\ncludes query on key -cube and query on files, and the former \\nincludes backtrace on key-cube as mentioned in section 6. \\nDefinition 15\\n Query in Haery . A query in Haery includes two \\nparts, targets and conditions. Targets is a list of keys, and condi-\\ntions includes keys and query ranges. Both targets and condi-\\ntions contain dimensional keys and trivial keys. The key -cube \\nimproves query performance if keys, no matter if they are in con-\\nditions or in targets, are dimensional keys. To reduce the search \\nspace, a key, if it is not contained in conditions but only in tar-\\nget, is treated as a special condition whose range is < -∞,+∞>. \\nQueries are denoted as: \\nquery={<key, ran ge, is_target >}={<k 1, α1, β1,true>,<k2, α2, \\nβ2,true>, … , <kc, αc, βc, false >} \\nFor example, a query “\\nselect a, b from table where a∈(0, 8] \\nand c∈(0, 10] ” is represent as {< a, 0, 8, true>,< b, -∞, + ∞, \\ntrue>,<c, 0, 10, false>}. \\nThe querying process is explained by the following steps:  \\n(1) Initial query on the latest key cube; \\n(2) Backtrace and query each versions of key-cubes;  \\n(3) Query on chunk files in a distributed and parallel man-\\nner.  \\nSteps (1) and (2) are to reduce the search space (queried \\nfiles), which can greatly improve the performance. Algorithm \\n2 explains the details of steps (1) and (2). Haery support dif-\\nferent computing frameworks which have different machines \\nor grammars to implement step (3), such as Hadoop Map Re-\\nduce. Given the diversity of implementations, details of step \\n(3) are abbreviated. \\n \\n \\n7  ARCHITECTURE \\nIn this section, the software architecture of Haery is explained \\nas in Figure 6\\n. Haery is a query focused column-oriented store \\nwhich is based on two i nfrastructures: a distributed compu-\\nting framework and a distributed file system. Upon these, \\nseveral modules collaborate to implement the key -cube’s \\nmanagement and query engine, they are Cube -base, Proxy, \\nExpansion, Partition, Placement, and Calculation. T he source \\ncode of Haery is available on https://github.com/CloudLab-\\nNEU/Haery. \\nCube-base\\n Proxy\\nCalculation\\nSpark or MapReduce\\n1.cube metadata\\n3.cube \\ninformation\\n4. generate tasks\\nnew  cube\\n5. return\\nresults\\n      Haery core Computing \\nFramework\\nPartition\\nPlacement\\nHDFS\\nExpansion\\nData loading\\nData loading Data query\\n2. query  request \\nStorage\\n \\nFig. 6. Software architecture of Haery  \\n•Cube-base: The metadata is stored in the Cube-base module \\nwhich is implemented by an in -memory database on the \\nmaster node. \\n•Proxy: When Proxy receives a query from clients, it analyzes \\nand validates the query by referring to Cube -base. Proxy \\nthen submits the query statement to Calculation, and waits'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 9 \\n \\nfor the results. Finally, Proxy receives the data from Com-\\nputing Framework, generates the result view, and reports to \\nthe client. \\n•Calculation: Calculation is the core of Haery that imple-\\nments algorithm 2 in section 6. \\n•Expansion: Expansion is a trigger for key -cube expansion \\nand implements the algorithms and strategies mentioned in \\nsection 5. \\n•\\nPartition and Placement: Partition horizontally and vertical-\\nly partitions rows of a cell into several chunk files and \\nPlacement places the chunk files in nodes.  \\n• Computing Framework. Haery is technically compatible \\nwith many distributed computing frameworks such  as \\nMapReduce and Spark. Currently only MapReduce is sup-\\nported, however, it can support other computing frame-\\nwork if the algorithms  mentioned in section 6 are imple-\\nmented using the framework. \\n•Storage. Key-value data is stored in a distributed file system, \\nHDFS for example. \\nWe briefly introduce the storage mechanism of Cube-base \\nand Storage. In a traditional data cube, a multi -dimensional \\narray is the uniform form of storage and the storage cost is \\nexpensive. In Haery,  the cube is not stored but calculated.  \\nEach cell contains no data but links to the physical directories, \\nand cell indexes are calculated, not stored. An address tree is \\nan in-memory data structure. For each segment, only the ac-\\ntive segment indexes are stored. For each dimension, only the \\ndimension index, dimension name (for query conditions), and \\nrange of values represented as an ordered tuple <index, name, \\nα, β>, are stored. For each key-cube, only its version is stored. \\nAll the data mentioned above is Haery’ s key-cube metadata.  \\nIn a traditional a column-oriented store, storing data with \\nits schema provides flexibility but waste s space. In \\nHaery, \\nstorage is saved by separating the schema and data through \\nthe uniform key-cube, and flexibility is ensured by accumula-\\ntion. A  data set is horizontally partitioned into dimensional \\nchunk files and trivial chunk files , implemented as Hadoop \\nArrayFile and SequenceFile. A dimensional chunk file only con-\\ntains values of dimensional keys because names of which are \\nfixed, ordered, and modeled as dimensions in the key -cube. \\nThey are inferred by their orders. A trivial chunk file contains \\nboth trivial keys and their values. \\n8  EXPERIMENTS \\nIn this section, we evaluate the core algorithms of Haery, and \\ncompare Haery with some popular NoSQL and relational \\ndatabases using generated datasets and various query work-\\nload.  \\n \\n8.1 Setup \\nScope. Verify the core algorithms, function g\\n() for lineari-\\nzation and function r() for addressing, proposed in this paper \\nin a standalone environment. Compare Haery with other da-\\ntabases from the point of view s of loading performance, que-\\nry performance and storage cost. \\nExperiment environment. We execute our experiments on \\na 24 node physical machine cluster. Each node has the same \\nconfiguration with a 1TB hard disk, 8 GB memory, 64-bit plat-\\nform, and moderate I/O performance, except for Intel Core 12 \\nnodes are i5 and the other 12 nodes are i7. The gigabi t ether-\\nnet is connected by a Dell PowerConnect 5548.  \\nExperiment Content. We conducted two experiments. One \\nin a standalone environment to evaluate the algorithms on \\nkey-cube. The other in a cluster environment to compare the \\nloading, querying and storage performance of Haery with \\ncompetitors. In Haery, there are four main alg orithms: func-\\ntions f, g\\n, h and r. On one hand, function f calculates segment \\nrange by its index, and function h returns the key-cube of the \\nprevious version. These two function are lightweight algo-\\nrithms which contribute little to query performance. On the  \\nother hand, functions g and r are heavyweight algorithms. \\nThe effectiveness of function g is dominated by the lineariza-\\ntion algorithm, so we firstly compared the performance of Z -\\norder and flat Z-order. We also evaluated the performance of \\nbuilding and querying the address tree, which dominates the \\neffectiveness of function r.  \\nIn the cluster environment, the relationship between load-\\ning performance and loaded data volume are compared and \\nanalyzed. Query performance under different queries and \\ndatasets, an d the storage cost, are also compared and ana-\\nlyzed. An extend YCSB [30] is adopted as the query client. \\nSelection of competitors. We compare Haery with Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nThe above competitors are commonly used data s tores in big \\ndata environments. The main difference between them is their \\ndatabase models and data schema. Table 3 compares their \\ndifferences. \\nTABLE 3. DESCRIPTION OF SEVEN COMPETITORS \\n \\nExperiment data. To highlight Haery’s advantages of \\nhigh-dimensionality, accumulation and efficient querying, \\nand to support  the key/value building processes in the next \\ntwo paragraphs, we adopt the generated data. There are three \\nattributes for the dataset scale: number of dimensions, data \\nvolume and number of segments on dimensions. We create \\nfive datasets of increasing scales, as explained in Table 4\\n.  \\nTABLE 4. DESCRIPTION OF FIVE DATASETS \\n \\nLoading Process. The loading process of Haery is carefully \\nplanned because otherwise the accumulative mechanism does \\nnot work. The volume and number of dimensional keys of the \\ndatasets increase from S1 to S5 gradually, so that both values \\nand keys of the larger dataset contains the small er one, and \\nthe query workload is performed on all datasets. Haery ex-\\npends the key-cube when the data and dimensions increases. \\nThe loading processes of other system s follow their default \\nconfigurations. The reason that we use YCSB with a deeply'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n10 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nextended loading module is that the original one does not \\nsupport such an accumulative mechanism.  \\nKey Building. To highlight the accumulative key-cube, the \\nnaming rule of key s is carefully planned. First, datasets are \\naccumulated from S1 to S5, where the larger one contains keys \\nwhich have occurred in the smaller one. Second, as shown in \\nTable 4, let x be the number of dimensional keys, and there \\nare 0.2x trial keys, so that the dimensional key s are named as \\n“K0” to “K(x -1)”, while the trivial keys are named as “K(x)” \\nto “K(1.2x-1)”. By such naming convention, the trivial keys of \\na small dataset overlap the dimensional keys of a larger da-\\ntaset. For example, for S1, the dimensional keys are “K0” to \\n“K9”, and trivial keys are “K10” and “K11”. When the dataset \\nincreases in size to S2 whose dimensional keys are “K0” to \\n“K19”, and trivial keys are “K20” to “K23”, the trivial keys of \\nS1 expend to become dimensional keys in S2. For Hive and  \\nPostgresXL, a predefined schema is required before data is \\nloaded, so we create a table with columns “K0” to “K199” , \\nand let columns “K13” to “K199” be nullable. For HyperDex, \\nwe built five hyperspace for five datasets, each of them con-\\ntains keys of S1 to S5, respectively.  \\nValue Building. For key-value pairs of a row in any da-\\ntasets, there are only two distinct values, one is a random \\nfloat from 0 to 106, named as the numeric value, the other is a \\nrandom 10-characters sequence, named as the textual value. \\nThe values of any  keys are either the numerical one or the \\ntextual one, and the chance of being numeric or textual is fif-\\nty-fifty. Without losing generality,  K0 and K1 are numeric \\nand textual values generated by the YCSB default data gener-\\nation algorithms. According to these, for any datasets, t here \\nare only 2 independent dimensions  and the others are dupli-\\ncation, while equal-width segments on a dimension are also \\nequal-frequency due to  data being uniformly distributed \\nacross its range. Therefore, if the query ranges cover half of \\nthe numeric values and the textual values, respectively, \\nnamely half active segments on each dimension are selected, \\nas a result, 25% (1/22) rows are returned. \\nQuery workload. The query workload is generated by \\nYCSB using the customized Workload E (short ranges)  be-\\ncause Haery focus es on range query but not insert, update, \\nread-by-row-key, or other workload s. There are two attrib-\\nutes of a range query relevant to our experiment: number of \\nqueried dimensions and number of matched segments. In \\nHaery, dimensions which are not in the query conditions, are \\ntreated as ALL (all active segments are m atched). It means \\nthat querying on less dimensions does not means less calcula-\\ntion on key cube.  Therefore, all dimensions are contained in \\nthe query conditions. The matched segments could be more \\nor less. In the customized workload E of YCSB, we define that \\nEi is a query whose conditions covers 1/2i active segments of \\neach dimension, and due to the data distribution and value \\nduplication, the query rate of Ei is (1/2i)2. We implement E1 to \\nE6 because there are at least 64 segments in a dimension, and \\nthe query rates are from 0.25 (1/22) to approximate 0.00025 \\n(1/2\\n12). For other data  stores, the query workloads are the \\nsame as for Haery. \\n \\n8.2 Algorithms on key-cube \\nLinearization \\nWe compared the performance of flat Z-order and Z-order \\nusing different number s of dimensions. The datasets are \\nshown in Table 2 and the average linearization time of the \\ntwo algorithms are compared in Figure 7. As is analyzed in \\nsection 4, flat Z -order and Z -order have the same time com-\\nplexity; the experiment shows their performance is almost the \\nsame. \\n \\nFig. 7. Linearization time of flat Z-order and Z-order \\nFigure 7 shows that the performance of the proposed flat \\nZ-order is almost the same as that of Z -order. When more \\ndimensions are involved, the linearization time of flat Z-order \\nis slightly longer than that of Z -order. With the tiny cost, flat \\nZ-order improves flexibility by allowing the key-cube to be an \\nirregular cube, as mentioned in section 4. \\nAddressing \\nWe analyzed the performance of building an address tree, \\nand addressing ( querying) via an address tree. As per the \\nexplanation in section 4, it is not necessary for the addressing \\ntree to be a full -tree whose leaves are mapped to the finest -\\ngranularity cells. The level of the addressing tree also does \\nnot need to be the maximum  one, but according to the num-\\nber of directories. According to definition 11, an address tree \\nis a quad -tree li nked structure, and the θ-level address tree \\napproximately maps 22(θ-1) directories. A computer with Linux \\nmanaging about 2 15 \\ndirectories is suitable for quick storage \\nand search [31], so that a 9-level address tree, which means 216 \\nmapped directories, is enough for indexing all directories in a \\nnode. In this experiment, we evaluated the performance of \\nbuilding and addressing the address tree with 11,12,13,14 \\nlevels, which are suitable for clusters with 32 (25=220/215), 128, \\n512, 2048 nodes, respectivel y. A large scale data center con-\\ntains thousands of servers, and a middle-size data center may \\ncontain hundreds of servers [32]. Thus the experiment scale of \\na 2048-nodes cluster is sufficient. The experiment results are \\nshown in Figure 8. \\n \\nFig. 8. Time cost of building address tree and addressing  \\nFigure 8\\n-(a) shows that the build cost of an address tree \\n(logarithmic axis) is slight except when the number of levels \\n0\\n400\\n800\\n1200\\n1600\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\nLinearization  Time (s)\\nNumber of dimesions\\nZ-order\\nFlat z-order\\n1.8 \\n5.9 \\n32.5 \\n292.7 \\n1\\n10\\n100\\n11\\n 12\\n 13\\n 14\\nBuilding  Time (s)\\nNumber of Levels\\n(a) Buliding \\n0.039 \\n0.059 \\n0.075 \\n0.093 \\n0.026 \\n0.046 \\n0.056 \\n0.068 \\n0.00\\n0.05\\n0.10\\n11\\n 12\\n 13\\n 14\\nAddressing Time (s)\\nNumber of Levels\\nWithout locality\\nWith locality\\n(b) Addressing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 11 \\n \\nis up to 14. As the number of levels increases, the time of \\nbuilding the address tree increases exponentially because the \\nnumber of nodes in every level increases exponentially. Gen-\\nerally, it is definitely sufficient that the master node, on which \\nthe addr ess tree resides, manages 2048 nodes . Building an \\naddress tree in about 4 minutes is affordable. The cost of \\nbuilding the address tree only happens once, and is counter-\\nacted by the query optimization. \\nFigure 8-(b) describes the average addressing time unde r \\ntwo conditions, addressing without locality and addressing \\nwith locality. “ With locality” indicates that the next queried \\ncell is close to the previous one, and vice versa. In a ll condi-\\ntions, the addressing time is no more than 0.1 second s; it is \\nfast enough for big data queries. The addressing time is line-\\narly related to the level of the address tree. For comparison, \\nwhen the levels of the address tree grow, the time of address-\\ning without locality increases faster, and that with locality \\nincreases more s lowly. In terms of addressing with locality, \\nadjacent nodes can be addressed together since their indexes \\nare continuous. \\n \\n8.3 Performance and Cost of Haery \\nA. Loading \\nSince Hive writes data into HDFS directly without writing \\nmetadata during the loading process, its loading performance \\nis the same as that of HDFS, therefore we do not compare \\nloading performance of Hive with the others. Figure 9\\n shows \\nthe average loading speed of HBase, Cassandra, MongoDB, \\nPostgresXL and HyperDex  on datasets with different data \\nvolumes. In the loading experiment, Haery’s loading perfor-\\nmance on average is 1.5 , 9.8, 1.4 and 37.3  times higher than \\nHBase, Cassandra, MongoDB and HyperDex, respectively.  \\n \\nFig. 9. Comparison of loading speed among six stores \\nAll of these data stores, except HyperDex  employ batch \\nloading. The batch loading process consist s of four steps: (1) \\nuploading the original data files to the nodes; (2) reading the \\noriginal data files and writing  schema-related metadata; (3) \\nsharding\\n the data among nodes; (4) transfer ing original data \\nfiles to the system related file format. For these six data stores, \\nstep (1) is almost the same, Table 5 compares the remaining \\nthree steps and some details are emphasized as the following \\npoints. \\n(1) Compared to HBase, Cassandra and MongoDB, Haery \\nemploys a simpler loading approach, it transmits  the dataset \\nto the nodes, calculates the key-cube, and shards the original \\nfiles to chunk files on each node by a MapReduce task. The \\nformat of chunk files is lightweight because dimensional keys \\nare removed. Due to the metadata, key-cube and cell address, \\nare not stored data but are calculated results, there is no extra \\nwork required to maintain them. The other three stores have \\nto maintain more schema -related metadata . MongoDB also \\ntakes time to transfer TextFile format to BSON file format \\nwhich is complex. \\nTABLE 5. LOADING STEPS OF FIVE DATA STORES \\n \\n(2\\n) The loading speed of PostgresXL, a distributed RBDMS, \\nis faster and stable.  Not only the schema, but also the parti-\\ntion of the distributed table are predefined, and then the load-\\ning process is simpl y parallel insert operations on Postgres \\nnodes. Haery‘s loading speed is 0.834 times slower than Post-\\ngresXL. However, it is still comparable, and in the application \\nscenario of NoSQL database, the predefined schema is infea-\\nsible, while changing table schmea dynamically is not sup-\\nported by PostgresXL. \\n(3) As reported in Figure 4 in [24], HyperDex loads at least \\n4 times faster than Cassandra and MongoDB. However, it is \\nincomparable slow in our experiments. It loads 0.25 MB data, \\napproximate 2000 rows, per second. In [24], the loading per-\\nformance is not discussed, w e infer that the following two \\nreasons lead to the different results: First, a different dataset is \\nused. In [24], there are only 10 7 rows and each row only con-\\ntains 10 attributes; while in our experiments, there are a max-\\nimum 109 rows and e ach row contains 120 keys.  HyperDex \\nperforms an ordered hash function on every value of keys in \\neach row, and then hashes the row to the Hyperspace, finally \\nreplicating the object to each subspace. A massive dataset \\ndemonstrates how costly these calculation are. Second, Hy-\\nperDex does not provide batch loading tools so data is loaded \\nfrom the client machine to the servers row by row. An \\nacknowledgement of the previous load is required before the \\nnext one is executed.  \\n(4) The systems load data slower when more data is load-\\ned, but the downtrend is not significant. For Haery, it is be-\\ncause the cost of expansion and linearization when the data \\nvolume increases. For the other stores, it is because the cost of \\nstep (2) increases with the data volume. Besides, with HDFS, \\nthe cost of locating the file also increases with the data vol-\\nume. \\nB. Query \\nFigures 10 and 11 show the query time of each system on \\nthe test cases ( E1, E2, E3, E4, E5 and E6) using different datasets \\n(S1 to S5). Generally, the query performance of Haery is the \\nbest of the NoSQL database s, and is also better than Post-\\ngresXL in some cases. Query performance of HyperDex is the \\nworst. On an average of all cases, Haery is 4.57x, 4.23x, 3.55x, \\n1.79x, 1.82x and 120.6x faster than Hive, HBase, Cassandra, \\nMongoDB, PostgresXL and HyperDex, respectively.  \\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n24\\n 72\\n 120\\n 168\\n 216\\n 264\\n 312\\n 360\\nLoading \\nperformance\\n (MB/s)\\nData \\nvolume (GB)\\nHaery\\n HBase\\n MongoDB\\nCassandra\\n HyperDex\\n PostgresXL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n12 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\n \\nFig. 10. Query performance of the six data stores \\n \\nFig. 11. Query performance of HyperDex  \\nThe dataset size increases from 36 GB to 360 GB. And for \\nqueries with different query rates (E1 to E6), the query time of \\nmost stores increases with data volumes  except PostgresXL \\nand HyperDex, and also increases with the query rate s. It \\nmatches the common sense expectation of database queries: \\nthe query performce  is better when less data is queried, or \\nless data is returned.  \\nAfter carefully studying these stores, we believe some op-\\ntimizations of Haery benefits the query performance, such as, \\nthe linearization algorithm ensures that adjace nt cells are ad-\\ndressed together, and preferring calculation rather than look-\\nup table when mapping logical space (cell space) to the physi-\\ncal space.  Howerer, the dominant optimization is sharding \\neither by vertical partitions on keys, or by horizontal parti-\\ntions on rows, or both. We explain the results from this point \\nof view in Table 6. Some important points to consider are as \\nfollows: \\n(1) The query process of Haery is similar to Hive, but the \\nperformance is quite a bit better. Hive’s query performance is \\nclose to a “full scan” query. \\n(2) The query process of HBase and Cassandra are similar, \\ntheir vertical partitions benefit performance . However, the \\nhorizontal partitions do not provide a benefit due to the high \\ndimensional queries. \\n(3) Without vertical partitions, MongoDB partit ions data \\nK0 and K1 horizontally . For our test data set, partitions on \\nthese two keys provide a significant benefit  because the val-\\nues of the other keys are the same as these two, however, this \\nis not a general situation.  \\n \\nTABLE 6. COMPARING THE QUERY OPTIMIZATION \\n \\n (4) With b enfits of  predefined schema, PostgresXL skip s \\nthe dataset where the query keys are null, and then only the \\nincreased part of data is sca nned. Referring to Table 4, S1 has \\n36 GB data and 374M rows; S2 increases 36GB data and 187M \\nrows, S3 increases 72GB data and 187M rows due to new rows \\nhave more keys, S4 increases 144GB data and 187M rows, fi-\\nnally S5 increases 72GB data and 74M rows. From S2 to S4, the \\nnumber of increased rows are same. It is why query time on \\nS2 is less than that on S1, and query time on S2 to S4 are almost \\nthe same. So far we are not sure why query performance of S5 \\nis worse than that of S2 to S4 since only 74M rows in S5 are \\nscanned. Maybe the 120-keys table is too wide for PostgresXL. \\nThe performance advantage of PostgresXL is not a fair com-\\nparison to the NoSQL databases because the keys cannot be \\npredefined or predicted, and no mechanism indicates wheth-\\ner a key is absent or not ex pected in  the data files that are \\nscanned. Haery may kno w a key, as a dimensional one, is \\nabsent by querying a key -cube, but Haery does not know \\nwhether the absent one is a trivial key without scanning the \\ntrivial data files. \\n(5) HyperDex also rel ies on a predefined schema. In [24], \\nflexibility is explained to be supported by multiple hyper-\\nspaces in a table, but it does not explain how these hyper-\\nspaces are created automatically, namely schema expansion. \\nIn our experiments, we create five hyperspaces for S1 to S5, \\nwith 12, 24, 48, 96 and 120 keys , respectively. So essentially \\nqueries are performed on different “tables”. It may be the \\nreason why the query performance is related to the increased \\nrow number and query rate . For the same query rate, que ry \\nperformance of S1 is the worst, S2 and S4 are similar, and S5 is \\nthe best. Even so, its performance is not comparable to the \\nother stores. As we explain in Table 6, HyperDex cannot meet \\nthe requirements of high -dimensional queries on high -\\ndimensional data. \\nC. Storage \\nThe storage costs of each system are compared in Figure 12. \\nHaery costs the least storage and HyperDex costs the most. \\nThe storage cost s of Hive and Cassandra are similar to the \\nsize of the original data set. Taking dataset S5 (360 GB) for \\nexample, the storage cost of Hive, HBase, Cassandra, Mon-\\ngoDB, PostgresXL and HyperDex is respectively 2.9x, 4. 7x, \\n2.6x, 5.7x, 3.9x and 8.8x larger than that of Haery. \\n0\\n1000\\n2000\\n3000\\n4000\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(a) E1 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(e) E5 \\n0\\n400\\n800\\n1200\\n1600\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(c) E3 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(d) E4 \\n0\\n1000\\n2000\\n3000\\n4000\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(b) E2 \\nHaery\\nHive\\nHBase\\nMongoDB\\nCassandra\\nPostgresXL\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(f) E6 \\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\nE1\\n E2\\n E3\\n E4\\n E5\\n E6\\nQuery time (\\ns)\\nQuery Case \\nE\\nS1\\n S2\\n S3\\n S4\\n S5'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 13 \\n \\n \\nFig. 12. Storage cost performance on two cases of seven data stores \\nThe experimental results are explained as follows: \\n(1) As explained in the loading experiment, there is no ex-\\ntra data being stored in Hive. Hive directly adopts the origi-\\nnal file format of TextFile to store key -value pair, therefore, \\nthe storage cost remains the same as the original data set.  \\n(2) The storage strategy of Haery is as simple as that of \\nHive. Compared with Hive, Haery saves storage for two rea-\\nsons: First, dimensional keys are not stored because they are \\nfixed and can be inferred by their indexes; Second, ArrayFile \\nand SequenceFile save more storag e than TextFile. Therefore, \\nthe storage cost is even lower than the original dataset. \\n(3) HBase is column oriented. The keys and values are \\nstored as HFile, which contains data blocks and index blocks. \\nThe index block stores three levels of indexes: root index, in-\\ntermediate level index and leaf level index. Storing these ad-\\nditional indexes are a performance cost for HBase. Therefore, \\nthe storage cost of HBase is higher than the original data set. \\n (4) Cassandra is column oriented. The keys and values are \\nstored as SSTable, which contains data files, index files (key \\nindexes) and filter files. Cassandra does not introduce much \\nmetadata to SSTable, it also provides a compaction mecha-\\nnism which merges multiple SSTables into a new one. There-\\nfore, the storage cost of Cassandra is slight ly less than the \\noriginal data set. \\n (5) MongoDB is document oriented, using BSON as the \\nstorage structure. BSON is a binary -encoded serialization of \\nJSON-like documents. Not only are the same keys stored re-\\npeatedly, but also there are massive descriptors to be stored. \\nTherefore, the storage cost is approximately double that of the \\noriginal data set. \\n(6) The storage of PostgresXL on e ach node is a Postgres \\ndatabase. In Postgres, rows are stored in heap files. Heap files \\nare structured as a collection of blocks, each containing a row, \\nnamely a collection of values without keys. This saves storage \\ncompared to the original dataset. But, the system tables and \\nindex on \\nthe primary key take additional storage. Overall, the \\nstorage cost is larger than the original data set. \\n(7) HyperDex replicates the data for each subspaces of a  \\nhyperspace, also there are five  hyperspaces for a table corre-\\nsponding to five datasets . Therfore, HyperDex is the most \\nstorage consuming store. \\n9 CONCLUSION \\nThis paper presents the design, implementation, and evalua-\\ntion of Haery, a column oriented store for big data. Haery is \\nbuilt on Hadoop HDFS and a distributed compu ting frame-\\nwork, e.g. MapReduce. We proposed the following models \\nand algorithms:  \\n(1\\n) Key-cube, which is a high -dimensional data model as \\nthe logical schema of the keys and values;  \\n(2) An i mproved Z -order based linearization algorithm \\nand an address tree, to map the cells to directories;  \\n(3) Accumulation, which is a key-cube expansion approach \\nto maintain the efficiency of the key -cube when new data is \\nimported;  \\n(4) Query algorithms to imp lement queries on key -cubes \\nand physical storage;  \\n(5) The system architecture, components and implementa-\\ntion of Haery.  \\nOur experimental results show that the loading and query \\nperformance of Haery is the most stable and efficient. Due to \\nthe accumulative data models, the query performance in-\\ncreases slowly when the data volume increases. There is no \\nadditional storage cost in Haery, what’s more, the storage cost \\nof dimensional keys are also saved. There are some reasons \\nleading to these advantages.  \\n(1\\n) Haery adopts a pre-defined key-cube which both sup-\\nports high-dimensional data and partitions data into regions. \\nQuery performance improved by greatly red ucing the search \\nspace. \\n(2) With linearization and the address tree, no additional \\nstorage is required to store the key -cube, the mapping rela-\\ntionship between cells and directories are calculated rather \\nthan queried, and the locality of queried data is considered to \\nimprove the addressing performance.  \\n(3) Accumulation ensures that the well -partitioned key -\\ncube remains efficient when new keys and values are import-\\ned to Haery. \\nIn the future, we would optimize Haery as follows: dis-\\ncussing whether the indexes of cells should be compressed, \\nhow to compress them, and whether the compression affects \\nthe query performance. And propose more sophisticated data \\nstructures for the address tree. \\nACKNOWLEDGMENT \\nThis research is supported by the National Natural Science \\nFoundation of China (61672143, 61433008, U1435216, \\n61662057, 61502090, 61402090).  And the Fundamental Re-\\nsearch Foundations for the Central Universities  (N1616020 \\n03). We thank Shenqiang HU, a master student of Software \\nCollege, Northeastern University, for conducting experiments \\nin the revision that greatly improved the paper.  \\nREFERENCES \\n[1] Agarwal S, Priyusha M K. “A Transformation from Relational \\nDatabases to Big Data.” J. International Journal of Advanced Trends \\nin Computer Science & Engineering , 2015. \\n[2] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreos \\nand Samuel Madden, \"The Design and Implementation of Mod-\\nern Column-Oriented Database Systems\", J Foundations and \\nTrends® in Databases: Vol. 5: No. 3, pp 197-280. 2013 \\n[3] Morton, G.M.: “A computer oriented geodetic data base; and a \\nnew technique in file sequencing. ” Technical report, IBM Ltd., \\nOttawa, Canada.1966 \\n[4] Pramod J. Sadalage; Martin Fowler, \"4: Distribution Models\", \\nNoSQL Distilled, ISBN 0321826620, 2012 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 72\\n 108\\n 144\\n 180\\n 216\\n 252\\n 288\\n 324\\n 360\\nActual Storage\\n (GB)\\nData \\nvolume (GB)\\nHaery\\n Hive\\nHBase\\n MongoDB\\nCassandra\\n HyperDex\\nPostgresXL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n14 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\n[5] Wu S, Jiang D, Ooi B C, et al. “Efficient B-tree Based Indexing for \\nCloud Data Processing ” J. Proceedings of the Vldb Endowment , \\n2010, 3(12):1207-1218.  \\n[6] Jagadish H V, Ooi B C, Vu Q H. “BATON: a balanced tree struc-\\nture for peer-to -peer networks ” Proc. International Conference on \\nVery Large Data Bases. VLDB Endowment, 2006:661-672. \\n[7] Wang J, Wu S, Gao H, et al. “Indexing multi-dimensional data in \\na cloud system ” Proc. ACM SIGMOD International Conference on \\nManagement of Data , SIGMOD 2010, Indianapolis, Indiana, Usa, \\nJune. 2010:591-602. \\n[8] Zhang X, Ai  J, Wang Z, et al. “An efficient multi -dimensional \\nindex for cloud data management ” Proc. International CIKM \\nWorkshop on Cloud Data Management , Clouddb 2009, Hong Kong, \\nChina, November. 2009:17 -24. \\n[9] Papadopoulos A, Katsaros D. “A-Tree: Distributed Indexing of \\nMultidimensional Data for Cloud Computing Environments ” \\nProc. IEEE Third International Conference on Cloud Computing \\nTechnology and Science. IEEE, 2011:407-414. \\n[10] Alternate indexed hbase implementation; speeds scans by add-\\ning indexes to regions rather secondary tables George L. Availa-\\nble on https://issues.apache.org/jira/browse/HBASE-2037. 2017  \\n[11] Indexed HBase. “An extnestion of HBASE core which support \\nfaster scans at the expense of larger RAM consumption. ” \\nhttps://github.com/gkulbak/ibase. 2017 \\n[12] Zou Y, Liu J, Wang S, et al. “CCIndex: A Complemental Cluster-\\ning Index on Distributed Ordered Tables for Multi -dimensional \\nRange Queries ” Proc. Ifip International Conference on Network and \\nParallel Computing. Springer-Verlag, 2010:247-261. \\n[13] Nievergelt J. “The Grid File: An Adaptable, Symmetric Multikey \\nFile Structure .” J. Acm Transactions on Database Systems , 1984, \\n9(1):38-71. \\n[14] Bentley J L. “Multidimensional binary search trees used for as-\\nsociative searching” J. Communications of the Acm, 1975, 18(9):509-\\n517. \\n[15] Guttman A. “R-trees: a dynamic index structure for spatial \\nsearching” J. Acm Sigmod Record, 1984, 14(2):47-57. \\n[16] Nishimura S, Das S, Agrawal D, et al. “MD-HBase: A Scalable \\nMulti-dimensional Data Infrastructure for Location Aware Ser-\\nvices” Proc. IEEE International Conference on Mobile Data Manage-\\nment. 2011:7-16. \\n[17] Liu Y, Liu Y, Kaikuo X U, et al. “SHG-Tree: An Efficient Index \\nStructure of Spatial Database ,“ J. Journal of Frontiers of Computer \\nScience & Technology, 2009. \\n[18] Dong Daoguo, Liang Liuhong, Xue Xiangyang. “VAR-Tree-A \\nNew High-Dimensional Data Index Structure .” J. Journal of Com-\\nputer Research & Development, 2005, 42(1):10 -17. \\n[19] Liang J J, Feng Y C. “CSA-Tree: an optimized high-dimensional \\nindex tree for main memory access ” J. Chinese Journal of Comput-\\ners, 2007, 30(3):415-423. \\n[20] Lejsek H, Asmundsson F H, Jonsson B T, et al. “NV-tree: an \\nefficient disk -based index for approximate search in very large \\nhigh-dimensional collections. ” J. IEEE Transactions on Pattern \\nAnalysis & Machine Intelligence, 2008, 31(5):869-883. \\n[21] Mior M J, Salem K, Aboulnaga A, et al. “NoSE: Schema design \\nfor NoSQL applications ” Proc. Data Engineering (ICDE) , 2016 \\nIEEE 32nd International Conference on. IEEE, 2016: 181-192. \\n[22] Vajk T, Deák L, Fekete K, et al. “Automatic NoSQL schema de-\\nvelopment: A case  study” Proc. Artificial Intelligence and Applica-\\ntions. 2013: 656-663. \\n[23] Escriva R., Bernard W., and Emin G . S. \"HyperDex: A distribut-\\ned, searchable key-value store.\" Proc. ACM SIGCOMM 2012 con-\\nference on Applications, technologies, architectures, and protocols for \\ncomputer communication. ACM, 2012. \\n[24] Escriva R., Bernard W., and Emin G. S. \"HyperDex: A distribut-\\ned, searchable key-value store for cloud computing.\" Computer \\nScience Department, Cornell University Technical Report \\n(2011). \\n[25] Chevalier M., El Malki M., Kopliku A., Teste O., Tournier R. \\nHow Can We Implement a Multidimensional Data Warehouse \\nUsing NoSQL?. In: Hammoudi S., Maciaszek L., Teniente E., \\nCamp O., Cordeiro J. (eds) Enterprise Information Systems. Lec-\\nture Notes in Business Information Processing, vol 241. Springer, \\nCham. 2015 \\n[26] Iqbal A, Wang H, Gao Q. “A Histogram Method for Summariz-\\ning Multi-dimensional Probabilistic Data ” J. Procedia Computer \\nScience, 2013, 19:971-976. \\n[27] Song J, Guo C, Wang Z, et al. “HaoLap: a Hadoop based OLAP \\nsystem for big\\n data.” J . Journal of Systems and Software, 2015, 102: \\n167-181. \\n[28] Asano T, Ranjan D, Roos T, et al. “Space Filling Curves and \\nTheir Use in the Design of Geometric Data Structures ” Proc. Lat-\\nin American Symposium on Theoretical Informatics. Springer -\\nVerlag, 1997:3-15 \\n[29] Mokbel M F, Aref W G, Kamel I. “Analysis of Multi-\\nDimensional Space-Filling Curves ” J. GeoInformatica, 2003, \\n7(3):179-209. \\n[30] Cooper, Brian F., et al. \"Benchmarking cloud serving systems \\nwith YCSB.\" Proc. the 1st ACM symposium on Cloud computing . \\nACM, 2010. \\n[31] Roger Nelson, “ How many sub-directories should be put into a \\ndirectory”. https: //stackoverflow.com/questions/494730/ how-\\nmany-sub-directories-should-be-put-into-a-directory. 2017  \\n[32] Kai H. , Jack D. , Geoffrey C. F.. “Cloud computing and distrib-\\nuted systems: from parallel processing to the Internet of things ” \\nM, Morgan Kaufmann Publishers Inc , 2011 \\n \\n \\n \\n \\nJie Song Ph.D, \\nassociate professor, \\nsongjie@mail.neu.e\\ndu.cn, his research \\ninterests include big \\ndata management  \\nand energy-efficient \\ncomputing. \\n  \\nHongYan HE  M.S \\ncandidate, her \\ncurrent research \\nfocuses on big \\ndata management. \\n \\n Richard Thomas  \\nPh.D., senior lec-\\nture, rich-\\nard.thomas@uq.edu\\n.au, his research \\ninterests include big \\ndata and software \\nengineering. \\n  \\nYubin BAO Ph.D., \\nprofessor, \\nbaoyb@mail.neu.e\\ndu.cn, his re-\\nsearch interests \\ninclude big data \\nmanagement. \\n \\n \\nGe Yu Ph.D., pro-\\nfessor, \\nyuge@mail.neu.edu.\\ncn, his research \\ninterests include \\ndatabase theory.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1'}, page_content='Nhan Nguyen \\nBuilding an E-commerce Application \\nUtilizing Firebase Cloud service  \\nMetropolia University of Applied Sciences \\nBachelor of Engineering \\nMobile solutions \\nBachelor’s Thesis \\n28 March 2022'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2'}, page_content='Abstract  \\nAuthor: Nhan Nguyen \\nTitle: Building an E-commerce Application Utilizing Cloud \\nservice \\nNumber of Pages: 33 pages + 1 appendices \\nDate: 28 March 2022 \\nDegree: Bachelor of Engineering \\nDegree Programme: Information of Technology \\nProfessional Major: Mobile Solutions \\nSupervisors: Antti Piironen, Principal Lecturer \\n \\nThe primary purpose of the thesis is to illustrate the way of establishing a serverless \\nbackend for the application by utilizing Firebase services. Besides, the thesis also \\ninvestigated the implementation and functioning of Firebase as well as its services. \\n \\nThe thesis is to establish a conceptual framework of Firebase services via an \\nexamination of Firebase documents and the implementation of a project utilizing \\nFirebase services and tools. Additionally, this research demonstrates how to create \\nFirebase project in the Firebase interface and how to integrate Firebase SDKs into the \\napplication. \\n \\nThe thesis project culminates in the creation of an e -commerce application prototype. \\nThe application’s objective is to enable individuals to purchase and sell things related \\nto their own demands. The E -commerce website exists as a practical tool in order to \\nhelp firms handle orders, receive payments and manage logistics. All key aspects of \\nthe working prototype application, in general, were successfully implemented, apart \\nfrom a few that were excluded for different causes. \\n \\nApplying serverless backend service such as Firebase brings a huge benefit to people, \\nespecially developers, in terms of the significant reduction of maintenance of the \\ntraditional backend. \\nKeywords: Firebase, React, web applications'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3'}, page_content='Contents \\nList of Abbreviations \\n1 Introduction 1 \\n2 ReactJS 3 \\n2.1 React Virtual DOM 3 \\n2.2 Data Flow 5 \\n2.3 React Components 6 \\n2.4 JSX Syntax 6 \\n3 Firebase 8 \\n3.1 Overview 8 \\n3.2 Firebase Authentication service 9 \\n3.3 Cloud Function for Firebase service 11 \\n3.4 The Real-time Database services 12 \\n3.5 Firebase Hosting service 14 \\n3.6 Cloud Storage for Firebase service 16 \\n4 Implementation and result 18 \\n4.1 Project environment 18 \\n4.2 Project structure 19 \\n4.3 Elements of web application 20 \\n4.3.1 HTML 20 \\n4.3.2 Stylesheets CSS 18 \\n4.3.3 Javascript 19 \\n4.4 Firebase configuration 24 \\n4.5 Interface layout 25 \\n5 Deploying the website 29 \\n6 Testing 30 \\n7 Discussion 31 \\n8 Conclusion 33 \\nReferences 1'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 3, 'page_label': '4'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5'}, page_content='List of Abbreviations \\nAJAX: Asynchronous JavaScript and XML is a collection of client -side web \\ndevelopment tools for creating asynchronous web applications that \\nmay transmit and receive data without interfering with other \\ncomponents of the user interface. \\nAPI: The Program Programming Interface (API) is a collection of public \\nmethods and attributes that your application utilizes to communicate \\nwith other objects. \\nDOM: The Document Object Model (DOM) is an HTML and XML document \\nprogramming interface. It depicts the page so that programs may \\nalter the structure, style, and content of the document. It uses nodes \\nand objects to represent the document. \\nJSON: JSON stands for JavaScript Object Notation and is a lightweight data \\nstorage and transmission format. When data is transmitted from a \\nserver to a web page, this is often utilized. \\nNpm: Node package manager is a package manag er that manages the \\nexternal code dependencies of an application. \\nReact: It is a JavaScript framework and tool for creating user interface \\ncomponents. \\nRedux: It is a state container for JavaScript programs that is predictable. \\nREST: Representational State Transfer is a software architecture style that \\nestablishes web -based protocols for communication between \\ncomputer systems. \\nUI: User Interface refers to the area where people and machines \\ninteract.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6'}, page_content='JSX: Javascript XML is a React syntax that enables HTML to be included \\nin JavaScript code. \\nMVC: Model–view–controller is a software design pattern that is often used \\nin the development of user interfaces.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, page_content=\"1 \\n \\n1 Introduction \\nThe web has progressed from serving static HTML content to serving application-\\nlike complex user interfaces that are sometimes as capable as their native \\nequivalents. By contrast, Web browsers have stayed relatively stable throughout \\ntime. \\nStatistics show that mobile devices accounted for 52.2% of total internet traffic in \\n2019. However, it takes an average of 22 seconds for a mobile landing page to \\ncompletely load. When you consider that 53% of users will abandon a mobile \\nwebsite if it takes mor e than 3 seconds to load, it  is no surprise that improved \\nmobile experiences are in high demand. \\nA huge percentage of web users have switched to native apps nowadays. This \\nsector has developed particularly rapidly in the e -commerce market, with a 54 \\npercent rise in 2017 alone. In exchange, spending time in browsers has declined \\ndramatically, and web developers are increasingly looking for methods to stay up \\nwith the experiences that native apps provide. \\nJavaScript is currently one of the most powerful web d evelopment programming \\nlanguages available. JavaScript allows programmers to create extremely \\nresponsive websites with dynamic functionality that responds to user requests, \\nenhancing the user experience. \\nReact generates a memory -based data structure cache.  When the code is \\namended, React detects the changes and updates the browser accordingly. This \\nunique feature will improve the webpage's efficiency since the React library only \\nrenders components that change, rather than loading the whole page as in other \\nmethods. That is why, in this thesis, React, an open-source JavaScript toolkit, is \\nutilized to create a user-friendly, scalable, and high-performance homepage.  \\nThe purpose of this thesis  is to create a working responsive e -commerce web \\napplication for an online retailer using React and Firebase t o assist businesses\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8'}, page_content=\"2 \\n \\nin handling orders, receiving payments, and managing logistical processes and \\nprocedures. The React will handle the work in front -end, meanwhile, Firebase \\ncloud service helps manage the data and authentication. \\nThe thesis is divided into 8 sections. The sections of this thesis are organized in \\nthe same order as the phases of implementation of the project that was \\ndeveloped in this thesis. Sections 1 and 2 provide an overview of the technologies \\nthat will be used in the projects. The first section is devoted to ReactJs. The \\nsecond section delves deeper into Firebase. Additionally, the purpose of this part \\nis to analyze the advantages of using numerous services on Cloud Firebase. \\nFollowing the project's execution stages, Sections 4, 5, and 6 cover everything \\nfrom the design of the architecture to specific examples, all while discussing the \\ntechnologies that were employed to reach the ultimate result. Section 6 focuses \\non the testing procedure. Section 7 discusses the application's outcome and the \\nconclusion of the thesis. Finally, there are parts dedicated to observations on and \\ndebate of the whole endeavour.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9'}, page_content='3 \\n \\n2 ReactJS \\nReact is a library for designing composable user interfaces. As a result, \\nreusable UI components are more likely to be created, and data that changes \\nover time may be shown. The V in MVC is often replaced with React. React \\ntakes away the DOM from you, enabling a simpler programming approach and \\ngreater performance. \\n2.1 React Virtual DOM \\nThe DOM is an acronym for Document Object Model. It is commonly referred to \\nplay an important role of the contemporary internet nowadays. It is basica lly an \\nabstraction of how online documents look and work. However, it is not as fast as \\nother JavaScript operations since most of frameworks update not the part but \\nwhole the DOM without requirement. Although they are  not essential, these \\nupdates are made automatically even if you do not ask for them. Assume that \\nnine goods have been placed in a shopping cart at an online web store. For \\nexample, changing a name in a list of (n) names would cost (n+1) times harder \\nfor a normal technology to recreate exact the same amount of names in that list. \\nThe Virtual DOM is existed before the creation of React, however, React makes \\nit accessible to developers for free. Virtual Dom is technically representation of \\nthe HTML Document Object  Model. For any DOM object, for instance a \\ncorrespondent or a light copy .  React has a virtual DOM object  of its own . The \\ncharacteristics of a virtual DOM are comparable to those of a real DOM. On the \\nother hand, it is unable to alter the perspective immed iately. It takes a long time \\nto manipulate the DOM. Update of Virtual DOM, on the other hand, is quicker \\ncomponent as well as does not alter the screen.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10'}, page_content='4 \\n \\n \\nFigure 1. How the DOM working in React [1] \\nAs shown in Figure 1, the virtual DOM in React is a duplicate of the actual DOM \\nthat is used for testing. React employs a technique known as \"diffing,\" which \\nmeans that when a JSX element is rendered, every single Virtual DOM element \\nis changed. This may seem to be inefficient, however, it is not since Virtual DOM \\nis very quick to update and has no influence on the overall process. In order to \\ndetect which virtual DOM has been altered, React compares an updated state of \\nthe DOM with an earlier state of the DOM. When React recognizes that the DOMs \\nhave been modified, it only updates the objects that have been modified to the \\nactual DOM. \\nAs a result, React accelerates the updating process by using Virtual DOM. In the \\nabove example, React would have just updated the changed item, leaving the \\nother things unchanged. When changing a page in an application, this makes a \\ndifference since React may only make modifications to the areas of the DOM that \\nare necessary. A large part of the rationale for React increasing popularity within \\nthe developer community is due to this virtual DOM modification mechanism. \\nWhen utilizing React Virtual DOM, there are benefits and drawbacks to consider. \\nThe use of JSX and hyper script enables us to create multiple frontends for a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, page_content=\"5 \\n \\nsingle application; allow the creation of apps without having to consider state \\ntransitions; no need for React to be used simple and effective in raising output; \\nReact diffing techniques are very quick and efficient are just a few of the many \\nadvantages of the ReactJS library. Mean while, React has a few drawbacks, \\nincluding a lack of differentiation between static and dynamic components and \\nthe use of a substantial amount of memory. In memory, a full copy of the DOM is \\npreserved. \\n2.2 Data Flow \\nFramework, for example, like Angular use tw o-way data binding. When using \\nAngular's two -way data binding, for example, changing the model instantly \\nupdates the view and the other way around. An input field in the model may also \\nalter the model. It works well in the majority of cases, however switch ing to a \\ndifferent model may trigger cascade changes in other models. Again, because \\nthe state is modifiable by both view and controller, the data flow might be \\nunexpected in certain instances. Flux or Redux with React might be a better \\napproach to prevent such uncertainties as both designs follow one-way data flow. \\nCascading updates and modifications are not visible in a one-way data flow.  \\nAn application's states and models may be more tightly coordinated when data is \\nsent in a single path across the syst em. Additionally, a one -way data flow \\nsimplifies and clarifies the design. Flux architecture is a functional approach. In \\nthis case, the view is seen as a result of the current state of the application. \\nEventually, if the state receives some modifications the view also gets re -\\nrendered 16 automatically. In addition, the states provide a comparable \\nperspective, which aids in the application's comprehension and predictability. \\nData from parent to child in an application flow in a single path to increase \\npredictability. Any data may be updated from any view, anytime under this \\ntechnique. This also simplifies debugging if anything goes wrong.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, page_content='6 \\n \\n2.3 React Components \\nComponents are critical in React. To form the full user interface, these individual \\ncomponents are then layered within one another. When using components, it \\nbreaks up user interface into smaller, more manageable sections so that \\ndeveloper can focus on each one individually. The term UI refers to the user \\ninterface, or what is shown on the screen. Components  operate similarly to \\nJavaScript functions. They execute the exact same goal, but in quite distinct \\nenvironments and with very different techniques. They, like functions, accept \\nprops as inputs and return React components. These components define what \\nthe user sees on the screen while interacting with the interface. React \\ncomponents may be used to build a whole interface or simply parts of it. \\nReact components may be built in the same way that a JavaScript function is. \\nThis method takes props as arguments and returns text of html which normally is \\ncalled as JSX. These components are referred to as functional components. \\nAdditionally, a React component may be generated in a variety of various \\nmethods. Extending, inheriting, or deriving a class from  the primary component \\ntied to an object is another method of creating a component. [2] \\nAdditionally, stateless functional components are possible. By rendering each \\ncomponent, the user interface experience becomes more responsive and \\nefficient. \\n2.4 JSX Syntax \\nJSX is neither a string nor a hypertext markup language. It is a JavaScript \\nsyntactic extension with statically typed syntax. It is comparable to an object -\\noriented programming language that is optimized for use with current web \\nbrowsers. To design and develop the user interface, it is advised to utilize JSX in \\nconjunction with React. While it has all of the capabilities of JavaScript, it may'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13'}, page_content=\"7 \\n \\nseem to be a template language at first look. JSX is used to generate the React \\nelement. It is also capable of being displayed to the React Virtual DOM. \\nTo begin, it is faster: Because the JSX source code is converted to JavaScript, \\nthe outcome is highly optimized. When compared to similar JavaScript code, JSX \\nproduced code is quicker. JSX has been shown to be 13 % quicker on iOS and \\n29% faster in Android. \\nSecond, it is more secure: JSX is statically coded and largely typesafe, in contrast \\nto JavaScript. When programs are created with JSX, their quality improves \\nsignificantly, since many problems are discovered dur ing the compilation \\nprocess. Additionally, it provides debugging capabilities at the compiler level.  \\nTo round things up, JSX provides a class structure that is extremely comparable \\nto Java, which means that developers no longer have to deal with JavaScrip t's \\nbasic prototype-based inheritance system. However, since the expressions and \\nstatements in JSX are almost comparable to those in JavaScript, programmers \\nalready familiar with JavaScript may begin using it right away. Additionally, there \\nare proposals f or language -services for editors / integrated programming \\nenvironments (IDEs), such as code completion to make coding easier. \\nTo assist with the presentation of nested components, JSX elements may be \\ngiven as children. Different kinds of children may be co mbined, allowing for the \\nusage of JSX children with string literals. This is another JSX attribute that \\ncorresponds to an HTML element. [3] \\nMultiple children may be added to a JSX expression. Therefore, it must be \\nincluded in a div if the component is requ ired to render several items. Within \\nenclosing, JavaScript expressions may be passed as children.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14'}, page_content=\"8 \\n \\n3 Firebase \\nFirebase developed from Envolve, a former firm started by James Tamplin and \\nAndrew Lee in 2011 offer a backend development solution. The feature \\ncombination in Firebase speeds the integration of cloud databases into online \\nand mobile applications. Firebase is a Backend as a Service platform that \\nsignificantly lowers configuration and setup time. \\n3.1 Overview \\nFirebase is a Cloud -hosted, NoSQL database tha t employs a document -model. \\nWhile allowing for real -time data sharing and syncing across users, it can be \\nscaled horizontally. This is ideal for cross-platform apps, such as those for mobile \\nphones. In addition to server -based applications, Firebase's robu st user-based \\nsecurity makes it ideal for off-line usage. \\nFirebase is a self-scaling service built on top of Google's infrastructure. Firebase \\nadds analytics, authentication, performance monitoring, messaging, and crash \\nreporting to the features of a typic al NoSQL database. The integration is \\nextensive, given it is a Google product. Additionally, Google Ads, the Google Play \\nStore are also integrated with AdMob. \\nCreating a Firebase application from the ground up is not difficult. As seen in \\nFigure 2, the app lication may be classified as developing, growing, or earning. \\nDevelopers are able to make their own selections to use these pillars. Certain \\ntechnologies, such as A/B Testing, Analytics, App Distribution are provided free \\nof charge.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15'}, page_content='9 \\n \\n \\nFigure 2. The overview of Firebase Cloud service [4] \\n3.2 Firebase Authentication service \\nCurrently, many online services rely on authentication in order to identify users \\nas well as safeguard data by limiting access. For example, Google requires an \\naccount and password to access some programs. The authentication process \\nhas been complicated by the addition of a third -party authentication mechanism \\n- APIs. Thus, A simple API that allows users to log in through federated providers \\nis provided by Firebase to complete the authentication process. It also works in \\nconcert with the real-time database to limit access, as demonstrated in Figure 3.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16'}, page_content=\"10 \\n \\n \\nFigure 3. Firebase Authentication service [5] \\nGoogle, GitHub, Facebook, and Twitter are all popular federated providers. If \\ndevelopers alter Firebase, they do not need to sign in again, since it is already \\nconnected with these providers' authentication systems. A connection to the \\nauthentication system will be establ ished as soon as an account has been \\nvalidated in the database. \\nPassword recovery and user verification are both available via the Firebase user \\ninterface. Users may create a temporary account without revealing their identity, \\nwhich is then linked to their  federated provider -based account. Additionally, it \\nsupports a technology named Smart Lock in remembering and signing in \\nautomatically using the credentials. The Firebase authentication SDK requires \\nthe user's email address as a sign-in credential.  \\nFirebase Auth requires the least amount of time to implement into your code. The \\nreal Firebase authentication project takes more time to develop than Firebase \\nAuth. It is an online library that enables clients to alter typical Firebase situations \\nin order to give user authentication implementations in the form of user interface \\nscreens. Firebase Auth supports a variety of authentication methods on the web \\nand on mobile devices and is very simple to use. There are three primary stages \\nto integrating Firebase Auth user authentication into apps such as configuring\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, page_content=\"11 \\n \\nsign-in methods, configuring the sign-in UI, and completing the sign-in flow using \\nFirebase Auth. \\nThe Firebase Authentication SDK is somewhat more time -consuming than the \\nprevious one. Additionally, it enab les authentication process verification over \\ncomplete management of the outlook. Eventually, this option will maintain a \\ncomprehensive authentication process that includes several stages to \\nauthenticate the user's account. It is used to perform a variety o f functions, \\nincluding sign-in and sign-out, automated log-in, and data updating for new users. \\nAdditionally, the Firebase Authentication SDK facilitates connection with identity \\nproviders and anonymous authentication. Firebase Authentication is generally \\napproached via the Firebase Authentication SDK. There are the following steps \\nsuch as establish a sign -in method, create user interface flows for your login \\nmethods, and provide the user's credentials to the Firebase Authentication SDK. \\n3.3 Cloud Function for Firebase service \\nAs seen in Figure 4, Cloud Functions is a component of the Firebase and Google \\nCloud platforms that enables developers to handle fewer configurations, such as \\napp updates and general security for Google services and Firebase products. A \\nCloud function connects the Firebase application to the Google server, which \\nexecutes it. It minimizes boilerplate code by generating an admin SDK, third-party \\nservices, and a Cloud Function. The command is used to scale the design's \\nusage automatically depe nding on the computing data. As a result, human \\nmaintenance is not required for updating settings, credentials, or \\ndecommissioning. Cloud Function assumes users' security and privacy \\nobligations. Following the addition of a command line, the Cloud Function  is \\nisolated from the application logic, allowing the client to freely deploy code on the \\nprimary server. Since a result, developers choose to master their application, as \\nthese are delivered through Google and Firebase features.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18'}, page_content=\"12 \\n \\n \\nFigure 4. Cloud Function for Firebase [6] \\nThe Firebase project is deployed with a specified provider to enable this \\ncapability. Then, using JavaScript, the Cloud Function and Firebase CLI are \\ninstalled to manage the desired services. Through the Firebase CLI, the Firebase \\nconsole may be used to control and see a function: create a Cloud Function, write \\nit, deploy it, and monitor it. \\n3.4 The Real-time Database services \\nTraditionally, real -time databases have been used to demonstrate how a \\ndatabase system may overcome the real -time limitation in order to create a \\nreliable database system. Due to the fact that it offers an easy API -based \\ncapabilities, this is Firebase's flagship service. When new information is added, it \\nis automatically synced as JSON across all devices. A real-time database is one \\nthat works with both web and mobile versions on the same device and exchanges \\ndata between devices in an extremely short period of time, whereas a batch \\ndatabase takes a long time to sync.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19'}, page_content='13 \\n \\n \\nFigure 5. The example of Firebase real-time database. \\nIt restricts access to particular information by using a few codes provided by the \\nclient. No operation or maintenance server is necessary since the cloud service \\nprovides a real -time database that is ac cessible from anywhere (based on \\nNoSQL). The Real-time database, like the other products, is available for Android, \\niOS, Web, C++, Unity Platform, and JavaScript, in addition to the other platforms \\nmentioned above. There are up to 100 000 concurrent connec tions and 1,000 \\nconnections per second supported by each database in Real-time Database. \\nIn order to increase client access time, the data set is adjusted and improved due \\nto the use of a new database and integration of the real-time database with Cocoa \\nPods or Gradle, among other factors. Additionally, this technique allows for \\nwriting to take place even when the client is not connected, while still \\nguaranteeing client security and confidentiality. For the purpose of constructing a \\nFirebase Real-time Database, the following are the five basic stages: Configure \\nData and Listen for Changes, enable Offline Persistence, and safeguard your \\ndata by integrating the Firebase Realtime Database SDKs.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20'}, page_content='14 \\n \\nIn order to do this, the Firebase SDKs platform, which communicates  with the \\nFirebase Cloud service, was established. Administrators have the ability to read, \\nprogrammatically send, access, and establish Firebase Auth via the \\nAdministration SDK. To understand how Cloud Storage works, consider the \\nfollowing steps: connect the Firebase SDKs for Cloud Storage, create a \\nreference, upload or download your files, and safeguard them. \\n3.5 Firebase Hosting service \\nWeb developers may use Firebase Hosting to host not just static websites, but \\nalso dynamic websites that are entirely built of HTML, CSS, and JavaScript files. \\nIn order to satisfy a range of needs, it is a software that maintains static web -\\nhosting services. Because HTTPS is based on SSL, consumers gain from faster \\nwebsite access, while developers profit from the ability to launch web applications \\nwithout registering an HTTPS connection certificate with the server. The Firebase \\nCommand Line Interface (CLI) allows for the creation and processing of Firebase \\nin a matter of seconds.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21'}, page_content='15 \\n \\n \\nFigure 6. The flow of using Firebase Hosting [7] \\nWhen Firebase Hosting is used in conjunction with a contemporary website, the \\nmaterial is totally protected using zero -configure SSL. The chemicals are \\nreceived quickly since Firebase Hosti ng is cached on SSDs and CDN edges. \\nAdditionally, it has an undo feature in case of development errors, making \\nFirebase hosting a complete management solution. \\nTo run Firebase Hosting, the Firebase CLI is considered a full -featured solution \\nfor websites, a pplications, and Progressive Web Apps (PWA). The following \\nphases should be noted for front -end implementation of features, infrastructure, \\nand customized tooling: Install the Firebase command-line interface (CLI), create \\na project directory, and publish the site.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22'}, page_content='16 \\n \\n3.6 Cloud Storage for Firebase service \\nVideos, images, and documents are stored online using Firebase Cloud Storage, \\nand they are backed up using Google Cloud Storage (which is free). The \\ninfrastructure for processing large files is created and mainta ined via the use of \\na clear API. As a result, the Realtime Database Rule enables for the distribution \\nof documents to certain individuals. This shows that since it is associated with \\nFirebase Authentication, the storage has been cached for faster access. \\nUploading and distributing files from mobile devices that are accessible to both \\nGoogle Cloud and Firebase is straightforward with the help of Firebase SDKs, \\nwhich are available on the Firebase website. Due to the automated scaling \\ncapabilities of Cloud Sto rage, there is no need to move the document to a \\ndifferent storage service provider. Depending on the file type, filename, or file \\nsize, declarative security for Firebase files may be given. Although the network is \\ndisqualified, Firebase SDKs continue to o perate in an unbroken manner; for \\nexample, if a video is not successfully uploaded, the movie will resume \\ntransmission from the point at which it was interrupted when a proper internet \\nconnection is obtained. \\n \\nFigure 7. An example of Cloud Storage for Firebase service'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23'}, page_content='17 \\n \\nFor this purpose, the Firebase SDKs platform was created to communicate with \\nthe Firebase Cloud service, which was developed by Google. Firebase \\nAuthentication may be read, programmatically sent, and accessed via the Admin \\nSDK, which is available only in a privileged context. To demonstrate how Cloud \\nStorage works, the following procedures are taken: Creating a reference, \\nuploading or downloading your files, and safeguarding them are all possible using \\nthe Firebase SDKs for Cloud Storage.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24'}, page_content='18 \\n \\n4 Implementation and result \\nIn this section, the essential phases of the development process and the results \\nproduced are described. A well performing website is constructed with five main \\npages. The Authentication page is the webs ite entrance that asks user \\nidentification, which is accomplished by provide username and password. \\nAuthentication is needed to access an entire site. The Homepage enables visitors \\nto do a variety of tasks, including seeing a list of goods with photographs, filtering \\nthe products by name as well as category, selecting the products which is \\ndisplayed in the Cart page. In general, the Order page summarizes all user -\\nplaced orders. Following that comes the Cart page. The Cart page enables \\nvisitors to browse the list of items. Administrator accounts enable users to control \\nall items, users, and orders from the admin page. \\n4.1 Project environment \\nPrior to developing the application, the environment must be configured. The \\nNodeJS version 13.12.0 is the first item on the list of the tools that are being \\nused here. Essentially, NodeJS is a tool that supports JavaScript on both the \\nclient and the server. Node.js is a ground-breaking platform that supports both \\nthe client and the server. \\nThe next tool is Visual Studio Code, which is one of the most popular software \\ndevelopment tools today since it is packed into a single graphical user interface \\nand includes certain snippets such as ES7 React/Redux to make writing easier \\nand more convenient. In addition, the Firebase CLI version 10.5.0 maintains, \\nviews, and deploys Firebase projects using a variety of tools, and from there, \\nwe can configure communication between the Firebase Cloud and a local \\nproject, which is the last step.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25'}, page_content=\"19 \\n \\n4.2 Project structure \\nFigure 8 depicts the project 's structure, which is separated into three major \\nelements. The index.html file is the primary HTML file for our application, which \\ncontains your React code and serves as a container for React to render. The \\nproject's CSS is to style its apps and component s. The style property adds \\ndynamically calculated styles to React applications at render time. It takes a \\nJavaScript object instead of a CSS string. CSS files are divided into two major \\nfiles: index.css and App.css. App.js, the JavaScript file that contain s the other \\ncomponents, functions as a container for them, and redux is vital to the \\napplication's operation. The last portion contains the file fireConfig.js for \\nconfiguring Firebase.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26'}, page_content='20 \\n \\n \\nFigure 8. The structure of project \\n4.3 Elements of web application \\n4.3.1 HTML/CSS and JavaScript  \\nThe HTML core section is seen in figure 9. This file includes the links to external \\nCSS sources, which is Bootstrap, in this project. The web application is assigned \\nthe “root” id.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27'}, page_content='21 \\n \\n \\nFigure 9. The screenshot of HTML file \\nCSS specifies how HTML components should appear on screen, in print, or in \\nother media. CSS eliminates a great deal of labour. It is capable of simultaneously \\ncontrolling the layout of many web pages. In this appl ication, the stylesheets \\ninclude Index.css, App.css, authentication.css, layout.css as well as \\nproducts.css. \\n \\nFigure 10. The screenshots of product.css file'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, page_content='22 \\n \\nThe application was built by multiple parts which are Authentication, Homepage, \\nOrders and Cart. In terms of Authentication, there are two pages which are \\nrespectively Login and Register pages. They are the website entrance that asks \\nfor user identification, which is accomplished by providing username and \\npassword. Authentication is needed to access an entire site. The Homepage \\nenables visitors to do a variety of tasks, including seeing a list of goods with \\nphotographs, and filtering the products by name as well as category. Selecting a \\nparticular product link  to the product detail. The next one is the Order page. In \\ngeneral, the Order page summarizes all user placed orders. Following that comes \\nthe Cart page. The Cart page enables visitors to browse the list of items. \\nAdministrator accounts enable users to con trol all items, users, and orders from \\nthe admin page. Using the React -router-dom package ensures that pages are \\nnavigated accurately and quickly. \\nThe following list of additional components is shown in Figure 11. Considering \\nthe layout of the website, the re are three main parts which are respectively \\nFooter, Header and Layout. First, component Footer is used to display the footer \\nof the page. Secondly, component Footer is used to display the footer of the page. \\nLast, Component Layout is the custom componen t combine the Header, Footer \\ncomponents and the content to be passed to the Layout. Aside from that, the \\napplication was developed as part of a larger set of components. Component \\nLoader is simply the spinner which is displayed when loading resources. \\nComponent AdminPage, which is only visible to admin users, contains three \\nprimary sections: a list of users, a list of items that may be edited/removed, and \\na list of user orders.  Component CartPage displays the list of products ready to \\ncheck out. Component Homepage allows users to do a range of functions, \\nincluding seeing a list of items  with accompanying images and filtering products \\nby both name and category. Component OrdersPage is place that us er may \\nmanage their paid orders. Component ProductInfo, as its name implies, displays \\ninformation about the product, such as its name, category, and price. Component \\nLoginPage is the initial page of the website, user need a valid account with \\nusername and password. Component RegisterPage allows users register a new \\naccount with correct email, password and confirmation password.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29'}, page_content='23 \\n \\n \\nFigure 11. The screenshot list of components \\n4.3.2 Redux \\nRedux is the one of most popular libraries to manage state of the application, with \\nfour pillars such Predictable, Centralized, Debuggable and Flexible. Predictable \\nmeans that Redux enables you to create apps that act consistently, are simple to \\ntest, and operate in a variety of settings. Centralized means t hat Adding \\nsophisticated features like undo/redo, state persistence, and other capabilities by \\ncentralizing your application\\'s data and logic is a great way to improve \\nperformance. Debuggable means that Redux DevTools, it is simple to see when, \\nwhen, why, and how your app\\'s state has changed in real time. Changes may be \\nlogged and debugged using \"time travel debugging\" in Redux\\'s design. Error \\nreports can also be sent to the server. Flexible means that Redux can be used \\nwith almost any UI layer and has a thriving add-on ecosystem.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30'}, page_content='24 \\n \\nIn the application, we use redux to manage the state of products when loading, \\nadding and removing stuffs. \\n \\nFigure 12. The screenshot of redux file \\n4.4 Firebase configuration \\nIn the application, we use Firebase cloud services such as Authentication, \\nFirestore Database, Hosting. To set up the services in the application, we follow \\nthe following steps: \\n• Creating a Firebase project \\n• Installing the SDK and initialize Firebase \\n• Accessing the Firebase in the application'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31'}, page_content='25 \\n \\n \\nFigure 13. The configuration file of Firebase in the application \\n4.5 Interface layout \\nThe screenshots below are of the application. The figure 14 is the homepage that \\nshows a list of goods and a search box for quickly finding things by name. \\nAdditionally, customers may filter items by category on the Homepage. By \\nchoosing certain goods, as seen in Figure 15, the detail page displays some \\nproduct information and a button to add the item to the basket. Once an order \\nhas been placed, the user may manage it via the Order page in figure 16. \\nFollowing the addition of products to the cart, figure 17 displays the list of chosen \\nitems and allows the user to check out the order. The Orders tab displays a list \\nof all previously placed orders in chronological order, allowing the user to quickly \\nexamine the total cost of the transaction as well as the pro duct details. For \\nauthentication, there are two pages. In figure 18, there is a login page, and there \\nis a register page. Register page is the only one where users are supposed to fill'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32'}, page_content='26 \\n \\nin things like their username and password. The user can switch between  the \\nRegister and Login pages on their own. \\n \\nFigure 14. The Home page \\n \\nFigure 15. The product detail page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33'}, page_content='27 \\n \\n \\nFigure 16. The Orders page \\n \\nFigure 17. The Cart page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34'}, page_content='28 \\n \\n \\nFigure 18. The Login page \\n \\nFigure 19. The Register Page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35'}, page_content=\"29 \\n \\n5 Deploying the website \\nIn order to get started, install the Firebase tools that will allow you to deploy \\nyour app, and then log into Firebase from your terminal to get things going. If \\nyou haven't already done so, you'll be prompted to enter your email address as \\nwell as your password to continue. It is also necessary to examine the root \\ndirectory of your React application. Your React application will now need the \\nuse of Firebase to be setup.  \\nAs we go, we configure through a series of questions and configuration options, \\nsuch as Select Hosting: Configure and deploy Firebase Hosting sites with a \\ngraphical user interface. Select From the drop-down menu, choose your \\nFirebase Project to work on. After that, you'll need to inform Firebase where to \\nlook for your assets after they've been deployed, which you can do here. Create \\nReact App creates a build folder that includes all the production assets by \\ndefault, and this is what you should expect. Assuming the default configuration \\nhas not been changed, this should be set to the build state. Finally, you have \\nthe option of having Firebase rebuild your existing build/index.html file with a \\nnew one created by itself. You will not be able to use this functionality until you \\nhave a fully functional version of your application. When presented with this \\nchoice, you should pick N. (No). You should see two new files on your hard disk \\nwhen the starting process is complete. firebase.json and firebaserc are two \\ndifferent types of firebase. Because these files include information about your \\nFirebase hosting setup, you should save and commit them to your git \\nrepository. Everything seems to be in working order, so you can continue with \\ndeploying your application.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36'}, page_content=\"30 \\n \\n6 Testing \\nIt's also critical to do testing to guarantee that the application is functioning and \\ncompatible with the vast majority of browsers available. Moreover, the web text \\nis reviewed for grammatical errors in addition to being error-free. \\nEverything was checked, goods were seen with images, the list was sorted both \\nascending and descending, the products were filtered by name as well as \\ncategory.  Preferred things were put to the cart and deleted from the basket as \\nappropriate. The system functions just as it should in every respect. There were \\nno problems with the examination. \\nThe responsiveness test was conducted on different environments such as \\nAndroid, iOS, and Windows operating systems. There was a favorable conclusion \\nsince not all of the mobile phones that were tested were able to connect to the \\nwebsite without experiencing any difficulties. It seems like all of the visuals, \\nbuttons, and oth er parts of the design are being shown appropriately on the \\nscreen.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, page_content=\"31 \\n \\n7 Discussion \\nIt is now simpler than ever to develop a website utilizing a number of platforms \\nand computer languages, such as PHP, Vue, and JavaScript, as a result of recent \\ntechnological advancements . The majority of website builders, mostly \\nconventional ones such as WordPress, provide pre -designed website templates \\nfor users who do not have technical skills. These templat es, on the other hand, \\nare often incapable of being customized as readily as developers would want. \\nOne of the most significant disadvantages is the high initial and ongoing costs of \\ncreating and maintaining the process.  \\nAs a result, we chose React for th is project. JavaScript offers a number of \\nbenefits, including its speed, simplicity, server load, and interoperability with other \\nprogramming languages, to name a few. JavaScript is more efficient since it does \\nnot rely on external resources or a backend server. Because JavaScript is totally \\ninterwoven with HTML and CSS, it can be introduced to any web page in a matter \\nof seconds. \\nKnown for its effectiveness in the development of online applications, React is \\none of the most popular JavaScript lightweight frameworks for web development. \\nAs a form of cache, React retains a copy of its data structures in memory. Each \\ntime the code is changed, React automatically identifies and propagates the \\nchanges to the browser, ensuring that the user experience is uninterr upted. The \\nperformance of the website will be improved as a result of this unique feature. \\nThere are several advantages to using Firebase cloud services. The technique \\naids in the discovery of information by customers and increases the website's \\npresence o n the internet. Additionally, the rapid and secure hosting solution \\nfacilitates the deployment of web applications. Encryption without the need for \\nsetup Firebase Hosting use SSL encryption to protect the information. Domains \\nare protected from external attacks and data breaches thanks to the use of SSL \\ntechnology. Firebase Hosting makes use of SSD and content delivery networks \\n(CDNs) to deliver material quickly and improve application performance.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38'}, page_content=\"32 \\n \\nPayment and bills will not be the focus of this project due  to lack of time and \\nresources. However, additional development of these characteristics is required \\nin order to boost the website's economic efficiency. Customer service \\nrepresentatives should notify customers of any changes to the store's promotion \\nand discount policy through email on a regular basis. Another tip is to increase \\nyour website's search engine visibility and network marketing efforts, both of \\nwhich are becoming more important when running an online business. \\nCustomers who are not fluent in English should be able to place orders in a range \\nof languages, including but not limited to specific languages such as German and  \\nFinnish.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39'}, page_content='33 \\n \\n8 Conclusion \\nThe goal of this thesis is to utilize React to create an e-commerce website that is \\nfast, flexible, and easy to use . The website should also be able to manage the \\nbackend effectively just like, for instance,  Firebase’s cloud services.  This thesis \\nwas a success since the website functions as expected. \\nThere are several features on this website that allow visitors to do a variety of \\ntasks such as seeing a list of goods with thumbnails and full -sized photographs, \\narranging the products by size and adding or deleting things from the shopping \\ncart. \\nThis project was made using React in the front-end and Firebase plays a role as \\nthe backend site. The project was put online after it had completed the \\ndevelopment phase. \\nDifferent platforms were used to co nduct the responsiveness, functionality, and \\nbrowser compatibility tests. We were pleased with the end outcome since our \\nwebsite suited all our needs. \\nThere are a lot of useful features in this web app, yet it is still basic a s well as  \\nappealing enough for an online business. At a modest cost, it gives customers a \\nway to grow their company.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40'}, page_content='1 \\n \\n \\n \\nReferences  \\n1. What is Diffing Algorithm. Online. 02 February 2022.  <What is Diffing \\nAlgorithm>. Accessed 3 April 2022. \\n2. Horton Adam. Mastering React. Book. 23 February 2016. Accessed 3 April \\n2022. \\n3. Occhino, Tom. 2015. React Native: Bringing modern web techniques to \\nmobile. Online. 26 March 2015. < React Native: Bringing modern web \\ntechniques to mobile>. Accessed 5 April 2022.  \\n4. Sutch, Caelin. 2020. “What is Firebase?”. Online. 17 May 2020. <What is \\nFirebase>. Accessed 5 April 2022. \\n5. 2021. Firebase - Introduction. Online. 15 July 2021. < Firebase- \\nIntroduction>. Accessed 5 April 2022. \\n6. Ogbonna, Chizoba. 2022. Device to Device Push Notification using Cloud \\nFunctions for Firebase. Online. 27 March 2017. <Device to Device Push \\nNotification using Cloud Functions for Firebase>. Accessed 12 April 2022. \\n7. Firebase hosting. Online. <Firebase hosting>. Accessed 12 April 2022.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41'}, page_content='Appendix 1 \\n1 (1) \\n \\nFigures \\nFigure 1. How the DOM working in React 4 \\nFigure 2. The overview of Firebase Cloud service. 9 \\nFigure 3. Firebase Authentication service 10 \\nFigure 4. Cloud Function for Firebase 12 \\nFigure 5. The example of Firebase real-time database. 13 \\nFigure 6. The flow of using Firebase Hosting. 15 \\nFigure 7. An example of Cloud Storage for Firebase service 16 \\nFigure 8. The structure of project 20 \\nFigure 9. The screenshot of HTML file 21 \\nFigure 10. The screenshots of product.css file 21 \\nFigure 11. The screenshot list of components 23 \\nFigure 12. The screenshot of redux file 24 \\nFigure 13. The configuration file of Firebase in the application 25 \\nFigure 14. The Home page 26 \\nFigure 15. The product detail page 26 \\nFigure 16. The Orders page 27 \\nFigure 17. The Cart page 27 \\nFigure 18. The Login page 28 \\nFigure 19. The Register Page 28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42'}, page_content='Appendix 2 \\n1 (1)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Academic Journal of Nawroz University (AJNU), Vol.10, No.3, 2022  \\nThis is an open access article distributed under the Cre ative Commons Attribution License  \\nCopyright ©2017. e-ISSN: 2520- 789X \\nhttps://doi.org/10.25007/ajnu.v11n3a14 80 \\n410 \\n \\nFirebase Efficiency in CSV Data Exchange Through PHP-\\nBased Websites \\nRidwan Boya Marqas1, Saman M. Almufti 2, Renas Rajab Asaad 3 \\n1,2,3Department of Computer Science, Nawroz University, Kurdistan Region  –  Iraq \\nABSTRACT \\nA database is a collection of data that may be organized into two distinct forms: relational databases, which use \\nSQL (structure query language), and distributed databases, which use No SQL (non-relational SQL). Both types of \\ndatabases are referred to as databases. The amount of information around the globe is increasing at an exponential \\nrate, leading to the development of data as the world gets  more technologically advanced and computerized. In \\nthe vast majority of studies, the NoSQL database is simpl y referred to by its acronym, \"NoSQL.\" In this \\ninvestigation, data is transferred from a website written in PHP t o a CSV file by way of a NoSQL database known \\nas Firebase. To import and export the experimental data, it was necessary to use two different CSV files, each of \\nwhich included 1,000 and 4,997 records, respectively. Exchangin g CSV files with a website that was built using \\nPHP was the method that this study used to test the performance of  Firebase.  \\nKEYWORDS: Database, Firebase, Website, PHP, NoSQL.  \\n1. INTRODUCTION\\nThe term \"database\" is mostly used to refer to a \\ncollection of data that has been structured, saved, and \\naccessible via a computer system (Ullman 2007). \\nToday\\'s digital systems are susceptible to data with \\nhuge dimensions because of the extent, diversity, and \\ncomplexity of the data universe. [citation needed] In \\naddition, enormous amounts of data must be saved, \\nmanaged, or analyzed using cloud and social media \\nstorage and management systems (Ramzan and Bajwa \\n2018). NoSQL databases are used for enterprise and \\nopen-source database management. These databases \\nstore vast amounts of data across a network of \\ncomputers. The name \"Not Just SQL\" was given to the \\nNoSQL database management system to dispel the \\nwidespread belief that SQL cannot be contained \\n(Ganesh Chandra 2015). \\nNoSQL makes it possible to scale horizontally; hence, \\nits many implementations are always kept on distinct \\nservers. The column-based NoSQL database stores \\ndata in a single huge table, as opposed to the relational \\ndatabase, which stores data in numerous tables. This is \\ndone to make autoscaling easier (Lee and Zheng 2015). \\nFirebase is a NoSQL database that is hosted on the \\ncloud and runs in that environment. Even when the \\nlocal cache network is off, it is still feasible to \\nsynchronize all the devices that are linked together. \\nThis database is driven by events, and in comparison, \\nto ordinary SQL databases, it operates considerably \\ndifferently (Moroney and Moroney 2017a). PHP was \\ninitially developed by Rasmus Lerdorf, a Canadian of \\nDanish ancestry, and the most recent version, PHP 7, \\nincorporates all the most recent enhancements (Jentsch \\n1997). \\nThis article looks at how well Firebase performs while \\nimporting and exporting CSV files with websites that \\nare based on the PHP programming language. \\n2. MATERIALS AND METHODS \\nA. Programming Hypertext Protocol (PHP)  \\nWeb pages are often built using a dynamic \\nprogramming language called PHP, which may be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content=\"Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n411 \\n \\n \\nfound on a broad variety of websites and web \\napplications that are constantly updated (Mon et al. \\n2019). To access and save data on a variety of \\nplatforms, the PHP code may be modified to operate \\nwith other web scripting languages. Comma Separated \\nValue (CSV) files may be read and written using \\nfunctions in PHP (Mon et al. 2019). \\nLarge volumes of data are frequently sent across \\ndisconnected applications using CSV files. Commas \\nare commonly used to separate spreadsheet fields in \\nCSV files, whereas system end- of-line characters are \\ncommonly used to separate CSV records in Microsoft \\nExcel or Text pad (Hapeez, Yassin, and Hamzah 2010). \\n2.1 Non-relational database (NoSQL) \\nNon-relational databases, such as NoSQL, do not \\nhave a defined structure and may be accessed using a \\nsimple query language. Data may be distributed and \\nreproduced in a less controlled environment using \\nNoSQL's huge database. For the foreseeable future, \\nthis database will maintain the ability to store data \\nindependently of any other databases. NoSQL \\ndatabases are organized hierarchically. It's also \\ncapable of handling large volumes of data at high \\nspeeds. The structure of a NoSQL database is \\nhorizontal. NoSQL databases like Cassandra and \\nHBase are among the few that may be used in a NoSQL \\napplication. \\nIn general, there are numerous Relational databases \\nand non-Relational databases. SQL databases include \\nMySQL, Oracle, Microsoft SQL Server, PostgreSQL, \\nand DB2, whereas NoSQL databases include Redis, \\nAmazon DynamoDB, Cassandra, Scylla, HBase, \\nFirebase, MongoDB, Couchbase, Neo4j, Datastax \\nEnterprise Graph, Elasticsearch, Splunk, and Solr. See \\nFig. 1. \\n \\nFig. 1.  Classes of SQL and NoSQL \\nThis table compares several aspects of SQL and \\nNoSQL Databases, such as their scalability and pricing \\nas well as their capacity to hold a large quantity of data \\nand how quickly they can be accessed. \\n3. PROPOSED METHOD \\nThis section describes the proposed technique for \\nreading CSV files and importing data into the firebase \\ndatabase, as well as exporting data from the firebase \\nservice. The execution time was calculated to assess the \\neffectiveness of importing and exporting PHP-based \\nwebsite files using firebase Information. \\n3.1 PHP & CSV \\nPHP needs the following functions for reading and \\nwriting data to and from CSV files: \\nI. FOpen operation \\nThis function is used to open a CSV file \\nTABLE 1 \\nComparison of SQL and NoSQL criteria  \\nCriteria RELATIONAL \\nDATABASE \\n(SQL) \\nNoSQL \\ndiversity Open and \\nclosed source \\nOpen source  \\nScalability Upgrade a \\nsingle server \\nwith devices \\nUsing standard servers scale \\nhorizontally \\nprice Costly data \\naccess \\nsolution \\ninexpensive than open source \\nand cheaper update \\nAmount of \\ndata \\nLimited Vast data hold \\naccessibility Affect by \\nsingle fail \\nUnaffected by one point of failure \\nthat’s distributed \\nExecution \\ntime \\nLong process \\ntime \\nShort process time \\nComplexity Complex \\ndata creation \\nLess complex data creation \\nimplementati\\non \\nSmall \\nimprovement \\noccurs \\nEach stage own improvement \\noccurs \\nuniformity Structured  Unstructured  \\nSecurity Strong \\nSecurity \\nSecurity not included is related to \\nother parts \\n   \\n   \\n.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n412 \\n \\n \\nusing open (file, mode), where the file \\nrepresents the target file and mode represents \\nthe access required for reading or writing in \\nthe CSV file. \\nII. Fgetcsv function \\nThis function is used to read data from a \\nCSV file by iteratively parsing an open file \\nline by line and checking for data fields. \\nfgetcsv (file, length, separator), where the file \\nis the target file, length is the maximum \\nlength of a CSV row, and separator is a \\ncomma to separate CSV data. \\nIII.  Fputcsv function \\nThis function is used to write data to a \\nCSV file fputcsv (file, fields), where the file is \\nthe destination file and fields are the data \\narray. \\nIV.  Fclose Method \\nThis function is used to open the CSV file fclose (file), \\nwhere the file represents the file to be opened. \\nThe features of Firebase are broken down into several \\ncategories and described in Table 2. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n3.2 PHP & FIREBASE \\nFirebase is a cloud-based, real-time database \\ndeveloped for mobile and web apps, although it \\ncannot be used directly with PHP to create websites. \\nFirebase stores data as JSON; hence, the Composer \\ndependency manager, which provides a standard \\nstructure for managing PHP and library dependencies, \\nis required for linking PHP with Firebase. \\n \\nI. Getreference function  \\nThis function is used to retrieve and push \\nvalues from the source database. \\ngetreference (DB) \\nwhere DB is the Database being \\nreferenced. \\nII. Push functionality \\nThis method inserts a list of data records \\ninto the Firebase database. When adding a \\nnode to a list of data, the Firebase database \\ngenerates a new unique key. \\nPush(Data) \\nData represents the list of data to be \\nuploaded into Database. \\nIII. The getValue method  \\nretrieves data from the Firebase database. \\ngetValue() \\nTABLE 2 \\nCharacteristics of firebase  \\nCriteria FIREBASE \\nType Cloud hosted \\nRealtime \\nDatabase \\nModel \\nDocument \\nStore \\nDevelop by Google \\ncompany \\nRelease 2012 \\nCommercial Yes \\nCloud based Yes \\nServer OS Hosted \\nScheme of \\nData \\nFree schema \\nXML support No \\nSQL No \\nAccess \\nmethods and \\nAPI’s \\nAndroid \\nSupport \\nprogram \\nlanguage \\niOS \\nServer-side \\nscripts \\nJavaScript \\nAPI \\nTriggers RESTful \\nHTTP API \\nConsistency Java \\nForeign keys JavaScript \\nIntegrity Objective-C \\nAuthenticatio\\nn \\nFunctionality \\nare limited \\nwith rules'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=\"Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n413 \\n \\n \\n \\nFig. 2.  process of import & export. \\nFigure 2 depicts the data import and export processes \\nfor both the firebase databases. \\n4. EXPERIMENT & RESULT \\nThis section demonstrates the results of an \\nexperiment including the import and export of data \\nfrom the firebase databases, with download speeds of \\n34.65 Mbps and upload speeds of 36.06 Mbps. \\nIn the experiment, two hospital-related CSV files \\nwere employed; the first CSV file included 1000 \\nentries, while the second CSV file contained 4997 \\nrecords; both files had 11 columns, as seen in (figure 3). \\n \\nFig. 3.  columns name of CSV file \\n(Figure 4) depicts the full system connection \\nbeginning with the connection from the computer to \\nthe 000webhost server. The server hosts the website \\nthat connects to the distinct databases listed below: \\n• Firebase: in Google server Using NoSQL. \\n \\nFig. 4.  Connection System \\nA connection system in (figure 4) demonstrates the \\nconnection of the system used that import and export \\nprocess occurred through it which the connection \\ndistributes between firebase and CSV file by using \\n000webhost hosting server. \\nIn the Firebase online database, a table named hospital \\nwas created according to the CSV structure depicted in \\n(figure 5). \\n \\nFig. 5.  demonstrates the hospital table database \\nstructure online Firebase \\nTable 3 depicts the execution times for importing and \\nexporting data in the firebase database. \\nTABLE 3 \\nduration of Firebase's execution \\nprocess Record No. Firebase \\nimport 1000 138.649094 seconds \\n4997 735.564 seconds \\nexport 1000 0.495048046 seconds \\n4997 0.774774 seconds \\nTable 3 displays the import and export execution \\ntimes for 1000 and 4997 CSV entries in firebase.  \\n(Figure 6) depicts the import process execution time \\nfor 1000 and 4,997 CSV records into Firebase, while \\n(Figure 7) depicts the export process execution time for \\n1000 and 4,997 CSV records in the Firebase database.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n414 \\n \\n \\n \\nFig. 6.  demonstrates the import process for Firebase \\n \\nFig. 7.  demonstrates the export process for Firebase \\nThe import and export process show the percentage \\nfor 1000 and 4997 CSV record which for import one \\nuse 18% for 1000 and 82% for 4997, while the export \\nprocess show 16% for 1000 and 84% for 4997 CSV \\nrecord. \\n5. CONCLUSION AND RECOMMENDATIONS \\nThis study presents a time performance assessment \\nfor the process of data transferring (import and export) \\nbetween CSV files and Firebase online databases with \\na PHP-based website. \\nThe experimental results for two CSV files containing \\n1000 and 4997 records indicate that the data exporting \\nprocess for Firebase in a PHP-based website is faster \\nthan the importing process. This is because, during the \\nimport process, the connection between the website \\nand Firebase is indirect and requires more time to \\nreach the database on an external server, whereas the \\nexport process directly reaches the database and \\nretrieves the data. \\nREFERENCES \\nBinani, Sneha, Ajinkya Gutti, and Shivam Upadhyay. \\n2016. “SQL vs. NoSQL vs. NewSQL- A \\nComparative Study.” 6(1):43–46. \\nDB-Engines.com. 2020. “Firebase Realtime Database \\nvs. Sqlite Comparison.” Retrieved June 2, 2020 \\n(https://db-\\nengines.com/en/system/Firebase+Realtime+Dat\\nabase%3BSQLite). \\nGanesh Chandra, Deka. 2015. “BASE Analysis of \\nNoSQL Database.” Future Generation Computer \\nSystems 52:13–21. \\nHapeez, Mohammad Shukri, Mohd Ihsan Mohd \\nYassin, and Mustafar Kamal Hamzah. 2010. \\n“Storage of Online HPLC Data for \\nPharmaceutical Research Applications Using \\nXML Database.” Proceedings of the 2010 5th IEEE \\nConference on Industrial Electronics and \\nApplications, ICIEA 2010 1605–9. \\nJentsch, Birgit. 1997.                             Gender Politics \\nand Post-Communism: Reflections from Eastern \\nEurope and the Former Soviet Union. Vol. 3. \\nLee, Chao Hsien and Yu Lin Zheng. 2015. “Automatic \\nSQL-to-NoSQL Schema Transformation over the \\nMySQL and HBase Databases.” 2015 IEEE \\nInternational Conference on Consumer \\nElectronics - Taiwan, ICCE-TW 2015 426–27. \\nMon, Cho Thet, Su Su Hlaing, Mie Mie Tin, Mie Mie \\nKhin, Tin Moh Moh Lwin, and Khin Mar Myo. \\n2019. “Code Readability Metric for PHP.” 2019 \\nIEEE 8th Global Conference on Consumer \\nElectronics, GCCE 2019 929–30. \\nMoroney, Laurence and Laurence Moroney. 2017a. \\n“An Introduction to Firebase.” The Definitive \\nGuide to Firebase 1–24. \\nMoroney, Laurence and Laurence Moroney. 2017b. \\n“The Firebase Realtime Database.” The Definitive \\nGuide to Firebase 51–71. \\nRamzan, Shabana and Imran Sarwar Bajwa. 2018. “SS \\nSymmetry An Intelligent Approach for Handling \\nComplexity by Migrating from Conventional \\nDatabases to Big Data.” \\nStephens, Jon and Chad Russell. 2004. Beginning \\nMySQL Database Design and Optimization. \\nUllman, J. D. 2007. “A First Course in Database \\nSystems.” Pearson Education India. Wang, K. C. \\n2018. Systems Programming in Unix/Linux.  \\n \\n \\n18%\\n82%\\nImport\\n1000\\n4997\\n16%\\n84%\\nexport\\n1000\\n4997'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/362539877\\nFIREBASE -OVERVIEW AND USAGE\\nArticle\\xa0\\xa0in\\xa0\\xa0Journal of Engineering and Technology Management · August 2022\\nCITATIONS\\n35\\nREADS\\n15,713\\n5 authors, including:\\nAnil Trimbakrao Gaikwad\\nBharati Vidyapeeth Deemed to be University\\n19 PUBLICATIONS\\xa0\\xa0\\xa039 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Anil Trimbakrao Gaikwad on 07 August 2022.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1178] \\nFIREBASE - OVERVIEW AND USAGE \\nPankaj Chougale*1, Vaibhav Yadav*2, Dr. Anil Gaikwad*3 \\n*1,2Student, Bharati Vidyapeeth Deemed To Be University, Pune. Institute Of Management,  \\nKolhapur, India. \\n*3Guide, Bharati Vidyapeeth Deemed To Be University, Pune. Institute Of Management,  \\nKolhapur, India. \\nABSTRACT \\nThe web application has relied heavily on large amounts of websites and random data such as videos, photos, \\naudio, text, files and other inappropriate content. Icon It is difficult for the Relational Database Management \\nSystem (RDBMS) to manage random data . Firebase is a new technology for managing large amounts random \\ndata. Very fast compared to RDBMS. This research paper focuses on us age of Firebase for Android and aims to \\nfamiliarize itself with its  functions, related concepts names, benefits and limitat ions. The paper also tries to \\nshow some nakedness Firebase features for building an Android app.  \\nKeywords: Cloud Based-Firebase, Cloud Based-Android. \\nI. INTRODUCTION \\nGoogle Firebase  may be a  Google-backed application development software  that allows developers to develop \\nIOS, Android and Web apps. Firebase provides tools for tracking analytics, reporting and fixing app crashes, \\ncreating marketing and products experiment. \\nII. METHODOLOGY \\nFirebase offers variety of services, including: \\nAnalytics – Google Cloud Analytics for Firebase offers free, unlimited reporting’s on as many as 500  or more \\nseparate events. Statistics present content on user behavior in IOS and Android app lications to enable best \\ndecision-making about improving an application performance and marketing. \\n \\n(source: https://i.ytimg.com/vi/8iZpH7O6zXo/maxresdefault.jpg) \\nAuthentication: Firebase authentication makes it easy for developers to protect authentication systems and \\nimproves login and boarding experience for users. This the feature offers an all -in-one identity solution, \\nsupporting email accounts and password, phone auth, moreov er like Google login, Facebook, GitHub, Twitter \\nand more.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1179] \\n \\n(source: https://firebase.google.com/images/products/auth/auth -3.png) \\nCloud Messaging: Firebase Cloud Messaging (FCM) may be various messaging tool allowing companies to trust \\nand receive messages on IOS, Android and therefore the web at no cost. \\n \\n(source: https://i.ytimg.com/vi/sioEY4tWmLI/maxresdefault.jpg) \\nRealtime Database: The Firebase Realtime Database may be a cloud -based NoSQL site that allows data to be \\nstored and synchronized between users in real time. Information is synced to all clients in real time and is \\nalways available when the app is offline. \\n \\n(source: https://media.geeksforgeeks.org/wp-content/uploads/20190421141241/gfg53.png)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1180] \\nCrashlytics: Firebase C rashlytics may be a real-time crash journalist assisting engineers  track, prioritize and \\nfix problems that diminish the quality of their applications. With Crashlytics, engineers spend less time \\nplanning and resolving crashes and long construction features for his or her apps. \\n \\n(source: https://firebase.google.com/docs/crashlytics) \\nPerformance: The Cloud-Firebase Performance Monitoring feature provides developers with an \\nunderstanding of app lication features of their IOS and Android apps to help them decide where to reach and \\nwhen the performance of their applications is improved. \\n \\n(source: https://miro.medium.com/max/1400/1*yevJMct9erdfRGH8YgJ1_g.png ) \\nTest Lab:  Firebase Test Lab  may be cloud-based application testing infrastructure. By one performa nce, \\ndevelopers can test their I OS or Android apps across all device devices as well device con figuration. They will \\nsee results, including videos, screenshots and logs, inside the Firebase console. \\n \\n(source:https://d1qmdf3vop2l07.cloudfront.net/unwavering-snake.cloudvent.net/hash-\\nstore/15f9baced6535d0042fc1d098c262d2c.png)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1181] \\nUse Cases: \\nConditions for using Firebase include: \\nCreate on boarding flows:  Developers can provide users with a fast, intuitive login process Firebase \\nVerification. allow users to sign in to their apps via Google, Twitter, Facebook or GitHub accounts in but five \\nminutes. Engineers can track each step their ride flow to enhance use r experience. Additionally, engineers can \\nuse it Google Analytics Firebase entry events for each step flow ride, create panels to see where users are \\nleaving and using the remote control adjusting to make changes to their operating systems to determine how  \\nthose changes affect them conversion. \\nCustomize a “welcome back” screen:  Engineers can use it to make it your own to stop everything user very \\neasy experience customizable first user -supported screen favorites, usage history, location or language. \\nEngineers can explain by audience, by section, in user behavior and display targeted content to all audiences.  \\nGradually unleash new features: Developers can launch new features . \\nProgressively roll out new features:  Developers can introduce new low -risk features by first testing those \\nfeatures in some users to determine how they operate and how users respond. Then, If the engineers are \\nsatisfied. \\nIn June 2018, mobile security company Ap pthority reported thousands of I OS and Android devices mobile \\napplications expo sing more than 113 GB of information about 2,271 unprepared Firebase database.  As of \\nJanuary 2018, Authority researchers are scanning Android applications that use Firebase systems to store users \\ndata or content  and updated communication patterns for applications made on Cloud-Firebase domains.  \\nAfter scanning mor e than 2.7 million Android and I OS apps, researchers identified 28,502 mobile apps (1,275 \\nIOS and 27,227 Android) that connected and stored data within t he Firebase background. Of these, 3,046 apps \\n(600 IOS and pear, 446 Android) store data within 2,271 Firebase poorly designed websites that give anyone \\nthe ability to view their content. \\nThe database revealed more than 100 million user data records, includ ing the actual LinkedIn, Firebase, \\nFacebook and company data store tokens; 25 million GPS location records; more than 4 million health \\ninformation records are protected, such as medical information and chat messages; 2.6 million User IDs and \\npasswords; and  50,000 financial records, including payments, banking and transactions for Bit coin . \\nPricing: \\nFirebase offers a free 1 GB real -time data storage system and two paid subscriptions plans: Flame Plan ($ 25 \\nper month with 2.5 GB of storage) and Blaze Plan (pa y-as-you-go, $ 5per GB per storage). All programs include \\nA / B testing, statistics, application identification, authentication (without phone auth), cloud messaging, \\nCrashlytics, dynamic links, invitations, functionality monitoring, forecasting and remote  preparation. The main \\ndifferences between programs include real -time shared storage  website, number of download tasks, Cloud \\nFirestone bandwidth, and more. Internet of Things Engineers share many common usage requirements across a \\nwide range of IoT  applications. Data collection, delivery of content with low latency, and to prevent \\ncommunication between devices and back -up services there are only three  of these common requirements.  \\nWhile meeting common needs  are often challenging from time to time, I oT development platforms  like Google \\nFirebase provide services and functionality that allows developers to satisfy many of those requirements. \\nBenefits of Google Firebase: \\nThe Firebase platform as a service includes a NoSQL document data store. Application s store data as JSON \\nobjects and interact with the database  employing a  JavaScript API. Mobile  or Android  developers have  the \\noption of using Android or I OS APIs, too.  The information store is intended  to scale with application demand, \\nso there\\'s no have to add servers, partition data or perform other database administration that comes  together \\nwith maintaining your own database  rear. The REST  full API includes queries  operations but not SQL \\noperations. The query API is ready-made to figure with channels of information. for instance, the \"on\" operation \\nlistens for changes in location and invokes a callback Function if a selected event occurs. There also are queries \\nand operators with SQL -equivalents, like order-By-Key, order-By-Value and order -By-Priority. The limit, limit -\\nTo-First and limit-To-Last query operations can restrict  the amount of JSON documents returned by the query.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1182] \\nIn addition to storing dynamic contents within the firebase store, developers can store static content  that it will \\nbe required within the app in a Firebase managed service. Way of providing static content storage is the same \\nas installing a website; Firebase will do verify domain ownership, provide an SSL certificate and use it \\nthroughout Firebase content delivery network. Finally, name service records must be updated to become a map  \\nthe name of your hosted site. \\nAnother advantage of Firebase is that it provides support for offline operation. Website  functions are locally \\nrecorded and synchronized with the Firebase site when a network connection is established.  Google Firebase \\nincludes mobile data protection controls at rest. Data is transmitted using SSL / TLS 2048 -bit encryption, and \\nwithin the site , users are authenticated and may be restricted to certain functions using a set of security rules. \\nFirebase authentication uses an existing login server or client side code. Firebase supports currently usernames \\n/ password login such as social login resou rces like Google, Twitter and Facebook. Custom code can be used if \\nyou would like to receive your tokens.  In the case of users or in the case of  IOT, devices - already authorized, \\nsecurity rules govern the activities they will perform and the data they wi ll access. Safety rules support reading \\n/ writing performance, integrated can expect, moreover as a guaranteed job. this can usually determine the \\ncorrect format of the information element and its data type. New apps automatically provide full read / write  \\naccess to the site; engineers should ensure that they review safety rules to limit the performance and breadth of \\ninformation the tool can use. Attackers can gain multiple or higher rights to compromise the integrity or \\nconfidentiality of information within a data store. \\nGoogle Cloud Firebase provides six price categories with its different services: - Free, Spark, Candle, Bonfire, \\nBlaze and Inferno. Sans Free tier, price from $ 5.00 / month on Spark to $ 1,499.00 / month on Inferno. Three to \\nsix stages al l include unlimited real -time internet connection support with users, additionally as 1 TB data \\ntransfer. The biggest difference between categories is reflected in the final volume of real -time data and \\ntransfer grants. Spark, for example, offers 1 GB of s torage and 10 GB of transmission while Inferno, the leading \\ntier, offers 300 GB of storage space and allows up to 1.5 TB of transfer data. the three most advanced categories \\nalso offer non-public backup options to customers. \\nDrawbacks  Of  Google-Firebase: \\nFirebase is very useful for IOT apps  that use data that can use the Javascript API to access Content and security \\nservices. However, developers should look to other tools to increase the back -end performance of their I OT \\nsystem. Basic server side processing is provided, but more advanced a nalysis may require uploading IO T data \\nto a different location, such as Hadoop or Spark. \\nAnother challenge is that the Firebase query function is limited. If you need more advanced query functions, \\nconsider sending data to an Elastic  Search server or collection for more search options. In addition, if visibility \\nand alerts are important when monitoring information distribution, consider tools such as Kibana, Elastic  \\nsearch data display. \\nIII. RESULTS AND DISCUSSION \\nOut-of-container notification response. you can use Firebase Notifications, a notification server with an Internet \\nconsole that allows anyone to send alerts to specific target audiences based on Firebase Analytics data.  \\nRealtime Database in Firestore uses NoSQL , a cloud -based website to store and sync real -time data between \\nusers and devices. In addition, Firebase Cloud Storage stores and stores user -generated content such as images,  \\naudio, and video more smoothly. Using Cloud Tasks, you can measure your applica tions at any time in your life \\ncycle without measuring the servers in use.  Hosting and verification is easily managed efficiently in Firebase \\nwith simple methods and effective tools. The ready -to-use Firebase ML Kit APIs carefully add advanced features \\nto machine learning in  the operating system. The best thing is that your limited knowledge and experience in \\nmachine learning would not be an obstacle to adding a little ingenuity to an advanced app.  \\nIV. CONCLUSION \\nThis paper highlights research on the Firebase API provided by Google and its unique features. This paper helps  \\nyou to learn how to utilize the Firebase in the Android app based on the Clients  requirements. This also helps \\nto make android apps faster and more efficient as there is no PHP required as a third party language to \\ncommunicate with the website. Provides a secure communication channel and website directly from JAVA.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1183] \\nLearning materials are based on data provided online and refer to the examples provided. Google has been \\nconstantly updating Firebase, Ad Sense  is a beta section of Firebase. Not only for Android but also for cross -\\nplatform connectivity .The function can be expanded by adding new feature s and testing new possibilities to \\nAndroid apps. \\nV. REFERENCES \\n[1] https://firebase.google.com/ \\n[2] https://www.javatpoint.com/firebase \\n[3] https://www.tutorialspoint.com/firebase/index.html \\n[4] https://youtu.be/b1bGrWrx5Mo \\nView publication stats')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cf52f",
   "metadata": {},
   "source": [
    "#### Adding two new fields in metadata of each loaded document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "179d1ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A\\nBenchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nRishi Kesav Mohan\\nrkmohan2@illinois.edu\\nRisheek Rakshit Sukumar Kanmani\\nrrs7@illinois.edu\\nKrishna Anandan Ganesan\\nkag8@illinois.edu\\nNisha Ramasubramanian\\nnr50@illinois.edu\\nABSTRACT\\nIn the era of big data, conventional RDBMS models have become\\nimpractical for handling colossal workloads. Consequently, NoSQL\\ndatabases have emerged as the preferred storage solutions for exe-\\ncuting processing-intensive Online Analytical Processing (OLAP)\\ntasks. Within the realm of NoSQL databases, various classifications\\nexist based on their data storage mechanisms, making it challenging\\nto select the most suitable one for a given OLAP workload. While\\neach NoSQL database boasts distinct advantages, inherent scalabil-\\nity, adaptability to diverse data formats, and high data availability\\nare universally recognized benefits crucial for managing OLAP\\nworkloads effectively. Existing research predominantly evaluates\\nindividual databases within custom data pipeline setups, lacking\\na standardized approach for comparative analysis across different\\ndatabases to identify the optimal data pipeline for OLAP workloads.\\nIn this paper, we present our experimental insights into how vari-\\nous NoSQL databases handle OLAP workloads within a standard-\\nized data processing pipeline. Our experimental pipeline comprises\\nApache Spark for large-scale transformations, data cleansing, and\\nschema normalization, diverse NoSQL databases as data stores, and\\na Business Intelligence tool for data analysis and visualization.\\nThe wide-ranging classifications of NoSQL databases include\\ndocument-oriented, key-value stores, columnar databases, and graph\\ndatabases. For our experiments, we selected MongoDB, Redis, Apache\\nKudu, and ArangoDB, each representing a distinct family of NoSQL\\ndatabases. Leveraging a standardized pipeline, we assessed the per-\\nformance of these databases using Koalabench, a popular NoSQL\\nbenchmarking dataset collection. Each pipeline we setup has a\\nunique NoSQL database and the performance of each pipeline has\\nbeen evaluated for data loading time and query execution time. We\\nhave also compared the performance of a standard SQL solution\\n(PostgreSQL) against the different NoSQL alternatives. Koalabench\\ngenerated datasets of varying sizes in our desired data model and\\nwe conducted a series of experiments using data belonging to two\\ndifferent data models - flat and snow. The insights gleaned from\\nthese experiments will facilitate the establishment of an optimal\\nOLAP data pipeline, pairing the ideal NoSQL database as the data\\nwarehousing solution.\\n1 INTRODUCTION\\nModern day data engineering has seen numerous enhancements.\\nEvery last ounce of data is being considered as a feature. With data\\nhaving immense significance in the modern world, the way in which\\nwe handle and process data should also evolve with requirements\\nand time. To develop a holistic data engineering pipeline, there are\\nthree crucial components:\\n• Data Loading and Processing\\n• Data Store\\n• Data Analysis\\n1.1 Data Loading and Processing\\nData loading and processing consists of the seamless ingestion,\\ntransformation, and preparation of data sourced from various ac-\\ncessible outlets. Given the enormous volume of data to be managed,\\nit requires underlying systems with enhanced processing capabili-\\nties to facilitate efficient handling. Apache’s Hadoop Distributed\\nFile System (HDFS) [1] emerges as one of the prominent solutions\\nbuilt for large-scale data storage. HDFS offers high-throughput ac-\\ncess to application data and is ideal for batch processing operations.\\nWith HDFS taking care of large storage, we would have to imple-\\nment a solution to handle data processing.Data processing is a key\\naspect of any data pipeline and it governs the process of converting\\nthe raw data into a database compatible format before loading it\\ninto the database. Apache Spark [2] stands out as a prime choice\\nfor swift and efficient data processing. Its seamless integration with\\nHDFS enables the seamless transfer of data from HDFS to Spark\\nfor processing or streaming to subsequent components within the\\npipeline. Employing these two tools in tandem establishes an opti-\\nmal platform for transforming data, rendering it ready for import\\ninto the designated data store.\\n1.2 Data Store\\nOnce the voluminous data has been stored and processed, we would\\nrequire a solution for storing the processed data. Ideally, this compo-\\nnent would be a database, which helps in storing data in a structured\\nformat and it can be accessed using query languages specific to\\nthe chosen database. The traditional relational database manage-\\nment system (RDBMS) [17] can store structured data effectively\\nbased on a predefined schema. RDBMS also requires complex query\\noperations such as joins in the event of requiring data from more\\nthan one table and these complex query operations are generally\\nresource intensive.\\nWith the rise of semi-structured data, there is scope for exploring\\nthe possibility of OLAP workloads. NoSQL databases have been\\nadept at leveraging semi-structured data formats such as CSV and\\nJSON which has allowed for the incorporation of a flexible data\\nschema, ease of scalability and support for heterogenous datatypes.\\nNoSQL databases promote flexibility in schema and their data stor-\\nage mechanism allows for easy access of different data features\\nwithout the need for executing complex joins to derive insights.\\nDespite the benefits of NoSQL over SQL, prevailing architectures\\nhave predominantly relied on SQL databases as their primary data\\narXiv:2405.17731v1  [cs.DB]  28 May 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\nstore. There exists a significant disparity in evaluating the perfor-\\nmance of NoSQL databases within an OLAP-intensive pipeline.\\n2 RELATED WORK\\nThe literature survey undertaken centered on evaluating prior re-\\nsearch within the realms of data processing and databases. These\\ndomains were identified as primary areas of interest for assessing\\nthe overarching structure of our envisioned pipeline.\\n2.1 HDFS and Spark for OLAP\\nOn the the topic of data processing, we looked far and wide to\\nunderstand the depth of research that had been performed in order\\nto gauge the progress from the perspective of data engineering\\npipelines. One of the key ideas, HDFS [22] has paved the way for\\nstoring large datasets and also provides an invaluable framework\\nfor carrying out analysis and transformation operations on the\\nlarge data using the MapReduce [10] algorithm. HDFS has paved\\nthe way for several database and data streaming solutions to be\\nbuilt on top it. One such data streaming service that has become\\nwidely popular is Apache Spark [ 25]. Apache Spark helps in the\\nstreaming and processing of voluminous data from HDFS. HDFS\\nand Spark in tandem can handle the load of data processing on any\\nindustry-level data engineering pipeline.\\nResearch done based on leveraging HDFS and Spark for OLAP\\nworkloads is rather insignificant in the context of NoSQL databases.\\nThe standalone research of OLAP pipelines was limited in the pre-\\nvious years, however certain hybrid ideas were proposed. One such\\nidea called Hybrid Transaction/Analytical Processing (HTAP) [18]\\nproposed for a unified storage for both OLTP and OLAP workloads.\\nWhile this may seem feasible considering the flexibility of use that\\nsystems like HDFS and Spark provide in connecting with different\\ndatabase solutions, this idea does not account for the data rigidity\\nthat exists in transactional and analytical processing. RDBMS solu-\\ntions require a rigid schema and lack support for heterogeneous\\ndatatypes whereas NoSQL addresses both concerns. Hence, having\\na common store and unifying OLTP and OLAP is a little far-fetched\\nfor now.\\nWildfire by IBM [4] leveraged a HTAP like solution using HDFS\\nand Spark to perform data analysis and was one of the first solutions\\nto implement a data processing component in their pipeline. This\\nwork had stressed on the importance of having distributed data\\nstore and distributed data processing to make OLAP more effective.\\nHowever, Wildfire was based on Spark SQL which is more similar to\\nRDBMS. Stream processing [21] became a popular research interest\\npost HDFS and Spark as many ideas revolved around bringing\\ntogether Spark and OLAP through the metaphorical ’cube’. This\\nprocess involved transforming raw data using Spark into an OLAP\\nfriendly format via a process called cubification. The resultant OLAP\\ncube, contained the features extracted from the provided data, in the\\nform of a cube on top of which data analysis could be performed to\\ngather insights. When distributed processing is employed, the use\\nof OLAP cubes becomes restricted to gathering insights from the\\nprocessed data and does not aid in determining a scalable strategy\\nfor processed data ingestion into the database.\\n2.2 Benchmarking for NoSQL-OLAP\\nThe second broad topic we researched on is the availability of\\nliterature for NoSQL databases being used in OLAP/OLTP work-\\nloads. This exploration was an effort to understand whether there\\nwas a possibility to use NoSQL databases for OLAP. Based on the\\nNoSQL OLAP literature we identified, we also looked for previous\\nbenchmarking experiments performed for NoSQL OLAP works.\\nOne of the premier works in the NoSQL-OLAP domain involved\\nthe creation of OLAP cubes using distributed processing without\\nthe need of RDBMS [11]. Using the MC-Cube operator developed\\nin this paper, the idea is to perform transformations on the data\\nstored in columnar NoSQL databases and the data features get bun-\\ndled into \"cubes\" which can then be used for analytical insights.\\nThis idea came up short because of the use of just a columnar\\nNoSQL database as other NoSQL databases were not tested in their\\nexperiments. Another idea on the contrary [8] used a document-\\noriented NoSQL database as the data store solution but employed\\na series of techniques such as shingling, chunck, minhashing, and\\nlocality-sensitive hashing MapReduce. These techniques do serve\\nthe purpose of extracting features from the available data but fall\\nshort when the data scales.\\nWhilst the initial literature stresses more on the usage of NoSQL\\ndatabases for OLAP workloads, there has been some research done\\non benchmarking NoSQL databases purely from a query processing\\nperspective for OLAP. The current evaluations of NoSQL databases\\nhave primarily focused on simplistic metrics, such as the loading\\ntime when used as a data warehouse. However, these comparisons\\nfail to capture the nuanced performance differences within a pro-\\ncessing pipeline. Extensive research [6, 7] has delved into NoSQL\\nbenchmarks, yet these studies often prove inadequate for com-\\nprehensive NoSQL database comparisons in a processing context.\\nThey emphasize the stark disparities between new benchmarks and\\ntraditional Relational benchmarks, illustrating the superiority of\\nthe former. For instance, Chevalier et al. ’s [6] introduction of Koal-\\naBench — a versatile benchmarking tool—enables the analysis of\\nboth relational and NoSQL databases, thereby addressing this gap.\\nNumerous studies explore different aspects of NoSQL databases or\\ncompare various NoSQL solutions. For instance, [13] highlights the\\nsignificance of Graph Databases in data analysis, while [7] provides\\na comparative analysis of three popular NoSQL databases in the\\ncontext of Big Data and Cloud Computing. Moreover, [12] details\\nthe evaluation of Columnar stores, particularly focusing on HBase\\nand utilizing the Star schema as a metric. Additionally, [13] con-\\nducts a comparative study between HBase and MongoDB using a\\nstar schema setup, further enriching our understanding of these\\ndatabase systems.\\n3 THE GENERIC PIPELINE\\nThe generic pipeline that we envisioned consists of three broad\\ncomponents - data processing, the data store and the data analytics\\nsolution. By feeding data to the data processor, we transform the\\ndata into a usable format for the data store. All the pipelines we have\\ncreated uses a different type of NoSQL database as its data store.\\nFinally, all the pipelines culminate with a data analytics solution\\nthat reads off the processed data stored in the database to create'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nFigure 1: Schema of the dataset based off the Snow Logical\\nData Model generated by Koalabench\\nanalytical insights. The pipeline can be further explained in the\\nfollowing sections:\\n3.1 Data Processing\\nThe data processor section consists of two major components -\\nApache HDFS and Apache Spark. All the data files required for the\\npipeline are first loaded on to HDFS. Thanks to the distributed\\nstorage and distributed processing capabilities of using HDFS and\\nSpark in tandem, we were able to load data from HDFS and utilize\\nPySpark to load the transformed data into the database.\\nThe raw data we load into HDFS is provided to us by Koalabench\\n[14]. Koalabench can be broadly defined as a decision support bench-\\nmark for Big Data requirements. It is derived from the TPC-H\\nbenchmark, the reference benchmark in research and industry for\\ndecision support systems. It has been adapted to support Big Data\\ntechnologies such as NoSQL databases and HDFS. It can generate\\ndata in different data formats and in different data models. There\\nare four primary data logical models supported by Koalabench, out\\nof which the two data models that we have used are the following :\\n3.1.1 Snowflake Data Model . This data model is very close to\\nthe one used in the TPC- H benchmark with small modifications.In\\na classic snowflake data model, there are three major components.\\nFact Tables are the prominent tables based on which most of the data\\nquerying is performed - ideally they are the centre of attraction in\\na snowflake data model. Component Tables are supporting tables to\\nthe fact table in the sense that they contain most of the required data\\nattributes to know more about the fact table records. Relationship\\ntables are used when multiple component tables need to be linked\\nto create a compound component table that aids in providing more\\ninformation about the fact table records.\\nAs per Figure 1, the element in green, Lineitem would be the only\\nfact table for our data model. All tables except PartSupplier will be\\ncomponent tables which aid in providing attributes to Lineitem via\\nsimple relationships. PartSupplier is a relationship table composed\\nof attributes from Part and Supplier component tables.\\n3.1.2 Flat Data Model . This is the simplest data model of all\\nthe supported ones. As per Figure 2, Lineitem carries over as the\\nFigure 2: Schema of the dataset based off the Flat Logical Data\\nModel generated by Koalabench\\npremier entity based on which all the data features are developed\\non. In the flat data model, the significant data parameters from\\ncomponent tables is included within the Lineitem fact table. All\\nrelationship tables removed as the flat data model promotes a sim-\\npler way of storing data without the need for establishing complex\\nrelationships.\\nThe common file format used by all the pipelines for their raw\\ndata is CSV and we have utilized the datasets generated using the\\nflat logical data model for the NoSQL databases and the dataset\\nbased on the Snowflake logical data model has been used in the\\nPostgreSQL pipeline. This helps us evaluate the performance of the\\ndatabases with the way in which they handle scalable data upon\\ndata ingestion.\\nThe data from HDFS is pushed to Spark for performing data\\ntransformations. As our test environment had limited hardware re-\\nsources to play with, we resorted to utilizing thenum_partitions fea-\\nture of Spark. In Apache Spark, num_partitions refers to the param-\\neter used to specify the number of partitions to divide an Resilient\\nDistributed Dataset (RDD) or dataframe into during processing.\\nProperly setting num_partitions can optimize performance by bal-\\nancing data distribution across the available computing resources.\\nIt is essential to choose an appropriate value for num_partitions to\\nensure efficient parallel processing and avoid resource contention\\nin Spark applications.\\n3.2 Data Store\\nOnce the processsed data is ready from Spark, we begin the process\\nof inserting the records into the data store component. For simplic-\\nity, we have taken up different NoSQL databases to act as the data\\nstore for the processed data from Spark. Spark supports interactions\\nwith multiple popular SQL and NoSQL databases which has allowed\\nus to use four different NoSQL databases based on the four broad\\nclassifications of NoSQL databases. They are as follows:\\n3.2.1 MongoDB. MongoDB [15] is a popular NoSQL database\\nknown for its flexible document-oriented data model, which stores\\ndata in BSON (Binary JSON) format. It offers scalability and high per-\\nformance, with features like sharding and replication for handling\\nlarge-scale deployments. MongoDB’s expressive query language\\nand rich ecosystem of tools make it well-suited for a wide range of\\napplications, from web development to analytics.\\n3.2.2 ArangoDB. ArangoDB [24] is a multi-model database sys-\\ntem supporting document, key/value, and graph data models in\\na single database core. It offers a versatile query language, AQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\n(ArangoDB Query Language), and boasts features like distributed\\ngraph processing and geo-spatial indexing. With its flexible data\\nmodel and powerful querying capabilities, ArangoDB is suitable for\\ndiverse use cases including social networks, recommendation en-\\ngines, and real-time analytics. For the purposes of our experiments,\\nwe have leveraged the graph data models of ArangoDB owing to\\nthe Graph databases family of NoSQL solutions.\\n3.2.3 Apache Kudu. Apache Kudu [3] is an open-source columnar\\nstorage engine designed for fast analytics on rapidly changing data.\\nIt combines the performance of traditional columnar databases with\\nthe ease of use of Hadoop. With features like automatic partition-\\ning and fault tolerance, Apache Kudu is ideal for real-time data\\ningestion and interactive analytics applications.\\n3.2.4 Redis. Redis [19] is an in-memory data store known for its\\nhigh performance and versatility in caching, session management,\\nand real-time analytics. It supports various data structures like\\nstrings, hashes, lists, sets, and sorted sets, making it suitable for a\\nwide range of use cases.\\nApart from the mentioned NoSQL databases, we have performed\\none additional experiment using PostgreSQL [23] as the RDBMS\\nsolution for the data store. This experiment has helped us compare\\nand contrast the performance of SQL and NoSQL databases in\\nthe generic pipeline. The processed data has been saved on the\\nrespective databases thanks to the versatility provided by PySpark\\nin developing seamless interaction modules between Spark and the\\ndatabases.\\nOnce the data has been loaded, we have executed a subset of\\nthe queries belonging to the TPC-H benchmark. As Koalabench\\nderives its existence from TPC-H, it has allowed us to reuse the\\nsame template of the documented benchmark queries. However,\\nbased on how the data is stored in the different databases, we have\\ncome up with equivalent queries for each of the NoSQL databases\\nand executed them on the databases respectively.\\n4 ARCHITECTURE\\nFor the evaluation of each proposed data pipeline as part of our\\nbenchmarking process, we plan to execute datasets of varying scale\\nfactors generated from the Koalabench suite. These datasets will\\nbe run through each pipeline within our standardized local testing\\nenvironment, which consists of a system running MacOS Sonoma\\n14.4.1, powered by an Apple M2 chip with 8GB of RAM and 128GB\\nof storage. To support distributed data processing capabilities for\\nHadoop and Spark, we intend to create a dockerized environment\\nwithin the local system. This environment will comprise a master-\\nworker setup facilitating the execution of distributed tasks.\\nThe architecture shown in Figure 3 consists of one master node\\nconcurrently acting as a worker, along with three dedicated worker\\nnodes. The master node maintains bidirectional communication\\nroutes with each worker node, enabling seamless coordination\\nfor distributed processing across HDFS and Spark clusters. The\\ndockerized environment is built on Debian Linux v11 across all\\nnodes. To facilitate distributed data processing capabilities, each\\nnode is equipped with Hadoop-3.3.6 and Spark-3.4.1.\\nWe have incorporated the latest docker images for all the databases\\nunder evaluation, including Postgres-alpine-3.19, MongoDB-7.0.8,\\nFigure 3: Dockerized experimental setup consisting of one\\nmaster and three worker nodes\\nArangoDB-3.12.0, Redis-7.2.4, and Apache Kudu-1.17. These database\\nimages have been instantiated within the same docker network\\nas the experiment nodes, enabling efficient measurement of time-\\nbased metrics without introducing potential biases stemming from\\nnetwork latency. The data analysis component of our pipeline will\\nleverage a Business Intelligence (BI) tool connected to the respective\\ndatabase docker image for accessing and analyzing stored data. The\\ntentative BI tools identified for this purpose areMetabase v0.49.3 and\\nTableau 2024.1. The BI container will reside within the same docker\\nnetwork as the experimental setup to ensure seamless integration\\nand communication.\\n5 EXPERIMENTS\\nThe following section elaborates on the dataset that we have used\\nto evaluate the performance of the various pipelines and documents\\nthe findings from the individual experiments we have carried out\\nin each data pipeline we developed.\\n5.1 Dataset\\nTo evaluate our pipelines across varying data scales, we have used\\ndifferent datasets generated from Koalabench using the flat data\\nmodel based on varying scale factor(sf). The primary objective of\\nutilizing different sf values is to maximize the dataset size, enabling\\ncomprehensive testing of our pipelines’ performance and scalability\\nacross a broad range of data volumes. We have conducted exper-\\niments on all the pipelines using sf1, sf2, sf3, sf4 and sf5 datasets\\nwith the data format set as CSV for all datasets. The minimum\\nsize of the collection belongs to the sf1 dataset at 2.38 GB and the\\nmaximum size of the collections belongs to sf5 which scales up to\\napproximately 11GB. For the PostgreSQL pipeline, we have lever-\\naged the snow data model to generate datasets ranging from sf1 to\\nsf5 in the CSV format. The least size is with sf1 at 1.4GB and the\\nmaximum size is with sf5 at approximately 6GB.\\n5.2 Data Loading Time\\nTable 1 portrays the load times for different datasets across the five\\ndifferent databases.\\nApache Kudu has shown the smallest load times across all five\\ndatasets. One of the primary reasons for this observation is the\\ncolumnar-storage mechanism that Apache Kudu employs coupled\\nwith its superior in-memory management that flushes out records'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nTable 1: Data Loading times for all databases across the five\\ndatasets (time in seconds)\\nDatabases SF1 SF2 SF3 SF4 SF5\\nPostgreSQL 37s 375s 857s 1089s 1481s\\nMongoDB 90s 1250s 1701s 2275s 2810s\\nArangoDB 295s 2249s 3964s 12169s 15162s\\nRedis 1495s 3245s 5023s 7748s 10289s\\nApache Kudu 42s 95s 146s 192s 240s\\nas and when the threshold is reached. This allows Kudu to han-\\ndle scaling data efficiently and keep the overall loading time to\\nagreeable levels.\\nMongoDB seems to have progressively higher data ingestion\\ntimes. MongoDB’s document-oriented storage model and lack of\\nnative sharding capabilities may lead to increased data ingestion\\ntimes due to increased indexing and write operations overhead.\\nAdditionally, as the dataset size grows, MongoDB’s reliance on\\nmemory-mapped files for storage may result in slower write per-\\nformance and increased disk I/O operations.\\nRedis displays the poorest performance on all 5 datasets. The\\nvery high data loading times observed in Redis can be attributed\\nprimarily to the schema we used for the processed data. In order\\nto keep the schema uniform across all NoSQL databases, we opted\\nfor having the linenumber_id as the only key and all the remaining\\ndata features were aggregated into a single list and attributed to\\nthe corresponding linenumber_id. In general, we ended up creating\\na record that had a single key and 40 values. Additionally, redis\\nbeing a single threaded solution meant that for every insertion\\noperation had two call operations to make. With data scaling, this\\nwould increase exponentially and in the end be double of what an\\nideal PostgreSQL implementation would execute.\\nArangoDB was implemented using its document model and its\\ndata insertion times are exorbitant owing to the document model\\nthat was implemented. ArangoDB is definitely not an ideal solution\\nshould the data scale as for sf5, the data insertion time is almost\\n50x what it was for sf1.\\nPostgreSQL shows a gradual increase in data insertion times from\\nsf1 all the way upto sf5. It is able to handle scaling data much more\\nefficiently has compared to some of its NoSQL counterparts.\\nBased on this experiment, it is clear that with a NoSQL database\\nas your data store, it is preferrable to choose a columnar storage\\nsolution in case the pipeline has to be robust enough to handle\\nscaling data.\\n5.3 Query Processing Time\\nAs Koalabench is based off the TPC-H benchmark, we selected five\\nout of the seventeen benchmark queries that TPC-H has to offer. The\\nTPC-H benchmark is meant for running benchmark experiments\\non RDBMS solutions. Hence, the queries can be directly applied\\nto the PostgreSQL data pipeline as the schema is same as the one\\nproposed by TPC-H. For all NoSQL data pipelines that leverage\\nthe flat data model, we prepared queries in the native querying\\nlanguage of the NoSQL database present in the pipeline.\\nThe five queries we selected were different from one another\\nbased on the significant operation that was being performed by the\\nFigure 4: Query Execution time (in seconds) for PostgreSQL\\nacross the five datasets\\nquery. For context, Query 1 was aggregation intensive in which\\nalmost eight out of the ten values that were being fetched were\\nbased off aggregation. Query 2 had a good balance of aggregation\\nand join operations. Query 3 had an aggregation along with a sub-\\nquery based filter. Query 4 had five join operations and Query 5\\nwas the simplest query off them all with just a single aggregation\\noperation.\\n5.3.1 PostgreSQL. Figure 4 shows a graph depicting the query\\nexecution times for the PostgreSQL data pipeline. Across the five\\ndatasets and for the five queries, we see a steady increase by a factor\\nof 2x for almost all queries except Queries 3 and 4. The reason for\\nthe observation of exponential rise in query execution times for\\nthose two queries alone is because of the inherently costly join\\noperations of RDBMS solutions which tend to compare every row\\nof the two tables. For filtering and aggregation queries, the increase\\nin query execution time is 0.33x for every 2x scale in data which is\\npretty commendable.\\n5.3.2 MongoDB. Figure 5 shows a graph displaying the query\\nexecution times for the MongoDB pipeline for the five selected\\nbenchmark queries across the five datasets. MongoDB uses the\\ntranslated version of the benchmark queries and the major change\\nbetween the original and translated versions are that all join related\\noperations become filter related operations. This is because of the\\nnon-availability of support for joins in NoSQL databases.\\nMongoDB handles most of the queries easily with scaling data\\nwith a maximum increase in execution times by about 0.8x for every\\n2x increase in data. This can be attributed to MongoDB’s inbuilt\\nsharding capabilities, that promote horizontal scaling approach that\\nallows it to handle larger data sets and provide high throughput\\noperations by distributing data across multiple shards. One aspect\\nwhere MongoDB struggles is with intense aggregation queries\\nsuch as Query 1. With data scaling, MongoDB is forced to look\\nup multiple document objects and perform compute operations on\\nthem which are CPU intensive and tend to consume more time\\nthan rest of the query operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\nFigure 5: Query Execution time (in seconds) for MongoDB\\nacross the five datasets\\nFigure 6: Query Execution time (in seconds) for ArangoDB\\nacross the five datasets\\n5.3.3 ArangoDB. Figure 6 shows a graph displaying the query\\nexecution times for the ArangoDB pipeline for the five selected\\nbenchmark queries across the five datasets. ArangoDB uses the\\nArango Query Language (AQL) version of the benchmark queries.\\nArangoDB sees a 1.5x scale in query execution times for every\\n2x scale in data size. Whilst ArangoDB does not selectively out-\\nperform other data pipelines in certain scenarios, it does show a\\nsteady increase in query execution time with scaling data. This can\\nbe attributed to the dynamic query optimizer that selects the per-\\nfect query execution plan depending on the query being executed.\\nArangoDB employs efficient memory management techniques to\\nminimize disk I/O and maximize query performance. It utilizes\\nmemory caches for frequently accessed data and implements buffer\\nmanagement strategies to optimize disk access.\\nFigure 7: Query Execution time (in seconds) for Redis across\\nthe five datasets\\nFigure 8: Query Execution time (in seconds) for Apache Kudu\\nacross the five datasets\\n5.3.4 Redis. Figure 7 portrays a graph containing information on\\nthe query execution times of the five benchmark queries across the\\nfive datasets on Redis. Irrespective of the size of data, Redis takes\\nlonger than all the other databases to execute the queries. For sf1,\\nthe smallest dataset, Redis displays a best case of 3x increase in\\nterms of query execution times. Redis suffers primarily from the\\ndata schema we had selected for the flat data model. By assigning\\nall features to a single key, linenumber_id, a single row of record\\nbecomes huge for redis to retrieve from memory. By default, redis\\nis an in-memory intensive data store and hence, holding such a big\\nrecord in memory is bound to make the query execution slow and\\nCPU intensive.\\n5.3.5 Apache Kudu. Figure 8 shows the recorded query execu-\\ntion times for the five benchmark queries across the five datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nBased on these readings, it is clear that Apache Kudu performs\\nthe best amongst all the selected databases in terms of query exe-\\ncution. Kudu stores data in a columnar format and this is highly\\nefficient for analytical workloads because it allows queries to read\\nonly the columns needed for the query, minimizing I/O operations\\nand improving query performance. This can be visibly seen from\\nthe very minimal increase in query execution times across datasets\\nfor queries 2 to 5. Even when data scales at 2x, query execution\\ntimes have tended to stay flat and do not see a proportional or\\nexponential increase like the rest of the databases. The reason why\\nquery 2 sees an exponential increase with scaling data is related\\nto a potentially incorrect time measurement on behalf of Apache\\nKudu. When fetching data records based on given filter conditions,\\nKudu tends to write the retrieved records onto the impala shell.\\nThe write operation time is also included in the overall execution\\ntime of the query and thus it seems beefed up with scaling data\\ncompared to the rest of the queires.\\n6 CONCLUSION\\nThrough this paper, we envisioned our idea of building the ideal data\\nengineering pipeline, that would be take care of data processing and\\ndata store. We experimented with datasets of varying scale from the\\nKoalabench dataset where data was generated in two different data\\nmodels. Our experiments measured the query execution times for\\n5 benchmark queries and recorded the data ingestion time for all\\ndatabases. The effect of the data schema was evident in the varied\\ndata ingestion times. Certain databases did not handle the scaling\\nlevels of data and ended up seeing exponential increase in the query\\nexecution times of complex queries. A definitive finding from our\\nexperiments is the prominence of Columnar data storage options\\nin future OLAP research. Columnar storages tend to load data 2x\\nfaster than other SQL and NoSQL options and the same goes for\\nquery execution as well.\\nThese experiments also highlight the immense scaling and dis-\\ntributed processing capabilities that modern-day NoSQL solutions\\nhave and how these can be put to use to solve complex data analyt-\\nical problems. We believe this idea would be a pioneer towards the\\nadoption of more NoSQL-based data store solutions for evaluating\\nOLAP workloads. As part of future work, we do recommend the in-\\ntegration of a data analysis tool to commensurate the data pipeline.\\nResearch can be extended across various columnar storage options\\nto identify the ideal columnar storage option to evaluate OLAP\\nworkloads.\\nREFERENCES\\n[1] Apache. 2006. Hadoop Distributed File System. (2006). https://hadoop.apache.\\norg/\\n[2] Apache. 2014. Apache Spark. (2014). https://spark.apache.org/\\n[3] Apache. 2022. Apache Kudu. (2022). https://kudu.apache.org\\n[4] Ronald Barber, et.al. 2017. Evolving Databases for New-Gen Big Data Applica-\\ntions.\\n[5] Zane Bicevska and Ivo Oditis. 2017. Towards NoSQL-based Data Warehouse\\nSolutions. Procedia Computer Science 104, 104–111. https://doi.org/10.1016/j.\\nprocs.2017.01.080 ICTE 2016, Riga Technical University, Latvia.\\n[6] Max Chevalier, Mohammed El Malki, Arlind Kopliku, Olivier Teste, and Ronan\\nTournier. 2015. Benchmark for OLAP on NoSQL technologies comparing NoSQL\\nmultidimensional data warehousing solutions. In 2015 IEEE 9th International\\nConference on Research Challenges in Information Science (RCIS) . 480–485. https:\\n//doi.org/10.1109/RCIS.2015.7128909\\n[7] Max Chevalier, Mohammed El malki, Arlind Kopliku, Olivier Teste, and Ronan\\nTournier. 2015. Implementing Multidimensional Data Warehouses into NoSQL.\\nICEIS 2015 - 17th International Conference on Enterprise Information Systems,\\nProceedings 1. https://doi.org/10.5220/0005379801720183\\n[8] Farnaz Davardoost, Amin Babazadeh Sangar, and Kambiz Majidzadeh. 2020.\\nExtracting OLAP Cubes From Document-Oriented NoSQL Database Based on\\nParallel Similarity Algorithms. Canadian Journal of Electrical and Computer\\nEngineering 43, 2 (2020), 111–118.\\n[9] Lucas de Carvalho Scabora, Jaqueline Joice Brito, Ricardo Rodrigues Ciferri,\\nand Cristina Dutra de Aguiar Ciferri. 2016. Physical Data Warehouse Design\\non NoSQL Databases - OLAP Query Processing over HBase. In International\\nConference on Enterprise Information Systems . https://api.semanticscholar.org/\\nCorpusID:2636732\\n[10] Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified Data Process-\\ning on Large Clusters.\\n[11] Khaled Dehdouh. 2016. Building OLAP Cubes from Columnar NoSQL Data\\nWarehouses. In Model and Data Engineering , Ladjel Bellatreche, Óscar Pastor,\\nJesús M. Almendros Jiménez, and Yamine Aït-Ameur (Eds.).\\n[12] Khaled Dehdouh, Omar Boussaid, and Fadila Bentayeb. 2014. Columnar NoSQL\\nStar Schema Benchmark. In Model and Data Engineering , Yamine Ait Ameur,\\nLadjel Bellatreche, and George A. Papadopoulos (Eds.). Cham.\\n[13] David Dominguez-Sal, Norbert Martinez-Bazan, Victor Muntes-Mulero, Pere\\nBaleta, and Josep Lluis Larriba-Pey. 2010. A discussion on the design of graph\\ndatabase benchmarks. In Technology Conference on Performance Evaluation and\\nBenchmarking. Springer, 25–40.\\n[14] Mohammed El Malki, et al. 2022. Benchmarking Big Data OLAP NoSQL\\nDatabases. In Ubiquitous Networking, 2018, pp. 82–94 .\\n[15] Kevin P. Ryan Eliot Horowitz, Dwight Merriman. 2009. MongoDB. (2009).\\nhttps://www.mongodb.com/\\n[16] Abdelhak Khalil and Mustapha Belaissaoui. 2023. An Approach for Implementing\\nOnline Analytical Processing Systems under Column-Family Databases. IAENG\\nInternational Journal of Applied Mathematics 53, 1 (2023).\\n[17] Astrahan M., et. al. [n.d.]. System R: Relational Approach to Database Manage-\\nment.\\n[18] Fatma Özcan, Yuanyuan Tian, and Pinar Tözün. 2017. Hybrid Transac-\\ntional/Analytical Processing: A Survey. In Proceedings of the 2017 ACM Interna-\\ntional Conference on Management of Data (SIGMOD ’17). Association for Com-\\nputing Machinery, 1771–1775. https://doi.org/10.1145/3035918.3054784\\n[19] Salvatore Sanfilippo. 2009. Redis. (2009). https://redis.io/\\n[20] Nadia Ben Seghier and Okba Kazar. 2021. Performance benchmarking and\\ncomparison of NoSQL databases: Redis vs mongodb vs Cassandra using YCSB\\ntool. In 2021 International Conference on Recent Advances in Mathematics and\\nInformatics (ICRAMI). IEEE, 1–6.\\n[21] Salman Ahmed Shaikh and Hiroyuki Kitagawa. 2020. StreamingCube: Seamless\\nIntegration of Stream Processing and OLAP Analysis. IEEE Access 8 (2020),\\n104632–104649. https://doi.org/10.1109/ACCESS.2020.2999572\\n[22] Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. 2010.\\nThe Hadoop Distributed File System. In 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) . 1–10. https://doi.org/10.1109/MSST.\\n2010.5496972\\n[23] Michael Stonebreaker. 1996. PostgreSQL. (1996). https://www.postgresql.org/\\n[24] Claudius Weinberger and Frank Celle. 2015. ArangoDB. (2015). https://arangodb.\\ncom/\\n[25] Matei Zaharia, et al. 2016. Apache Spark. Communications of the ACM, vol. 59,\\nno. 11, 28 Oct. 2016, pp. 56–65 .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 14 \\n \\nInfluence of CAP Theorem on Big Data Analysis  \\nDr Anand Kumar Pandey [2], Rashmi Pandey [2] \\n[1] Computer Science and Application, ITM University \\n[2] Computer Science, ITM Group of Institute - Gwalior \\n \\nABSTRACT \\nIn the current modern world different computing society has developed their innovative solutions to meet the difficult \\nchallenges to handle linear expansion in large data collection by different sources. In this paper, we took a research oriented \\nview to achieve more significant ideas in the field of big data world and data science engineering. The CAP Theorem is a \\ncommonly cited unfeasibility outcome in distributed computing systems, particularly with NoSQL distributed databases. Cap \\ntheorem is very much influenced by cloud providers in the reference of their usability, the latency limit and system requirement. \\nThe Cap theorem is also very much influenced by distributer database also. To discover a substitute for CAP with a latency-\\ncentric point of view we have to observe how operation latency is exaggerated by network latency at dissimilar levels of \\nconsistency. \\nKeywords:-  CAP, Big Data, Analysis, Distributed System, NoSQL. \\n \\nI.     INTRODUCTION \\n   The CAP theorem is also known as Brewer’s Theorem, \\nbecause it was introduced by MIT Professor Eric A. Brewer \\nduring 2000 with the concept of distributed computing. Our \\nmain purpose in this paper is to consider the influence of CAP \\nTheorem in the broader perspective of big data analysis and \\ndistributed computing theory. \\nData analysis is usually applicable on current distributed big \\ndata centres to accomplish high performance and accessibility. \\nMost of the data science services try to preserve their services \\nstability in all situations. In modern world big data analysis \\nand distributed database system is bound to have partitions in \\na real-world system due to network failure or some other \\nreason. To describe the practical implementation of CAP \\ntheorem we can choose any real big data computing \\nenvironment for data analysis such as MongoDB, Cassandra \\nand with NoSQL database [2] . CAP theorem describes tha t \\nbefore choosing any Database including distributed database, \\naccording to your requirement we have to choose only \\nappropriate properties out of three. CAP theorem allows us to \\nfind out how we want to operate our distributed database \\nsystems when some other database servers decline to \\ncommunicate with each other due to some imperfection in the \\nsystem. During data analysis operations we try to retain the \\noriginality of actual data received from big data pool and \\nfollows all the suitable rules and regulations. Different types \\nof database segments, operators and users are activated during \\nthe task of appropriate data analysis. Therefore it is very \\nimportant to know about that which segment of dataset is \\nconsistent and suitable to apply some partition tolerance \\nrelated operations. Services availability and database partition \\ntolerance are inter-dependent to each other. It seems \\nmotivating to investigate which levels of regularity and \\nreliability are strong enough to be straight implicit by the CAP \\nconstraints. \\n \\nII.     THE CONCEPTUAL ASPECTS OF CAP \\nTHEOREM \\nThe CAP theorem explained the thought that there is a \\nelementary transaction between availability, consistency, and \\npartition tolerance. This transaction, which has become \\nidentified as the CAP Theorem, has been commonly discussed \\never since.  CAP theorem supports non-relational database \\n(NoSQl) are best for distributed network applications and big \\ndata analysis [1]. \\n   With description of CAP theorem Professor Brewer \\nillustrates that it not possible that a distributed computer \\nsystem can support the 3 following properties at a time: \\n \\nFig. 1  CAP Theorem and Distributed Database \\nmanagement \\n    \\n     CAP Theorem is a considered that a distributed database \\norganization can only consist of 2 of the 3 properties : \\nConsistency, Availability and Partition Tolerance as shown in \\nFigure 1. Some of the well known attention about the CAP \\nTheorem which derived from the fact is as follows: \\n \\n \\nRESEARCH ARTICLE                                                OPEN ACCESS'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 15 \\nA. Safety \\nCAP theorem has standard protection property, because \\nevery client sent correct request and receives correct response \\nfrom data centres. These data centre can be personalised or \\ngeneralized.   \\nB. Liveness \\nThe theorem is well defined but overall, it is quite lively. \\nAvailability is a classic liveness property because, every \\nrequest receives a response. \\nC. Unreliable \\n   There are so many different ways in which a system seems \\nunreliable. Since every request at last receives a response so \\nthat structure can be unreliable. There may be partitions, as is \\ndiscussed in the CAP Theorem. \\nIII. THE ROLE OF CAP THEOREM IN BIG \\nDATA ANALYSIS \\n     During the review of CAP theorem as a consider its role \\nand its influence for Big data analysis to achieve the \\ndistributed solutions. May be the first time solution is not \\nperfect, so it needs to repeat the process of data analysis and \\nagain identify the best appropriate solution. In this case we are \\ntrying to get solutions for such kind of big data distributed \\nsystem it is impossible to assurance all three features of Cap \\ntheorem like Consistency, availability and partition tolerance \\nall at the equivalent time. Here we try to analyses the \\napplicability and relationship of the CAP theorem with Big \\nData and distributed systems [1]. The CAP theorem is also \\nrelate with Hadoop, Big data Analytics, DBMS, network \\ncommunication system and with advanced data structure.  \\n    During data analysis at first we have to partition the data in \\nto appropriate segments. At the time of partitioning the user \\ninteracts with operator and operator interacts with database. \\nLets discuss the relationship of CAP theorem with big data \\ncentres and their nodes associated with wireless area network. \\nAs shown in figure 2 we have two data centres with their \\nindividual single server nodes and interconnected with \\ndedicated network [3]. Here we try to propose a framework of \\ninterconnected big data centres for specifying a huge set of \\ndistributed data consistency model. The relationship of \\ndistributed big data described the correlated aspects of CAP \\ntheorem as per their requirements and their needs in this real \\nworld also. \\n \\n \\n \\n \\nFig. 2  Relationship of database \\n    In this framework the CAP theorem involves as a software \\nservice consist in distributed system which makes wisdom to \\nbelieve those reliability models in this extent. In above \\nrelational database, we have achieved Availability of the both \\ndata centres with respect to their concerned nodes as well \\nas Partition Tolerance in both data centres even if they cannot \\ncorrespond [5]. A communicated network divider is a \\nparticular type of communication defect that divides the \\nnetwork into subsets of nodes such that nodes in one subset \\ncannot communicate with other nodes.\\n \\n \\nIV. FUTURE OF CAP THEOREM \\n   The concept of CAP theorem is best possible futuristic \\napproach to discuss basic trade off for available big data \\nanalytic solutions and distributed systems. In future it will \\neasily scrutinize the inbuilt trade off some insights into that \\nhow system can be considered to gather an application’s needs, \\nin spite of unpredictable networks [6]. With the reference of \\nCAP theorem we must know all theoretical aspects to achieve \\nthese challenges, and some modern techniques for supporting \\nwith the problem in real big data world system.  \\nIn this artificial intelligent and Big data world, the \\nnetworked world has altered significantly in the last two \\ndecades, creating new challenges for system designers, and \\nnew areas in which these same inherent trade-offs can be \\nexplored. We need new theoretical insights to address these \\nchallenge, and new techniques for coping with the problem in \\nreal-world systems. \\n \\nA. Mobile Wireless Network \\n    The CAP Theorem primarily determined on modern \\nwireless network services. Now days, we illustrate that its \\nconsiderable growth proportion of network communication \\ngoing to initiated by advanced mobile devices. A big data \\ndistributed database system is dedicated to comprise partitions \\nin a real-world system due to network failure or some other \\nreason. \\n    Especially, wireless network communication is particularly \\nuntrustworthy. The main problem that the frequency is going \\nto change quickly, so it is not easy to motivated the CAP \\nTheorem for stable partitions. In every wireless networks, \\npartitions are less common. After re-evaluating the CAP \\nTheorem in the framework of wireless networks, we expect to \\nbetter recognize the best appropriate solutions that take place \\nin these types of scenarios [4]. By re-examining the CAP \\nTheorem in the situation of wireless networks, we might hope \\nto enhanced understand the unique trade-offs that occur in \\nthese types of scenarios. \\n \\nB. Scalability \\n    The CAP Theorem describes that in the current scenario of \\na network partition, the administrator has to decide between \\nconsistency and availability. Gradually, we involve that our \\nsystems be designed, scalable not just for today’ s consumers \\nbut also for future growth . Spontaneously, we believe that'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 16 \\nstructure as scalable if it can mature resourcefully, using \\ninnovative resources capably to handle extra load. \\n \\nC. Tolerating Attacks \\nPartition tolerance describes those clusters that must \\ncontinue to work even though any number of communication \\nbreakdowns between nodes in the system. The CAP Theorem \\nfocuses on network partitions: occasionally, a number of \\nservers did not communicate consistently [5]. Progressively, \\nhowever, we felt that more rigorous attacks on networks. \\nTolerating these extra problematic forms of interruption \\nrequires a somewhat different understanding of the \\nfundamental consistency/ availability trade-offs. A rejection \\nof service attack, however, cannot basically be modeled as a \\nnetwork partition. \\n \\nV.    SEGMENTING TASK OF DATA   \\n  ANALYSIS \\nThose systems who are using aspects of CAP theorem some \\nof them many systems do not include a single uniform \\nrequirement. Some aspects of the system require strong \\nconsistency, and some require high availability. In this section \\nof the paper, we describe few of the dimensions along which a \\nsystem might be partitioned. It is not always clear the specific \\nguarantees that such segmentation provides, as it tends to be \\nspecific to the given application and the particular partitioning. \\n \\nA. Data Partitioning \\n     In this big data world different types of data analysis may \\nrequire different levels of consistency and availability. For \\nexample, an on-line shopping cart may be highly available, \\nresponding rapidly to user requests; yet it may be occasionally \\ninconsistent, losing a recent update in anomalous \\ncircumstances. The on-line product information for an e-\\ncommerce site may be somewhat inconsistent: users will \\ntolerate somewhat out- of-date inventory information. The \\ncheck-out/billing/shipping records, however, have to be \\nstrongly consistent. \\n \\nB. Functional Partitioning \\n     Many services can be divided into different subservices \\nwhich have different requirements. For example, an \\napplication might use a service for coarse-grained locks and \\ndistributed coordination. Whatever service or function we \\nneed to be use can be partition as per condition and \\nrequirement. \\n \\nC. Operation Partitioning \\n     Different operations may require different levels of \\nconsistency and availability. Moreover, different types of \\nupdates might provide different levels of consistency. The \\nCAP theorem and its data analysis provide with differing \\ntrade-offs for different types of read and write operations. \\n \\n \\n \\nD. User Partitioning \\n     Network partitions, and unfortunate network performance \\nin general, normally correlate with real geographic distance: \\nusers that are far away are more likely to see poor \\nperformance. Usually, one could imagine that a social \\nnetworking site might try to partition its users, ensuring high \\navailability among groups of friends. \\nVI.    CONCLUSIONS \\n   In this paper we discussed several aspects of the CAP \\ntheorem: the definitions, the conceptual aspects of Cap \\ntheorem, the role of cap theorem in Big data analysis, future \\nof CAP theorem in the literature are fairly paradoxical and \\ncounter- intuitive. The Cap theorem is also very much \\ninfluenced by distributer database also and it is not possible to \\nmake available reliable data on both the nodes and \\naccessibility of complete data. The CAP theorem describes \\nthat proposed distributed database system has to compose a \\ntransaction between Consistency and Availability when a \\nPartition occurs. \\n  \\nREFERENCES \\n[1] Brewer EA (2012) CAP twelve years later: How the \\n“rules” have changed. IEEE Computer 45(2):23–29, DOI \\n10.1109/MC.2012.37. \\n[2] Daniel J Abadi. Consistency tradeoffs in modern \\ndistributed database system design. IEEE Computer \\nMagazine, 45(2):37 – 42, February 2012. \\ndoi:10.1109/MC.2012.33. \\n[3] Dobre D, Viotti P, Vukolic M (2014) Hybris: Robust \\nhybrid cloud storage. In: ACM Symposium on Cloud ´ \\nComputing (SoCC), Seattle, WA, USA, pp 12:1 –12:14, \\nDOI 10.1145/2670979.2670991. \\n[4] Fekete A, Gupta D, Luchangco V, Lynch NA, \\nShvartsman AA (1996) Eventually-serializable data \\nservices. In: 15th ACM Symposium on Principles of \\nDistributed Computing (PODC), Philadelphia, PA, USA, \\npp 300–309. \\n[5] Francesc D. Munoz-Esco, Ruben de Juan-Martin, J. R. \\nGonzalez de, , Jose M. Bernabeu, CAP Theorm: \\nRevision of its Related Consistency Models, Technical \\nReport TR-IUMTI-SIDI-2017/002, Universitat \\nPolitecnica de Valencia, 46022 Valencia (Spain). \\n[6] Martin Kleppmann, A critique of the CAP Theorem, \\narticle published in researchgate on Sept 2015.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='Reﬂection on MongoDB Database Logical and\\nPhysical Modeling\\n1st Pieter Willem Jordaan\\nSchool for Electrical, Electronic and Computer Engineering\\nNorth-West University\\nPotchefstroom, South Africa\\npieterwjordaanpc@gmail.com\\n2nd Johann Erich Wolfgang Holm\\nSchool for Electrical, Electronic and Computer Engineering\\nNorth-West University\\nPotchefstroom, South Africa\\njohann.holm@nwu.ac.za\\nAbstract—Traditional relational database design uses concep-\\ntual, logical, and physical modeling based on Peter Chen’s\\nmethods. UML (Uniﬁed Modeling Language), though being a\\nset of software development tools, is often used to conceptually\\nmodel data and relationships. This paper presents a methodical\\napproach to logically and physically model data in MongoDB\\nby utilizing UML conceptual modeling aids. Application of data\\nmodels greatly impacts performance, scalability and ﬂexibility of\\ndata systems. Furthermore, application life-cycle and expansion\\nimpact long-term decisions made during modeling. This paper\\ntakes into consideration application requirements, access patterns\\nand life-cycle during logical and physical modeling in a cloud\\nenvironment. The ﬂexibility that a NoSQL modeling paradigm\\nembodies makes logical and physical modeling complex, with\\nno hard-and-fast rules. This paper presents logical and physical\\nmodeling concepts required during designing database applica-\\ntions with MongoDB.\\nIndex Terms—database, modeling, UML, MongoDB, cloud\\nI. I NTRODUCTION\\nEntity-relationship modeling methods and tools developed\\nby Peter Chen [1] have become the de facto standard for\\nmodeling and designing databases. Though designed for and\\nused by relational databases, it is being applied to NoSQL\\n(Not Only SQL) databases to similar effect [2].\\nNoSQL databases are, however, very different to traditional\\ndatabases when it comes to modeling, since scalability, con-\\nsistency and performance greatly depend on the underlying\\ndesign, especially considering their ﬂexible schema options\\n[3].\\nCloud-based development deﬁnes application criteria re-\\nquirements for developers. Additional emphasis is placed on\\nthe following requirements speciﬁc to cloud systems [4]:\\n• Ease of use;\\n• Scalability;\\n• Availability;\\n• Security.\\nScalability and availability cannot always be predicted due to\\nchanging environments and requirements - these factors should\\nbe taken into account when designing a database to allow for\\nﬂexibility, which ﬁt well into NoSQL database systems traits\\n[5].\\nMongoDB is a NoSQL database capable of running in the\\ncloud. It provides high availability, horizontal scalability and\\na ﬂexible data structure [6]. MongoDB has full relational\\ncapability, which suits traditional entity-relational modeling\\ntechniques [7]. Recently MongoDB gained ACID (Atomicity\\nConsistency Isolation Durability) support, which allows for\\ntransactions, much like with relational databases [8], [9].\\nFlexible schemas with no hard-and-fast rules such as the\\ntraditional 3NF (third normal form) [10] for designing con-\\nceptual and physical models, make NoSQL database modeling\\ncomplex.\\nThis paper provides a broad, but detailed, overview of\\ndatabase modeling concepts that have a bearing on design\\nchoices for NoSQL databases. We provide speciﬁc guidelines\\non how to employ MongoDB’s unique feature set. As a case\\nin point, we conceptually, logically and physically model a\\nsimplistic use-case, based on Chen’s methods. A discussion\\non the complexity of NoSQL modeling is provided, followed\\nby a conclusion with further research suggestions.\\nII. R ELATED WORK AND CONTRIBUTIONS\\nRelated studies have contributed to NoSQL modeling re-\\nsearch. Shin et al. [2] has provided a framework for mod-\\neling NoSQL databases generically from a conceptual UML\\nmodel based on Chen’s framework. However, the study does\\nnot discuss the physical modeling phase, and has a generic\\ndocument-based logical model.\\nAn algorithmic approach was developed by Abdelhedi et\\nal. [11] to transform conceptual UML class models into\\nphysical models for various NoSQL databases. The logical\\nmodel is used as an intermediate abstraction layer for a subset\\nof features in three NoSQL database types: column-based,\\ndocument-based and graph-based.\\nNoAM (NoSQL Abstract Data Model) presented by Atzeni\\net al. [12] offers a database-independent data model for\\nNoSQL databases, also making use of a UML-based concep-\\ntual data model.\\nDe Lima et al. [3] has published an algorithmic approach to\\nNoSQL logical modeling. Workﬂow measurements are to be\\nspeciﬁed along with the conceptual model when developing\\nlogical models. Physical modeling and its effects are not\\ndemonstrated.\\nThe previously mentioned research publications does not\\nconsider application life-cycle with regards to growth, scala-\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='bility and ﬂexibility. Furthermore, our study does not attempt\\nto generically model multiple NoSQL databases, but rather\\nreﬂects on factors supporting and leveraging MongoDB and\\nits speciﬁc set of features.\\nIII. D ATABASE MODELING\\nA. Uniﬁed Modeling Language\\nUML, originally developed as a set of tools for software\\nengineering, can be used for data modeling. Class diagrams\\nare a superset of ER (Entity-Relationship) diagrams used in\\nChen’s methods [13].\\nTools that this paper will use for modeling include use-case\\ndiagrams and class diagrams. Use-case diagrams, as in Fig.1,\\nare used to model actors performing activities on systems [14].\\nThis diagram is used to identify the data elements used in\\nconceptual modeling.\\nClass diagrams represent data elements, attributes, methods,\\nrelationships, inheritance and cardinalities as shown in Fig.2.\\nUML stereotypes can be used to add additional information\\nduring logical and physical modeling, such as embedding,\\nreferences and other MongoDB speciﬁc features [2].\\nComposition and aggregation annotations in class diagrams\\nfurther give an indication of the nature of relationships, leading\\nto either referencing (entities are standalone) or embedding\\n(entities are not independent) [14].\\nUML was found to provide better support and be more\\ncomprehensive than ER (entity-relationship) diagrams at data\\nmodeling by De Lucia et al. [15]. Since UML could be used\\nto model software classes, documentation could be shared\\nbetween application and data models.\\nB. Conceptual Modeling\\nThe ER design approach starts with a need or problem fol-\\nlowed by a requirements analysis [16]. From the requirements\\nanalysis actors, interactions or activities and some attributes\\nshould be known [10]. From this a use case diagram can be\\ndrafted [17].\\nWhen requirements are satisfactorily determined, the con-\\nceptual model can be designed. This model still does not\\ncontain database speciﬁc annotations, and could be equivalent\\nto a relational database conceptual design. Conceptual designs\\ncontain high-level data entities, attributes, relationships and\\ncardinalities [10], [18]. Even during this phase the conceptual\\ndesign should have foresight and consideration of the full life-\\ncycle, and proper requirements elicitation will signiﬁcantly\\ncontribute to this cause [19].\\nThe following relationship cardinalities are possible [10]:\\n• One-to-One;\\n• One-to-Many;\\n• Many-to-Many.\\nCardinalities could also be speciﬁc, for example, a department\\nmay have between ﬁve and ﬁfteen employees [18].\\nC. MongoDB Logical Modeling\\nMongoDB is a document-oriented database and data objects\\nhave the familiar JSON (JavaScript Object Notation) format,\\nthough stored in a more efﬁcient binary format. Documents\\nare contained within collections which in turn belong to a\\ndatabase [20].\\nUnlike typical relational databases, MongoDB’s data struc-\\nture is not structured as rows and columns, but can have\\nhierarchy - arrays and sub-documents - and often closely\\nresembles data structures used in an application. There are\\ntherefore many ways to correctly model data and relationships,\\nbut performance characteristics could vary [20].\\nTraditionally, during logical modeling, the entity-relational\\nconceptual model is decomposed into 3NF as tables and rela-\\ntionships [1], [10]. Since MongoDB does not typically follow\\na normalization design paradigm, choices about relationships\\nboil down to deciding on:\\n• Referencing (normalization);\\n• Embedding (denormalization).\\nEven within these two options many variations exist [21].\\nSome of these options are discussed in the following sections,\\nwith some advanced patterns to consider during physical\\nmodeling.\\n1) Referencing: Referencing is the MongoDB equivalent\\nto primary/foreign key relationships in relational databases [9].\\nA collection can be referenced by including a ﬁeld referencing\\na unique attribute of a document in another collection. A\\nspecial case of this is referencing another document in the\\nsame collection, called self-joining [20]. MongoDB does not\\nhowever automatically ensure referential integrity and relies\\non application level functionality to achieve that [22].\\nOne factor that is always a certain scenario for referencing\\nis whether the relationship has a high arity. In other words,\\na distinction has to be made between many and few [22]. In\\nthe case of few, embedding could be considered, otherwise\\na single document would grow large (maximum of 16 MiB),\\nrequiring more RAM and cause slower data transfers [20].\\nHigh-arity many-to-many relationships should typically be\\nmodeled with a referenced join-collection, much like in a\\nrelational table. Again, if either side was few, embedding could\\nbe considered [20].\\nAdvantages of referencing include [20]:\\n• No redundancy;\\n• Immediate consistency;\\n• Supports high-arity relationships;\\n• Flexibility in queries and indexing (physical model);\\n• Expanded sharding options.\\nDisadvantages of referencing are [20]:\\n• Application level joins;\\n• Depending on query patterns, limited scalability;\\n2) Embedding: Embedding, here, refers to the process of\\nadding a sub-document or ﬁelds of a conceptual entity wholly\\nand directly into a related document or documents, instead of\\ncreating a separate collection and referencing only the primary\\nidentiﬁer of related documents [9].\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='Embedding has the following advantages [20]:\\n• Application side joins aren’t needed;\\n• Inherent atomic and isolated operations for one-to-few\\nrelationships exist;\\n• Data locality is utilized;\\n• Inheritance is efﬁciently modeled;\\n• Faster query performance is available.\\nDisadvantages of embedding data include [20]:\\n• Redundant copies are present;\\n• Eventual consistency is achieved;\\n• Application level cascading is required for consistency;\\n• Poor performance is offered for high-arity relationships;\\n• Less ﬂexibility is available as collections are pre-joined.\\n3) Summary: Table I summarizes UML class diagram\\nconcepts relating to MongoDB’s document model.\\nTABLE I\\nUML CLASSES TO MONGO DB ANALOGY\\nUML Concept MongoDB Concept\\nClass Collection\\nObject Document\\nAttribute Field\\nAssociation Embed or reference\\nAggregation Usually reference\\nComposition Usually embed\\nInheritance Usually embed\\nD. MongoDB Physical Modeling\\nDuring physical modeling, the database designer consid-\\ners access-path-independent and access-path-dependent design\\nchoices. This includes application access patterns, indexing\\nschemes and sharding keys [1], [2], [11]. MongoDB physical\\nmodeling consists of many factors to consider [9]:\\n• _id choice;\\n• Schema validation;\\n• Indexing;\\n• Sharding;\\n• Replication;\\n• Consistency level choices;\\n• Application access patterns;\\n• MongoDB advanced features.\\nMany of these factors will also have a bearing on the underly-\\ning logical modeling choices, especially indexing and sharding\\nkey choices [9].\\nAll documents in MongoDB must have an _id ﬁeld that\\nis unique. In fact all collections include a unique index on\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='Content Missing\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='Content Missing\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='TABLE II\\nCONCEPTUAL DESIGN OPTIONS\\nConcept Referencing/Embedding Design Pros Cons\\nLibrary\\nReference (collection) Shardable Joins may be required\\nEmbed within Book Data locality Eventual consistency; difﬁcult to query\\nPartially embed within Book Shardable; data locality Eventual consistency\\nBook\\nReference (collection) Shardable Joins required\\nEmbed within Library Data locality; single read operations Boundless; complex queries;\\nPartially embed within Library Additional feature options Additional complexity\\nBook\\nCopies\\nReference (collection) Shardable Disjoint from Books\\nEmbed within Library Data locality Boundless\\nEmbed within Book Data locality; single view on checkout status Larger working set\\nCheckout\\nReference (collection) Shardable Transactions required\\nEmbed within Reader Single view; quick access Transactions required; possibly boundless\\nEmbed within Reader with bucket pattern Single view; quick access; not bounded Transactions required; outlier pattern\\nEmbed within Book or Book Copies No transactions No history\\nPartially embed within Book Copies No transactions History; eventual consistency\\nReview\\nReference (collection) Shardable; single full-text index Joins may be required\\nEmbed within Checkout Single review per checkout Complex queries to group by book/reader\\nEmbed within Book Data locality Boundless\\nRatings\\nReference (collection) Shardable; single full-text index Joins may be required\\nEmbed within Checkout Single rating per checkout Complex queries to group by book/reader\\nEmbed within Book as counts Data locality; pre-calculation options Calculation required per read\\n<<collection>>\\nReader\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tname:\\tstring\\n+\\tcheckouts:\\t[<<embedded>>]\\n*\\n<<collection>>\\nBook\\n+\\tisbn:\\tstring\\t<<PK>>\\n+\\ttitle:\\tstring\\n+\\tauthor:\\t[string]\\n+\\tdescription:\\tstring\\n+\\tlibbooks:\\t[<<embedded>>]\\n+\\tratingtotal:\\tnumber\\n+\\tratingcount:\\tnumber\\n<<collection>>\\nReview\\n+\\tid:\\tstring\\t<<PK>>\\n+\\treader:\\t<<FK>>\\n+\\tlibbook:\\t<<partial\\tembed>>\\n+\\treview:\\tstring\\n+\\treviewdate:\\tdate\\n<<embedded>>\\nCheckout\\n+\\tlibbook:\\t<<partial\\tembed>>\\n+\\tcheckout:\\tdate\\n+\\tcheckin:\\tdate\\t<<nullable>>\\n*\\n* 1\\n<<collection>>\\nLibrary\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tname:\\tstring\\n+\\taddress:\\tstring\\n<<embedded>>\\nLibraryBook\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tbook:\\t<<FK>>\\n+\\tlibrary:\\t<<embedded>>\\n+\\tcheckout:\\tdate\\t<<nullable>>\\n1\\n*\\n1\\n1\\n1\\n1\\n*\\n1\\nFig. 4. Library logical model\\nﬁlter with the same constraints. The resulting list of documents\\nis then grouped and summed by reader. The result is sorted\\nand limited to 10 - the top ten readers of the month.\\nCounting the number of books rented requires an index on\\nthe embedded checkouts’ checkout date. Using the aggregation\\npipeline to match only readers that have checkouts in that\\nmonth. Unwinding and grouping by library, while summing\\neach checkout will deliver the number of rented books per\\nlibrary.\\nAverage ratings can be calculated on demand by taking\\nthe total summed ratings, divided by the count - the ﬁelds\\nwere already included in the logical model for Book. In order\\nto optimize this, an additional ﬁeld should be added to pre-\\ncalculate the monthly ratings. This can be scheduled daily for\\nexample.\\nConsidering the writes to the database, only checkouts are\\ncomplicated, since a library may have more than one copy of\\na book and there are more than one library. The LibraryBook\\nembedded array allows for a single book document to keep\\ntrack of each copy along with an optional checkout date ﬁeld.\\nIf this is set to null the book is available. During checkout\\nthis is set to the checkout date and when checked in it gets set\\nback to null. Similarly in order to keep track of each reader’s\\ncheckouts and check-ins, a transaction is done to modify the\\nLibraryBook element and modify the reader checkout array.\\nFor this project strong consistency can be employed for all\\ntransactions since the few embedded ﬁelds, such as library\\naddress and book ISBN rarely change.\\nIt is unlikely that sharding would be required for such a\\nproject, since reads and writes occur rarely and in person.\\nIf needed, though, Book and and Reader can be sharded on\\nprimary key.\\nIf it had been required that data per library be isolated a\\nmore normalized approach would have had to be followed\\nin order to make the shard key per library. This would\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='render checkouts, and library books as their own collection.\\nShard tags could then be employed to isolate speciﬁc library\\ninformation to speciﬁc database servers.\\nIn order to keep working sets small it could become\\nnecessary to move checkouts to a separate collection, which\\nwould require embedding reader information in the checkouts\\nand cascading updates to user details in order for aggregations\\nto group by reader.\\nReading from replicas could also improve read scalability,\\nwithout compromising on consistency for critical operations.\\nA. Discussion\\nAll design options given in Table II, though not inclusive\\nof every permutation, can validly be modeled in logical an\\nphysical form while still meeting requirements. However, each\\noption has an direct impact on the complexity and performance\\nof the application and database design. There is no golden\\nrule, such as the 3NF in relational databases, to aid NoSQL\\ndatabase designers.\\nSince many design choices rely on application logic, it can\\nbe said that the application and database have to be designed\\ntogether.\\nConsider, for example, that a reader may only rate a book\\nonce. A requirement such as that dramatically impacts design\\nchoices for the application and database. The current rating\\nsystem would not be able to support such a requirement.\\nThe complexity of the above use-case could be exponen-\\ntially increased with the addition of a new concept such as\\naccount management. The use-case is not intended to serve\\nas a complete and all-encompassing solution, but instead offer\\nan overview of the complexities involved with NoSQL data\\nmodeling due to its inherent ﬂexibility.\\nFurthermore, speciﬁc deployment requirements have not\\nbeen speciﬁed and also have a bearing on underlying design\\nchoices, adding an additional layer of complexity.\\nV. C ONCLUSION\\nFrom the example use case it can be seen that MongoDB\\nmakes it possible to implement use cases in a variety of ways.\\nHow to design a database depends on use case requirements,\\nwhich must be properly elicited in order to avoid large changes\\nlater on. The processes and factors weighing in on the design\\nchoices have been discussed in the previous sections and\\nprovide a guide to concepts and tradeoffs required during the\\ndesign phase.\\nThe complexity of designs are affected by the complexity\\nof requirements. Complex designs will inevitably cost more to\\ndevelop and maintain, though complexity handled during the\\ndesign phases may result in simpler and more robust solutions\\nin the future.\\nForm-follows-function [24] - this common term has the\\nimplication for database and application design, that function\\ndictates underlying form, structure and design choices. It is the\\nconjecture of the authors that the traditional entity-relational\\napproaches suffer from lack of foresight and understanding of\\nthe whole system and its functions and resource requirements.\\nIt is instead based on functions-follow-form where entities\\nwhich have data-value are modeled very directly as acted upon\\nby users. Rarely, application access patterns and underlying\\nresources are discussed and weighed in on design choices -\\nespecially when considering full life-cycle requirements.\\nMore speciﬁcally, future growth is often not accounted for\\nas it is usually addressed in the physical model as a means of\\noptimizing queries as dictated by the structure of the logical\\ndesign. As shown in the use-case example the physical model\\ndesign is greatly impacted by and reliant on the prior designed\\nlogical model. Furthermore, changes required in the physical\\nmodel requires changes on the underlying logical model - they\\nare clearly interdependent.\\nThis paper therefore submits that physical and logical\\nmodeling phases in NoSQL, and more speciﬁcally MongoDB,\\nshould coincide as a single detail design phase. This is due to\\nthe fact that choices on either side are dependent on the other.\\nWhen considered as a unit, it may lead to shorter design phases\\nand less back-to-the-drawing-board situations.\\nIn large projects, the requirement for an interactive design\\neffort (physical and logical) implies good communication and\\ncoordination between design teams, should such teams be\\nused.\\nFurthermore, requirements elicitation should provide insight\\ninto potential growth and outlier query patterns and require-\\nments, which often impact the general case. The requirements\\ndeﬁne the functions and constraints, which in turn dictates the\\nform that the application and database (the resources) should\\ntake on. It is an outside-in approach taking into consideration\\nthe application design and user access patterns chieﬂy.\\nThis is in contrast with applications of Chen’s methods,\\nwhich start off with data-yielding entities, leading to concepts,\\nlogical forms and ultimately a physical database. It is typically\\nonly after this where application access patterns are typically\\nﬁtted to pre-existing underlying data structures. This is an\\ninside-out data-oriented approach.\\nEven though the example use case was imagined and\\nartiﬁcial, it has conveyed the type of design choices and\\nmethods that should be followed for MongoDB applications.\\nFurther research is needed to formally design a database\\nand application framework for modeling. It should address\\nthe following problems and considerations:\\n• User activity workﬂows;\\n• Impact of requirements;\\n• Maintenance workﬂow impacts;\\n• Design and maintenance costs;\\n• Deployment impacts.\\nIn general, NoSQL modeling techniques are under-\\nresearched and due to their ﬂexible nature, may bring forth\\nnew design approaches previously impossible or too complex\\nto attempt with only rows and columns.\\nFinally, with cloud-based applications usually being the\\ntarget for NoSQL development, the impacts of its constraints\\nand features on design choices should be further investigated.\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'file_type': 'pdf', 'file_name': '[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf'}, page_content='VI. A UTHORS AND AFFILIATIONS\\nPieter Jordaan (MEng) is currently pursuing his PhD in\\nEngineering at the North-West University in South Africa.\\nHe is a student member of IEEE. His research ﬁeld includes\\ntopics on cloud applications, NoSQL database development\\nand scalability.\\nJohann Holm (PhD, PrEng) is an associate professor\\nat the School for Electrical, Electronic and Computer\\nEngineering of the North-West University in South Africa.\\nHe is a member of IEEE and a member of INCOSE, and\\nis actively involved in research and development, as well as\\nconsulting in operations and product development.\\nREFERENCES\\n[1] P. P.-S. Chen, “The entity-relationship model—toward a uniﬁed view of\\ndata,” ACM Transactions on Database Systems, vol. 1, no. 1, pp. 9–36,\\n3 1976.\\n[2] K. Shin, C. Hwang, and H. Jung, “NoSQL Database Design Using\\nUML Conceptual Data Model Based on Peter Chens Framework,”\\nInternational Journal of Applied Engineering Research, vol. 12, no. 5,\\npp. 632–636, 2017.\\n[3] C. de Lima and R. dos Santos Mello, “A workload-driven logical design\\napproach for NoSQL document databases,” in Proceedings of the 17th\\nInternational Conference on Information Integration and Web-based\\nApplications &Services - iiWAS ’15. New York, New York, USA:\\nACM Press, 2015, pp. 1–10.\\n[4] P. W. Jordaan and J. E. W. Holm, “Implementation and veriﬁcation of\\na cloud-based machine-to-machine data management system,” in 2013\\nAfricon. IEEE, 9 2013, pp. 1–5.\\n[5] K. Chodorow, Scaling MongoDB, 1st ed. Sebastopol: O’Reilly, 2011.\\n[6] A. Kanade, A. Gopal, and S. Kanade, “A study of normalization and\\nembedding in MongoDB,” Souvenir of the 2014 IEEE International\\nAdvance Computing Conference, IACC 2014, pp. 416–421, 2014.\\n[7] G. Zhao, W. Huang, S. Liang, and Y . Tang, “Modeling MongoDB\\nwith relational model,” Proceedings - 4th International Conference on\\nEmerging Intelligent Data and Web Technologies, EIDWT 2013, 2013.\\n[8] E. Horowitz, “MongoDB Drops ACID,” 2018. [Online]. Avail-\\nable: https://www.mongodb.com/blog/post/multi-document-transactions-\\nin-mongodb\\n[9] MongoDB, “The MongoDB 4.0 Manual,” 2019. [Online]. Available:\\nhttps://docs.mongodb.com/manual/\\n[10] C. Coronel, S. Morris, and P. Rob, Database Systems: Design, Im-\\nplementation, and Management, 11th ed. Stamford, CT: CENGAGE\\nLearning, 2009.\\n[11] F. Abdelhedi, A. A. Brahim, F. Atigui, and G. Zurﬂuh, “UMLtoNoSQL:\\nAutomatic transformation of conceptual schema to NoSQL databases,”\\nProceedings of IEEE/ACS International Conference on Computer Sys-\\ntems and Applications, AICCSA, vol. 2017-Octob, no. 1, pp. 272–279,\\n2018.\\n[12] P. Atzeni, F. Bugiotti, L. Cabibbo, and R. Torlone, “Data modeling in\\nthe NoSQL world,” Computer Standards & Interfaces, no. October, pp.\\n0–1, 10 2016.\\n[13] M. Yusufu, H. J. Zhang, G. Yusufu, Z. D. Liu, P. Cheng, and D. Dilisati,\\n“Modeling and Analysis of Complex System with UML: A Case Study,”\\nApplied Mechanics and Materials, vol. 513-517, pp. 1346–1351, 2014.\\n[14] Object Management Group, “OMG Uniﬁed Modeling Language\\nTM (OMG UML),” p. 794, 2015. [Online]. Available:\\nhttp://www.omg.org/spec/UML/2.5/\\n[15] A. De Lucia, C. Gravino, R. Oliveto, and G. Tortora, “An experimental\\ncomparison of ER and UML class diagrams for data modelling,”\\nEmpirical Software Engineering, vol. 15, no. 5, pp. 455–492, 2010.\\n[16] C. Fahrner and G. V ossen, “A survey of database design transforma-\\ntions based on the Entity-Relationship model,” Data and Knowledge\\nEngineering, vol. 15, no. 3, pp. 213–250, 1995.\\n[17] C. Churcher, Beginning database design. New York: Apress, 2012.\\n[18] P. Ponniah, Database design and development: an essential guide for\\nIT professionals. John Wiley & Sons, Inc., 2003.\\n[19] H. Sammaneh, “Requirements Elicitation with the Existence of Similar\\nApplications: A Conceptual Framework,” 2018 International Conference\\non Computer and Applications, ICCA 2018, pp. 444–449, 2018.\\n[20] R. Copeland, MongoDB Applied Design Patterns, 1st ed. Sebastopol:\\nO’Reilly, 2013.\\n[21] D. Coupal and K. W. Alger, “Building with Patterns: A Summary,” 2019.\\n[Online]. Available: https://www.mongodb.com/blog/post/building-with-\\npatterns-a-summary\\n[22] K. Chodorow and M. Dirolf, MongoDB: the deﬁnitive guide, 2nd ed.\\nSebastopol: O’Reilly, 2013.\\n[23] D. Coupal and K. W. Alger, “Building With Patterns: The Outlier\\nPattern,” 2019.\\n[24] M. H. Wen and V . O. Li, “Form Follows Function: Designing Smart Grid\\nCommunication Systems Using a Framework Approach,” IEEE Power\\nand Energy Magazine, vol. 12, no. 3, pp. 37–43, 2014.\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf'}, page_content='Abstract – Amazon DynamoDB is a next-gen NoSQL (Not \\nonly SQL), key-value and document database that delivers \\nsingle-digit millisecond performance at any scale. As more \\ncomplex web-based applications adopt DynamoDB, it is \\nimperative to develop sound design strategies for DynamoDB \\nschemas. Large-scale web-based applications often exhibit \\nconflicting requirements. Hence, the research problem is to \\ndesign the DynamoDB schemas of a given application with \\nconflicting requirements such that the database must satisfy \\ncertain predetermined performance goals. In the era of \\nrelational databases, normal forms were developed to guide \\nthe schema design process. Past knowledge and experiences, \\nwe believe, are also applicable to DynamoDB schema design. \\nSpecifically, based upon Nested Normal Form and XML \\ndatabases, this paper demonstrates the feasibility of a design \\nstrategy on DynamoDB schemas, particularly with the access \\npatterns of the data in mind. Simulation further substantiates \\nthe feasibility of our approach. \\n \\nKeywords – Amazon DynamoDB Schema Design, Nested \\nNormal Form, XML Databases \\n \\nI.  INTRODUCTION \\n \\n As large-scale web-based applications demand speed, \\nflexibility and scalability, NoSQL database systems, a \\nnext-gen database systems, are being developed. Many \\ncommercial NoSQL database systems are now available. \\nNotable examples include MongoDB [10], Cassandra [5], \\nRedis [12] and Amazon DynamoDB [1] and others. The \\nunderlying data models of NoSQL databases are quite \\ndifferent from the tabular data model of the traditional \\nrelational databases. Some prevalent data models for \\nNoSQL databases are the document model, the graph \\nmodel and the key-value model [11]. Amazon DynamoDB \\nadopts the key-value model and the document model, and \\nmany top-level entities of a database are modeled as nodes \\nand their relationships as edges as in graphs [1]. \\n The results of system analysis, an indispensable \\nactivity of any large-scale system development, include \\nmany different types of diagrams focusing on various \\naspects of the system under development. UML diagrams \\n[4], promoted by robust modeling tools like Visual \\nParadigm [13] and others, have become a software industry \\nstandard. The proposed DynamoDB schema design \\nstrategy also begins with a graphical diagram, called a \\nconceptual-model hypergraph, which is in a way similar to \\nbut simpler than UML domain model class diagrams.  The \\npurpose of such a hypergraph, like a UML domain model \\n`class diagram, is to capture the static, rather than the \\ndynamic, aspects of a system of interest. \\n Nested Normal Form [9] was originally designed for \\nnested relational databases, which allow tables to be nested \\nwithin other tables, resulting in embedded tables and thus \\nhierarchical structures. Since XML databases [3] also \\nallow hierarchical structures, it is natural to extend Nested \\nNormal Form to XML databases. A critical contribution of \\nNested Normal Form is that XML databases that conform \\nto Nested Normal Form are guaranteed to be free of \\nunwanted redundant data. In [8], we devised algorithms \\nthat generate Nested Normal Form XML databases. We \\nshall shortly show that the algorithms in [8], with some \\nmodifications, are also applicable to DynamoDB as well. \\n We believe conceptual-model hypergraphs and the \\nXML database design algorithms form the conceptual \\nframework in which the solution for the research problem \\nstated in the abstract can be formulated. In contrast, in one \\nsingle page the documentation of Amazon DynamoDB \\nstates some NoSQL design principles for DynamoDB \\nschemas. Although the suggested design principles are \\nsound, they are so vague to formulate a formal DynamoDB \\ndesign strategy. \\n To convey the central idea of the proposed design \\nstrategy, the rigorous mathematics underlying [7,8,9] is \\navoided. Instead, this paper adopts a high-level, step-by-\\nstep presentation. To proceed, Section 2 concisely presents \\nthe methodology and shows that such an approach is \\nfeasible and desirable. Section 3 presents the results of our \\nexperiments, which were conducted on the AWS (Amazon \\nWeb Services) Educate platform  [2]. Section 4 discusses \\nmany-to-many relationships and we shall conclude and \\nstate the future research goals of this project in Section 5. \\n \\nII.  METHODOLOGY \\n \\n In a nutshell, the proposed design strategy is \\nsummarized as follows: \\n• Capture an application of interest with a \\nconceptual-model hypergraph [8], or some other \\nmeans that can be translated to a conceptual-\\nmodel hypergraph. \\n• Identify the access patterns of the most commonly \\nexecuted queries and operations in the \\nconceptual-model hypergraph.  \\n• Simplify the conceptual-model hypergraph by \\nidentifying object sets that are in one-to-one \\ncorrespondence with each other. \\n• Generate a set of hierarchical schemas from the \\naccess patterns based on Nested Normal Form [9]. \\nA Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal \\nForm Approach \\n \\nWai Yin Mok \\nDepartment of Information Systems, The University of Alabama in Huntsville, Huntsville, USA \\n (mokw@uah.edu) \\n903978-1-5386-7220-4/20/$31.00 ©2020 IEEE\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf'}, page_content='• Load the data of the application to Amazon \\nDynamoDB in accordance to the generated \\nhierarchical schemas. \\n \\nCrnSecID\\nSection Pair\\nStudent SSN\\nSemester\\nGrade\\nCourse−\\n \\n \\nFig. 1: A sample conceptual-model hypergraph. \\n \\nA. Conceptual-Model Hypergraphs \\n  \\n A highly abstracted conceptual-model hypergraph for \\na simple reporting system of a small college is shown in \\nFig. 1. To show the essential features of the proposed \\ndesign strategy, many details of the hypergraph are \\nomitted. For example, the name, address, picture, and other \\npersonal items of a student are nowhere to be found in the \\nhypergraph. Here, we identify the salient features of the \\nhypergraph, although similar explanations can also be \\nfound in [8]. \\n The hypergraph of Fig. 1 has six sets of objects, which \\nare called Student, SSN (Social Security Number), \\nSemester, Course-Section Pair, CrnSecID, and Grade. The \\nsets Student, SSN, Semester, and Grade are all self-\\nexplanatory. The set Course-Section Pair contains the pairs \\nof every course and all the sections of that course. As an \\nexample, an element of that set might be IS301-01, which \\nrepresents Section 1 of the course IS301: Introduction of \\nInformation Systems. We use a six characters string in this \\npaper to represent a course-section pair and CrnSecID is \\nthe set of all such strings. \\n There are three relationship sets among the sets of \\nobjects in Fig. 1. The first relationship set, represented as a \\nline, is between the sets Student and SSN. It is called a \\nrelationship set because it is a set of relationships. As an \\nexample, relationships, or elements, of that set might be \\n(student\\n1, 111223333) and (student 2, 222334444), \\nrepresenting student1 is related to the SSN 111223333 and \\nstudent2 to 222334444. The line between Student and SSN \\nhas arrow heads at both ends, which represents a one-to-\\none correspondence between the sets Student and SSN. In \\nour example, student\\n1 is thus paired with 111223333 and \\nso is student2 with 222334444. The second relationship set \\nis between the sets Course-Section Pair and CrnSecID, \\nwhich is also represented as a line. Because both ends of \\nthat line have arrow heads, each course-section pair is \\npaired with exactly one CrnSecID. The third relationship \\nset is among the sets Student, Semester, Grade, and \\nCourse-Section Pair, represented as a diamond. There is a \\nsingle arrow head of this relationship set, pointing at the \\nobject set Grade. It represents the functional dependency \\n(FD) Student, Course-Section Pair, Semester → Grade, \\nwhich means that for each tuple of a Student element, a \\nCourse-Section Pair element and a Semester element, there \\ncan only be one Grade element. The circle close to the set \\nCourse-Section Pair means that a particular course-section \\npair might or might not relate to the elements of the other \\nthree sets. In other words, there are some existing course-\\nsection pairs are not used in the relationship set. \\n \\nB. Access Patterns of the most commonly executed Queries \\nand Operations \\n \\n Suppose that the current semester is Summer 2020 \\n(2020S). At the end of 2020S, every instructor will enter a \\ngrade for every student in his/her classes. The reporting \\nsystem thus needs to present a class list to each instructor \\nand the instructor will enter a grade for every SSN on the \\nclass list. Hence, there are two access patterns, which are \\nshortened as AP1 and AP2, and their involved object sets \\nof Fig. 1 are outlined in Fig. 2. \\n \\nAccess Pattern 2\\nSection Pair\\nStudent SSN\\nSemester\\nGrade\\nCrnSecID\\nAccess Pattern 1\\nCourse−\\n \\n \\nFig. 2: Access patterns of two commonly executed query and update \\noperation at the end of 2020S. \\n \\nAP1: The query that generates a class list of 2020S will \\nfirst select the semester 2020S and a particular CrnSecID. \\nThen a list of SSNs is generated. Thus, this query involves \\nall of the object sets in Fig. 1 except Grade.  \\nAP2: The update operation is to record the grade earned by \\na student for any class of 2020S for which the student \\nregistered. Thus, this operation involves every object set of \\nthe relationship set in Fig. 1. \\n \\nC. Simplification of Conceptual-model Hypergraphs  \\n \\n Because of the one-to-one correspondence between the \\nobject sets Student and SSN, each student can be replaced \\nby his/her SSN. The same is also true for the object sets \\nProceedings of the 2020 IEEE IEEM\\n904\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf'}, page_content='Course-Section Pair and CrnSecID. We thus obtain the \\nsimplified hypergraph in Fig. 3. \\n \\nAccess Pattern 2\\nSemester\\nGrade\\nAccess Pattern 1\\nSSN\\nCrnSecID\\n \\n \\nFig. 3: A simplified conceptual-model hypergraph. \\n \\nD. Generation of Hierarchical Schemas \\n \\n The algorithms in [8] can generate hierarchical \\nschemas from any conceptual-model hypergraph with any \\nnumber of relationship sets and each relationship set can \\ncontain any number of object sets. The hypergraph in Fig. \\n3 only has one single relationship set that contains four \\nobject sets. First, note that the single relationship set of Fig. \\n3 is in BCNF [6], meaning that it does not have data \\nredundancy with respect to the FD SSN, CrnSecID, \\nSemester → Grade. Based on Nested Normal Form, the \\nobject sets of the relationship set can be organized in any \\nsingle path hierarchical schema without any unwanted \\nredundant data. Two possible Nested Normal Form \\nhierarchical schemas are shown in Fig. 4. \\n The left hierarchical schema is derived from Access \\nPattern 1, denoting the fact that for each pair of CrnSecID \\nand Semester, there is a set of SSNs. Note that the object \\nset Grade is left out because Grade is not needed in the \\nquery that generates the class list of any class of 2020S. \\nThe right hierarchical schema is derived from Access \\nPattern 2. It means that for each pair of SSN and Semester, \\nthere is a list of pairs of the form CrnSecID and Grade. \\n  \\nIII.  RESULTS \\n \\nA.  Simulation Background \\n \\nThe experiments were conducted on AWS Educate, a \\nfree platform for cloud learning [2]. A Cloud9 \\nenvironment, which is a cloud IDE (Integrated \\nDevelopment Environment) for writing, running, and \\ndebugging code, was first created. We selected the default \\nsettings for the environment whose EC2 instance type is \\nt2.micro that has 1 virtual CPU, 1 GB of memory, and a \\nLinux OS. (Amazon Elastic Compute Cloud (EC2) is a web \\nservice that provides secure, resizable compute capacity in \\nthe cloud.) A real-world production environment will \\nclearly have a more substantial virtual computing \\nenvironment. \\n \\n \\n \\nFig. 4: Two hierarchical schemas generated from the access patterns in \\nFig. 3. \\n \\nAfter which, a Python program generated two \\nDynamoDB tables, called IEEM2020-CrnSec and \\nIEEM2020-Students where IEEM2020-CrnSec is to hold \\nthe data of the left hierarchical schema in Fig. 4 and \\nIEEM2020-Students for the right one. The read and write \\ncapacity of each table were set to 500 units, which costs \\nabout $290 monthly for each table. The partition key of \\nIEEM2020-CrnSec is set to Semester and the sort key to \\nCrnSecID. This is appropriate because the records of \\nIEEM2020-CrnSec ought to be partitioned according to \\nSemester, and within each semester the records are further \\nsorted according to their Cr nSecIDs. Each Semester and \\nCrnSecID pair in IEEM2020-CrnSec has a list of SSNs \\nwhere a DynamoDB list is similar to a Python list. For the \\ntable IEEM2020-Students, the partition key is SSN and the \\nsort key is Semester. This is also appropriate because \\nstudents are the focus of IEEM2020-Students and thus SSN \\npartitions the table. The records of a student are further \\nsorted according to Semester. Each SSN and Semester pair \\nin IEEM2020-Students is associated with a list of maps \\nwhere a DynamoDB map is similar to a Python dictionary. \\nNote that the root nodes of the two hierarchical \\nschemas each consists of two object sets. Although such \\nchoices are dictated in part by the semantics of the \\napplication, they are chosen also because primary keys of \\nDynamoDB tables can only contain two or less attributes. \\nNote that this is a severe limitation because the primary key \\nof the relationship set in Fig. 3 is in fact SSN, Semester, \\nCrnSecID and many real-world primary key may consist of \\nmore than two attributes. \\n \\nB.  Simulation Data \\n \\n We’ve randomly generated 1,000 through 10,000, in \\nthe increment of 1,000, active students to simulate the \\nworkload of the reporting system at the end of 2020S. Only \\nactive students have been simulated because only active \\nstudents have registered for classes in 2020S and only they \\nwill receive a grade for their classes of 2020S. Each active \\nstudent has enrolled in classes for a number x of semesters \\nwhere the integer x follows an exponential distribution with \\nthe scale set to 6. Hence, most active students have enrolled \\nin less than 5 semesters and at most 12 semesters. Each \\nactive student has registered a number x of courses in a \\nAccess Pattern 1\\nCrnSecID, Semester\\nSSN\\nSemesterSSN,\\nGradeCrnSecID,\\nAccess Pattern 2\\nProceedings of the 2020 IEEE IEEM\\n905\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf'}, page_content='semester where the integer x follows a normal distribution \\nwith the mean and standard derivation respectively set to 4 \\nand 1. All of these data were written out to JSON \\n(JavaScript Object Notation) files, preparing to be loaded \\ninto the DynamoDB tables. \\n \\n[ \\n{ \\n\"SSN\": 100327004, \\n\"Semester\": \"2020S\", \\n\"CrnSecList\":{\"432754\": \" \",\"567017\": \" \",\"716933\": \" \",\"290347\": \" \"} \\n}, \\n{ \\n\"SSN\": 100327004, \\n\"Semester\": \"2020W\", \\n\"CrnSecList\":{\"317239\": \"D\",\"312013\": \"A\",\"772832\": \"A\",\"294905\": \"D\"} \\n}, \\n... \\n] \\nFig. 5: Some randomly generated active student records. \\nFig. 5 shows a part of the JSON file that contains some \\nrandomly generated student records about an active student \\nwhose SSN is 100327004, and the semesters for which \\nhe/she enrolled (2020S and 2020W), and the grade he/she \\nearned for each CrnSecID of which he/she registered. Note \\nthat the grades for 2020S are all blank. \\n \\n[ \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"116447\", \\n\"StdList\": [143267942, 167456489, 191522130, 203634078, 218949345, 228395488, \\n262223274, 299359064, 340291307, 383893398, 402528408, 429940466, 491781834, \\n503895266, 524376955, 555747721, 588971187, 622489021, 632925410, 683107756, \\n707819456, 736073179, 746724915, 753864030, 759208116, 763603644, 764238408, \\n775280136, 783255531, 808439142, 814791444, 827636210, 865237592, 921601262, \\n942888972, 969574148, 972276135] \\n}, \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"143078\", \\n\"StdList\": [111183936, 143267942, 156225049, 157319776, 163586751, 224053635, \\n233214797, 234530365, 262458151, 295119560, 321794227, 329578153, 331832654, \\n393320569, 406218919, 459185184, 503871987, 509132188, 523055023, 547079471, \\n587846611, 589394822, 614437773, 655053546, 659577543, 665339504, 665548465, \\n738868279, 741704917, 775280136, 816330964, 823199550, 877078492, 897820459, \\n923783761, 944155639, 977837091] \\n}, \\n... \\n] \\nFig. 6: Some randomly generated class lists.   \\n Fig. 6 shows a part of the JSON file that contains the \\nclass lists of certain pairs of CrnSecID and 2020S. Each \\nclass list is essentially a list of SSNs \\n \\nC.  Simulation Results. \\n The query that retrieves every class list of 2020S and \\nthe update operation that assigns a grade to every class of \\neach active student of 2020S are written as parts of a \\nPython program, shown in Fig. 7. The code segment was \\nrepeated 5 times, and the elapsed time was printed out at \\nthe end of the loop.  \\nimport timeit \\n \\ncode_to_test = \"\"\" \\nimport random \\nimport boto3 \\nfrom boto3.dynamodb.conditions import Key \\n \\ndynamodb = boto3.resource(\\'dynamodb\\', region_name=\\'us-east-1\\') \\ntableC = dynamodb.Table(\\'IEEM2020-CrnSec\\') \\ntableS = dynamodb.Table(\\'IEEM2020-Students\\') \\n \\nrespC = tableC.query(KeyConditionExpression=Key(\\'Semester\\').eq(\\'2020S\\')) \\n \\nfor item in respC[\\'Items\\']: \\n    for sid in item[\\'StdList\\']: \\n        respS = tableS.update_item( \\n            Key = {\\'SSN\\': sid, \\'Semester\\': \\'2020S\\'}, \\n            ExpressionAttributeNames={ \\n                \"#crnsec\": item[\\'CrnSec\\'], \\n            }, \\n            ExpressionAttributeValues={ \\n                \":grade\": random.choice([\\'A\\',\\'B\\',\\'C\\',\\'D\\',\\'E\\']), \\n            }, \\n            UpdateExpression=\"set CrnSecList.#crnsec = :grade\", \\n            ) \\n     \\n\"\"\" \\nelapsed_time = timeit.timeit(code_to_test, number=5) \\nprint(elapsed_time) \\nFig. 7: The Python program that retrieves every class list of 2020S and \\nupdates the grade of every class of every active student.  \\nSince the SSNs of every class list are randomly \\ngenerated, every student has an equal probability of being \\nupdated. Therefore, there are no localities of updates. Table \\n1 shows the average time the Python program took to \\nperform an update and Fig. 8 shows the information \\ngraphically. Note that DynamoDB took less than 0.0085 \\nsecond to perform an update for no more than 10,000 \\nstudents.\\n \\n  TABLE I \\nAVERAGE TIME OF AN UPDATE \\n \\nNum of \\nstudents \\nNum of \\ncourse \\nsection \\npairs \\nNum of \\niterations \\nNum of \\nupdates \\nElapsed \\ntime \\nAverage \\ntime of \\neach \\nupdate  \\n1000 100 5 3975 140.821 0.00709 \\n2000 200 5 7924 298.602 0.00754 \\n3000 300 5 12027 448.486 0.00746 \\n4000 400 5 16109 547.187 0.00679 \\n5000 500 5 20015 746.519 0.00746 \\n6000 600 5 24031 885.607 0.00737 \\n7000 700 5 28020 1116.735 0.00797 \\n8000 800 5 31924 1293.276 0.00810 \\n9000 900 5 35851 1353.335 0.00755 \\n10000 1000 5 40207 1470.509 0.00731 \\n \\n \\n \\nFig. 8: Plotting average time of an update against number of updates.  \\n \\n To perform a stress test on the update operations on the \\ndatabase, we’ve randomly generated a long text string to \\nstore with each SSN and Semester pair in the table \\nIEEM2020-Students, where the length of every text string \\nfollows a normal distribution with the mean and standard \\nderivation set to 4,000 and 100 bytes respectively. We’ve \\nalso inserted a long text string for every pair of Semester \\nand CrnSecID in the table IEEM2020-CrnSec. The purpose \\nof these long text stings in both tables are to ensure that \\nthere are great physical distances among the updates. The \\nresults of the stress test are shown in Table 2. Note that \\neven though long text strings have been inserted in both \\ntables, the average time for each update in IEEM2020-\\nStudents is not much different than the case that has no long \\ninserted text strings. \\n \\n \\n \\nProceedings of the 2020 IEEE IEEM\\n906\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf'}, page_content='TABLE II \\nAVERAGE TIME OF AN UPDATE WITH LONG TEXT STRINGS \\nINSERTED. \\n \\nNum of \\nstudents \\nNum of \\ncourse \\nsection \\npairs \\nNum of \\niterations \\nNum of \\nupdates \\nElapsed \\ntime \\nAverage \\ntime of \\neach \\nupdate  \\n1000 100 5 3992 140.662 0.00705 \\n2000 200 5 7976 300.682 0.00754 \\n3000 300 5 11997 470.264 0.00784 \\n4000 400 5 15956 567.652 0.00712 \\n5000 500 5 19935 732.344 0.00735 \\n \\nIV.  DISCUSSION \\n \\n Astute readers might discover that the relationship \\nbetween a SSN, a Semester and a CrnSecID is being stored \\nin two distinct places, once in the table IEEM2020-CrnSec \\nand once in IEEM2020-Students, although the same \\nrelationship is being stored with different focuses in the \\ntwo tables. The Developer Guide for Amazon DynamoDB \\nin fact recommends using adjacency list to model many-to-\\nmany relationships, where we can see that a many-to-many \\nrelationship is also being stored in two distinct places in the \\nadjacency list. A basic tenet of the theory of relational \\ndatabases is that redundant data always lead to high update \\ncost. However, many practitioners of NoSQL databases \\noftentimes champion denormalization in order to improve \\nperformance. This, of course, deserves more study. \\n \\nV.  CONCLUSION \\n \\n In this paper, we have presented evidence for the \\nfeasibility of a DynamoDB schema design strategy, which \\nis based on Nested Normal Form and past experiences on \\nXML databases. Such a design strategy begins with a \\nconceptual-model hypergraph, which is in a way similar to \\nUML domain class diagrams. It then generates hierarchical \\nschemas in Nested Normal Form that are guaranteed to be \\nfree of unwanted redundant data. The generated \\nhierarchical schemas are then mapped to DynamoDB \\ntables for implementation. \\n We have performed experiments to further show the \\nusefulness of the proposed design strategy. We have \\nrandomly generated 1,000 through 10,000 active students, \\nin the increment of 1,000, and each active student has \\nenrolled in a number of semesters that follows an \\nexponential distribution. In addition, each active student \\nhas registered for a number of courses that follows a \\nnormal distribution in each semester. The results show that \\non average each update has taken less than 0.0085 second. \\nWe’ve also subjected the database to a stress test where a \\nlong text string was inserted into a student record and a \\ncourse-section record. The purpose of which is to ensure \\nthat updates are not done in close localities. It turns out that \\nthe average update time is about the same with or without \\nthe inserted long text strings. \\n Much more work remains to be done. Currently we \\nconstruct a hierarchical schema for each assess pattern, \\nwhich may lead to redundancy. However, we shall study \\nthe update cost for making such a choice. As more results \\nare being developed, a future journal paper shall report the \\nfiner details of the proposed schema design strategy.  \\n \\nREFERENCES \\n \\n[1] “Amazon DynamoDB: Fast and flexible NoSQL database \\nservice for any scale,” 2020. Accessed on: May 30, 2020. \\n[Online].Available: https://aws.amazon.com/dynamodb/ \\n[2] “AWS Educate: Your Cloud Journey Starts Here,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://aws.amazon.com/education/awseducate/ \\n[3] “BaseX: The XML Framework,” 2020. Accessed on: May 30, \\n2020. [Online]. Available: http://basex.org/ \\n[4] G. Booch, J. Rumbaugh and, I. Jacobson, The Unified \\nModeling Language User Guide . Addison Wesley object \\ntechnology series, Addison-Wesley, 2005 \\n[5] “Cassandra: Manage massive amounts of data, fast, without \\nlosing sleep,” 2020. Accessed on: May 30, 2020. [Online]. \\nAvailable: https://cassandra.apache.org/\\n \\n[6] D. Maier, The Theory of Relational Databases . Computer \\nScience Press, 1983. \\n[7] W.Y. Mok, and D.W. Embley, “Generating Compact \\nRedundancy-Free XML Documents from Conceptual-Model \\nHypergraphs,” IEEE Trans. Knowl. Data Eng. , vol. 18, no. \\n8, pp. 1082–1096, 2006. \\n[8] W.Y. Mok, J. Fong, and D.W. Embley, “Generating the \\nfewest redundancy-free scheme trees from acyclic \\nconceptual-model hypergraphs in polynomial time,” Inf. \\nSyst., vol. 41, pp. 20-44, 2014. \\n[9] W.Y. Mok, Y.K. Ng, and D.W. Embley, “A Normal Form \\nfor Precisely Characterizing Redundancy in Nested \\nRelations,” ACM Trans. Database Syst. , vol. 21, no. 1, pp. \\n77-106, 1996. \\n[10] “MongoDB: The database for modern applications,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.mongodb.com/\\n \\n[11] “NoSQL Databases.” 2020. A ccessed on: May 30, 2020. \\n[Online]. Available: https://www.trustradius.com/nosql-\\ndatabases \\n[12] “Redis: Experience the fastest NoSQL database in the \\ncloud.” 2020. Accessed on: May 30, 2020. [Online]. \\nAvailable: https://redislabs.com/ \\n[13] “Visual Paradigm: Stay Competitive and Responsive to \\nChange Faster & Better in the Digital World,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.visual-paradigm.com/\\n \\nProceedings of the 2020 IEEE IEEM\\n907\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n54 | P a g e  \\n \\nSQL vs. NoSQL: Choosing the Right Database for Your E-\\ncommerce Platform \\n1Lakshmi Nivas Nalla, 2Vijay Mallik Reddy \\n1Data Engineer Lead, Florida International University,11200 SW 8th St, Miami, FL 33199, \\nEmail:nallanivas@gmail.com \\n2Member of Technical Staff, University of North Carolina at Charlotte, Email: \\nvijaymr1012@gmail.com \\nAbstract: Selecting the appropriate database technology is a critical decision for e-commerce \\nplatforms aiming to scale effectively and accommodate the ever -increasing volume and \\ncomplexity of data. This paper provides an in -depth comparison of SQL (Structured Query \\nLanguage) and NoSQL (Not Only SQL) databases, elucidating their respecti ve strengths, \\nweaknesses, and suitability for e -commerce applications. Through a comprehensive analysis of \\nfactors such as data structure, scalability, performance, consistency, and flexibility, we offer \\nguidance to help e-commerce businesses make informed decisions when choosing between SQL \\nand NoSQL databases. By understanding the distinctive features and trade -offs associated with \\neach database paradigm, businesses can optimize their database architecture to support seamless \\noperations, enhance user experiences, and drive sustainable growth in the competitive e-commerce \\nlandscape. \\nKeywords: SQL, NoSQL, Database Management Systems, E -commerce, Scalability, \\nPerformance, Data Modeling, Consistency, Flexibility, Decision-Making. \\nIntroduction \\nIn the contempora ry digital landscape, the proliferation of e -commerce platforms has \\nrevolutionized the way consumers interact with businesses, ushering in an era of unprecedented \\nconvenience, choice, and accessibility. At the heart of these dynamic digital ecosystems lies  the \\ndatabase, serving as the foundational infrastructure that underpins the storage, retrieval, and \\nmanagement of vast volumes of transactional data, user profiles, and product catalogs. As e -\\ncommerce platforms strive to meet the evolving needs and expectations of modern consumers, the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n55 | P a g e  \\n \\nselection of an appropriate database technology emerges as a critical determinant of scalability, \\nperformance, and operational efficiency. \\nThe dichotomy between SQL (Structured Query Language) and NoSQL (Not Only SQL) \\ndatabases represents a fundamental choice faced by e -commerce businesses when architecting \\ntheir database systems. SQL databases, characterized by their relational data model and adherence \\nto ACID (Atomicity, Consistency, Isolation, Durability) properties, have long been the cornerstone \\nof traditional database management systems (DBMS). Conversely, NoSQL databases embrace a \\nmore flexible, schema -less approach, catering to the diverse data structures and distributed \\narchitectures prevalent in modern e-commerce applications. \\nThe decision -making process surrounding the selection of SQL versus NoSQL databases is \\nmultifaceted, encompassing a myriad of technical, operational, and business considerations. From \\ndata modeling and scalability to performance optimization and  data consistency, each database \\nparadigm presents unique strengths and trade-offs that must be carefully evaluated in the context \\nof the specific requirements and constraints of the e-commerce platform. \\nThis paper embarks on a comprehensive exploration of the SQL versus NoSQL debate in the realm \\nof e-commerce database management, with a fervent commitment to scientific rigor, empirical \\nvalidation, and practical relevance. Through a synthesis of existing literature, empirical studies, \\nand real-world case examples, we endeavor to elucidate the distinctive features, advantages, and \\nlimitations associated with each database paradigm. By providing a nuanced understanding of the \\ntechnical nuances and strategic implications inherent in the SQL versus NoSQL decisio n, this \\npaper aims to empower e -commerce businesses to make informed choices that align with their \\noverarching goals and objectives. \\nMoreover, the conduction of data relevant to the topics at hand forms the cornerstone of this \\ninquiry. By drawing upon a di verse array of data sources, including benchmarking studies, \\nperformance evaluations, and case studies from industry practitioners, we seek to ground our \\nanalysis in empirical evidence and real -world insights. Through meticulous data collection, \\nvalidation, and analysis, we strive to offer actionable recommendations and best practices that \\nresonate with the evolving needs and challenges faced by e -commerce businesses in an \\nincreasingly competitive marketplace. Thus, the scientific values upheld in this stud y underscore'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n56 | P a g e  \\n \\nour commitment to advancing knowledge and fostering informed decision -making within the e -\\ncommerce domain. \\nLiterature Review \\nThe discourse surrounding the SQL versus NoSQL debate in the context of e-commerce database \\nmanagement has been a focal point of scholarly inquiry and practical deliberation in recent years. \\nThis section offers a comprehensive review of the literature, synthesizing key findings, \\ncomparisons, and trends elucidated by researchers and industry experts. \\nHistorical Evolution: The historical evolution of database management systems (DBMS) sets the \\nstage for understanding the divergent trajectories of SQL and NoSQL databases. SQL databases, \\nrooted in the relational model pioneered by Edgar F. Codd in the 1970s, have long been \\nsynonymous with structured data storage, declarative querying, and transactional integrity. In \\ncontrast, the emergence of NoSQL databases in the late 2000s marked a paradigm shift towards \\ndistributed, non -relational data stores optimized for scalability, flex ibility, and performance in \\nweb-scale applications (Brewer, 2012). \\nPerformance and Scalability:  One of the primary motivations driving the adoption of NoSQL \\ndatabases in e-commerce applications is their superior performance and scalability characteristics \\ncompared to traditional SQL databases. Research by Stonebraker et al. (2007) demonstrated the \\nlimitations of SQL databases in handling the high volume and velocity of data generated by e -\\ncommerce transactions, highlighting the need for alternative approaches to database management. \\nNoSQL databases, with their distributed architectures and horizontal scalability, offer compelling \\nsolutions to the scalability challenges inherent in e -commerce platforms, enabling seamless \\nhandling of massive datasets and concurrent user interactions (Kaufman et al., 2010). \\nData Modeling and Flexibility:  The relational data model enforced by SQL databases imposes \\nrigid schema structures that can be cumbersome to adapt in the dynamic, rapidly evolving context \\nof e-commerce applications (Bruns, 2018). NoSQL databases, by contrast, embrace a schema-less \\nor schema -flexible approach, allowing for agile data modeling and iteration in response to \\nchanging business requirements (Fowler, 2013). This flexibility is particularly advantageous in e-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content=\"International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n57 | P a g e  \\n \\ncommerce environments characterized by diverse product catalogs, evolving user preferences, and \\ncomplex transactional workflows (Cattell, 2010). \\nConsistency and Transactional Integrity: A recurring critique leveled against NoSQL databases \\npertains to their relaxed consistency models and eventual consistency guarantees, which deviate \\nfrom the strict ACID properties upheld by traditional SQL databases (Bailis et al., 2013). While \\nthis eventual consistency model may suffice for certain e -commerce use ca ses, such as \\nrecommendation engines and content delivery systems, it may fall short in scenarios requiring \\nstrong transactional guarantees, such as order processing and inventory management (Coulouris et \\nal., 2012). \\nReal-World Case Studies:  Several real -world case studies provide empirical evidence of the \\nefficacy and trade -offs associated with SQL and NoSQL databases in e -commerce applications. \\nFor instance, Amazon's transition from a monolithic SQL -based architecture to a distributed, \\nmicroservices-based architecture powered by NoSQL databases has been well -documented, \\nhighlighting the scalability and agility advantages afforded by NoSQL technologies in handling \\nthe company's massive scale and dynamic workload patterns (Vogels, 2009). \\nRecent Trends and Emerging Technologies: Recent years have witnessed the convergence of \\nSQL and NoSQL paradigms through the emergence of NewSQL databases, which aim to combine \\nthe scalability and flexibility of NoSQL with the transactional integrity of traditional SQL \\ndatabases (Stonebraker, 2010). These hybrid approaches, exemplified by technologies such as \\nGoogle Spanner and CockroachDB, represent promising avenues for reconciling the divergent \\nrequirements of e -commerce applications while preserving the robustness and con sistency \\nguarantees of SQL databases (Corbett et al., 2012). \\nConclusion: In conclusion, the literature review underscores the multifaceted nature of the SQL \\nversus NoSQL debate in e -commerce database management, encompassing considerations of \\nperformance, scalability, flexibility, consistency, and transactional integrity. While NoSQL \\ndatabases offer compelling solutions to the scalability challenges inherent in e -commerce \\nplatforms, they come with trade -offs in terms of consistency and transactional guarant ees. \\nConversely, SQL databases provide strong consistency guarantees but may struggle to scale \\neffectively in high -volume, distributed environments. As e -commerce platforms continue to\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n58 | P a g e  \\n \\nevolve and scale, the selection of an appropriate database technology r emains a pivotal decision, \\nnecessitating a nuanced understanding of the technical nuances and strategic implications \\nassociated with SQL and NoSQL paradigms. \\nLiterature Review \\nThe debate surrounding SQL versus NoSQL databases in e -commerce has been fueled by a \\nplethora of empirical studies and industry reports, each offering nuanced insights into the \\ncomparative advantages and limitations of these database paradigms. A seminal study by \\nStonebraker et al. (2007) compared the performance of SQL and NoSQL databases in handling e-\\ncommerce workloads, revealing that NoSQL databases outperformed their SQL counterparts in \\nterms of throughput and latency under high concurrency scenarios. These findings underscored the \\nscalability advantages of NoSQL databases in acco mmodating the unpredictable traffic patterns \\nand surges characteristic of e-commerce platforms. \\nConversely, research by Das et al. (2011) highlighted the trade-offs inherent in NoSQL databases, \\nparticularly in terms of consistency and transactional integrity. Through a series of benchmarking \\nexperiments, Das et al. demonstrated that NoSQL databases, while excelling in scalability and \\navailability, often sacrificed strong consistency guarantees, leading to potential data inconsistency \\nand integrity issues in  e-commerce applications. These findings underscored the importance of \\ncarefully evaluating the consistency requirements and trade-offs associated with different database \\ntechnologies in the context of specific e-commerce use cases. \\nMoreover, the advent of  cloud computing and distributed computing architectures has further \\ncomplicated the SQL versus NoSQL debate, introducing new considerations related to data \\nlocality, network latency, and cost -effectiveness. Research by Strauch et al. (2015) explored the \\nperformance implications of deploying SQL and NoSQL databases in cloud environments, \\nrevealing that while NoSQL databases offered inherent scalability benefits in distributed \\nenvironments, they incurred higher operational overhead and complexity compared to  SQL \\ndatabases. These findings underscored the need for e-commerce businesses to weigh the trade-offs \\nbetween scalability and operational simplicity when choosing between SQL and NoSQL databases \\nin cloud deployments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n59 | P a g e  \\n \\nFurthermore, the proliferation of real-time analytics and personalized customer experiences in e -\\ncommerce has spurred interest in database technologies capable of supporting complex event \\nprocessing and real-time data ingestion. Studies by Chen et al. (2018) investigated the suitability \\nof SQL and NoSQL databases for real-time e-commerce analytics, highlighting the challenges and \\nopportunities associated with processing high -velocity, high -volume data streams in real -time. \\nWhile NoSQL databases offered inherent advantages in handling unstructured and semi-structured \\ndata, SQL databases excelled in complex query processing and ad-hoc analytics, underscoring the \\ncomplementary roles of both database paradigms in supporting diverse e-commerce use cases. \\nIn summary, the literature review provides a comprehensive overview of the SQL versus NoSQL \\ndebate in e -commerce database management, encompassing considerations of performance, \\nscalability, consistency, and suitability for real -time analytics. While NoSQL databases offer \\ninherent advantages in scalability and flexibility, they come with trade-offs in terms of consistency \\nand operational complexity. Conversely, SQL databases provide strong consistency guarantees \\nand robust query capabilities but may struggle to scale effectively in distributed, high-concurrency \\nenvironments. As e -commerce platforms continue to evolve and innovate, the selection of an \\nappropriate database technology remains a pivotal decision, necessitating a holistic understanding \\nof the technical nuances and strategic implications associated with SQL and NoSQL paradigms. \\nMethodology \\nResearch Design: This study adopts a comparative research design to evaluate the suitability of \\nSQL and NoSQL databases for e -commerce applications. The research design encompasses a \\nsystematic analysis of relevant literature, benchmarking studies, and real-world case examples to \\nelucidate the distinctive features, advantages, and limitations associated with each database \\nparadigm. \\nLiterature Review:  A comprehensive literature review was conducted to synthesiz e existing \\nresearch and scholarly discourse on SQL and NoSQL databases in the context of e -commerce. \\nThis entailed a thorough examination of peer-reviewed journals, conference proceedings, industry \\nreports, and academic textbooks to identify key findings, comparisons, trends, and empirical \\nstudies relevant to the research objectives.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n60 | P a g e  \\n \\nData Collection: Data collection encompassed the gathering of scholarly articles, research papers, \\nand industry reports from reputable sources such as academic databases (e.g., IEEE Xplore, ACM \\nDigital Library) and scholarly search engines (e.g., Google Scholar). Additionally, real-world case \\nstudies and benchmarking studies published by industry practitioners and database vendors were \\ncurated to provide empirical insights and validation for the research findings. \\nData Analysis: The collected data were subjected to rigorou s analysis and synthesis to extract \\nmeaningful insights and trends pertaining to the comparative evaluation of SQL and NoSQL \\ndatabases in e -commerce applications. This involved thematic analysis, content analysis, and \\nqualitative synthesis techniques to identify common themes, patterns, and discrepancies across the \\nliterature corpus. \\nComparison Framework: A structured comparison framework was developed to systematically \\nevaluate the performance, scalability, consistency, flexibility, and suitability for real-time analytics \\nof SQL and NoSQL databases in e -commerce contexts. This framework served as a guiding \\nframework for organizing and synthesizing the research findings into a coherent narrative and \\nfacilitating objective comparisons between the two database paradigms. \\nCase Study Analysis:  Real-world case studies and benchmarking studies were analyzed to \\ncomplement the theoretical insights derived from the literature review with empirical evidence and \\npractical implications. This involved scrutinizing the met hodologies, findings, and \\nrecommendations presented in case studies from leading e -commerce companies and database \\nvendors to glean actionable insights and best practices for database selection and architecture. \\nValidation and Reliability:  To enhance the v alidity and reliability of the research findings, \\nmultiple data sources were triangulated, and cross -referenced to ensure consistency and \\nrobustness. Moreover, peer -review feedback and expert validation were sought to verify the \\naccuracy and credibility of the research methodology, findings, and conclusions. \\nEthical Considerations: Throughout the research process, ethical considerations were upheld to \\nensure the responsible handling and use of copyrighted materials, proprietary data, and sensitive \\ninformation. Proper attribution and citation practices were adhered to, and permissions were \\nobtained where necessary to avoid plagiarism and copyright infringement.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n61 | P a g e  \\n \\nLimitations: It is important to acknowledge the limitations inherent in the research methodology, \\nincluding potential biases in the selection of literature and case studies, as well as the \\ngeneralizability of the findings to specific e-commerce contexts. Additionally, the dynamic nature \\nof database technology and e-commerce trends may introduce temporal limitations to the research \\nfindings, necessitating continuous monitoring and updating of the literature base. \\nConclusion: In conclusion, the research methodology adopted in this study provides a rigorous \\nand systematic approach to evaluating the suitabil ity of SQL and NoSQL databases for e -\\ncommerce applications. By integrating theoretical insights with empirical evidence and practical \\ncase examples, the methodology offers a comprehensive framework for informed decision-making \\nand strategic planning in database selection and architecture for e-commerce businesses. \\n \\nData Collection Methods:  The data collection process involved gathering scholarly articles, \\nresearch papers, and industry reports from reputable sources such as academic databases (e.g., \\nIEEE Xpl ore, ACM Digital Library) and scholarly search engines (e.g., Google Scholar). \\nAdditionally, real-world case studies and benchmarking studies published by industry practitioners \\nand database vendors were curated to provide empirical insights and validation  for the research \\nfindings. \\nFormulas Used in Analysis: \\n1. Performance Comparison: \\nPerformance metrics such as throughput (TP) and latency (LT) were calculated using the following \\nformulas: \\nThroughput (TP)=Total number of transactions processedTotal timeThroughput (TP)=Total time\\nTotal number of transactions processed \\nLatency (LT)=Total timeTotal number of transactions processedLatency (LT)=Total number of t\\nransactions processedTotal time \\n2. Scalability Assessment: \\nScalability was evaluated based on the scalability index (SI), calculated as:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n62 | P a g e  \\n \\nScalability Index (SI)=Throughput (TP2)Throughput (TP1)×100%Scalability Index (SI)=Throug\\nhput (TP1)Throughput (TP2)×100% \\nAnalysis Procedure: \\n1. Literature Review:  A comprehensive literature review was conducted to synth esize \\nexisting research and scholarly discourse on SQL and NoSQL databases in the context of \\ne-commerce. Key findings, comparisons, and trends were extracted from peer -reviewed \\njournals, conference proceedings, industry reports, and academic textbooks. \\n2. Data Synthesis: The collected data were subjected to thematic analysis, content analysis, \\nand qualitative synthesis techniques to identify common themes, patterns, and \\ndiscrepancies across the literature corpus. This involved organizing and categorizing the \\nresearch findings according to predefined comparison criteria and evaluation dimensions. \\n3. Performance Evaluation:  Performance metrics such as throughput and latency were \\ncalculated based on benchmarking studies and real -world case examples. Comparative \\nanalysis was conducted to assess the relative performance of SQL and NoSQL databases \\nin handling e-commerce workloads under varying conditions. \\n4. Scalability Analysis:  Scalability was evaluated using scalability index calculations, \\ncomparing the throughput of SQL and NoSQL databases under increasing workload levels. \\nThe scalability index provided insights into the ability of each database paradigm to scale \\nlinearly with growing transaction volumes. \\n5. Validation and Reliability:  To enhance the validity and reliabilit y of the research \\nfindings, multiple data sources were triangulated, and cross -referenced to ensure \\nconsistency and robustness. Peer -review feedback and expert validation were sought to \\nverify the accuracy and credibility of the analysis. \\nOriginal Work Published: \\nThe original work resulting from this analysis has been published in [Journal Name], [Year], by \\n[Author Name(s)]. The findings and insights presented in this study contribute to the body of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n63 | P a g e  \\n \\nknowledge on database management in e -commerce and offer p ractical guidance for database \\nselection and architecture in e-commerce businesses. \\nResults: \\nTo demonstrate the comparative performance of SQL and NoSQL databases in an e -commerce \\ncontext, we conducted a series of benchmarking experiments using synthetic workloads simulating \\ntypical e-commerce transaction scenarios. The results revealed notable differences in throughput \\nand latency between the two database paradigms under varying concurrency levels. \\nThroughput Analysis:  SQL databases exhibited robust throug hput performance in low to \\nmoderate concurrency scenarios, with transactional throughput averaging 1000 transactions per \\nsecond (TPS) under steady-state conditions. However, as concurrency levels increased beyond 100 \\nconcurrent users, SQL databases experie nced diminishing throughput gains, plateauing at \\napproximately 1200 TPS due to resource contention and locking overhead. \\nIn contrast, NoSQL databases demonstrated superior scalability and throughput efficiency, \\nachieving linear scalability up to 10,000 TPS under high concurrency conditions. The distributed \\narchitecture and partitioning strategies inherent in NoSQL databases enabled seamless scaling to \\naccommodate the influx of concurrent transactions, resulting in sustained high throughput levels \\neven at peak loads. \\nLatency Analysis:  Latency analysis revealed compelling performance advantages for NoSQL \\ndatabases compared to SQL counterparts, particularly under high concurrency conditions. SQL \\ndatabases exhibited increasing transaction latency as concurrency levels rose, peaking at 100 \\nmilliseconds (ms) for 100 concurrent users and escalating further to 200 ms for 500 concurrent \\nusers. \\nConversely, NoSQL databases maintained low and consistent latency levels across all concurrency \\nlevels, with transaction response times averaging below 50 ms even at peak loads. The distributed, \\nhorizontally scalable architecture of NoSQL databases facilitated efficient data partitioning and \\nparallel query processing, minimizing contention and latency overhead. \\nDiscussion:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n64 | P a g e  \\n \\nThe results of the benchmarking experiments underscore the significant performance disparities \\nbetween SQL and NoSQL databases in e -commerce applications, particularly in terms of \\nthroughput, scalability, and latency. While SQL databases offer robust transaction al capabilities \\nand strong consistency guarantees, they exhibit limitations in scalability and throughput efficiency \\nunder high concurrency scenarios. \\nConversely, NoSQL databases excel in scalability and throughput efficiency, leveraging \\ndistributed archit ectures and partitioning strategies to accommodate the dynamic workload \\npatterns and concurrency demands inherent in e -commerce platforms. The linear scalability and \\nlow-latency characteristics of NoSQL databases make them well -suited for handling the \\nunpredictability and variability of e -commerce transactions, enabling seamless scaling to meet \\ngrowing demands without sacrificing performance or user experience. \\nThe findings of this study have practical implications for e -commerce businesses seeking to \\noptimize their database architecture for performance and scalability. By leveraging NoSQL \\ndatabases, e-commerce platforms can enhance their ability to handle peak loads, support real-time \\ntransaction processing, and deliver responsive user experiences, ultimate ly driving customer \\nsatisfaction and business growth. \\nMoreover, the comparative analysis highlights the importance of aligning database technology \\nchoices with specific application requirements and performance objectives. While SQL databases \\nmay suffice fo r transactional workloads with predictable concurrency patterns and stringent \\nconsistency requirements, NoSQL databases offer a compelling alternative for e -commerce \\nplatforms prioritizing scalability, flexibility, and low -latency performance in dynamic, h igh-\\nconcurrency environments. \\nIn conclusion, the results and discussion presented in this study underscore the critical role of \\ndatabase technology in shaping the performance and scalability of e -commerce platforms. By \\nunderstanding the strengths and limit ations of SQL and NoSQL databases and their implications \\nfor transaction processing and user experience, e -commerce businesses can make informed \\ndecisions to optimize their database architecture and drive competitive advantage in the digital \\nmarketplace.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n65 | P a g e  \\n \\nThroughput Analysis: \\nTo quantify the transactional throughput of SQL and NoSQL databases under varying concurrency \\nlevels, we conducted benchmarking experiments using synthetic workloads. The results are \\nsummarized in Table 1 below: \\nTable 1: Transactional Throughput (Transactions per Second, TPS) \\nConcurrency Level SQL Database Throughput (TPS) NoSQL Database Throughput (TPS) \\n10 800 1000 \\n50 1000 2500 \\n100 1200 5000 \\n500 1300 10000 \\n \\n \\nLatency Analysis: \\nTransaction latency was measured as the time taken for a transaction to be processed and \\ncompleted by the database system. The results are presented in Table 2 below: \\nTable 2: Transactional Latency (Milliseconds, ms) \\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n10 20 10 \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n1 2 3 4\\nTransactional Throughput (Transactions per Second, TPS)\\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n66 | P a g e  \\n \\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n50 50 15 \\n100 100 20 \\n500 200 30 \\n \\n \\nFormulas Used: \\n1. Throughput (TPS): \\nThroughput (TPS)=Total number of transactionsTotal timeThroughput (TPS)=Total timeTotal n\\number of transactions \\n2. Latency (ms): \\nLatency (ms)=Total timeTotal number of transactionsLatency (ms)=Total number of transactions\\nTotal time \\nExcel File for Charts: \\nThe data provided in Tables 1 and 2 can be easily exported to an Excel file for chart creation. By \\nutilizing the values from these tables, you can generate visual representations such as line charts \\nor bar charts to depict the throughput and latency trends of SQL and NoSQL databases under \\nvarying concurrency levels. This will facilitate a more intuitive understanding of the performance \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n1 2 3 4\\nTransactional Latency (Milliseconds, ms)\\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n67 | P a g e  \\n \\ndifferences between the two database paradigms and enable stakeholders to make informed \\ndecisions regarding database selection and optimization for e-commerce applications. \\nConclusion \\nIn this study, we conducted benchmarking experiments to compare the performance of SQL and \\nNoSQL databases in e -commerce applications, focusing on transactional throughput and latency \\nunder varying concurrency levels. The results of our analysis highlight notable differences between \\nthe two database paradigms, with implications for scalability, performance, and user experience in \\ne-commerce platforms.  From the throughput analysis, it is evident that NoSQL databases \\ndemonstrate superior scalability and throughput efficiency compared to SQL counterparts. Under \\nincreasing concurrency levels, NoSQL databases exhibit linear scalability, accommodating higher \\ntransaction volumes with minimal degradation in throughput. This scalability advantage enables \\ne-commerce platforms to handle dynamic workload patterns and peak traffic lo ads without \\nsacrificing performance or user experience.  Similarly, the latency analysis reveals compelling \\nperformance advantages for NoSQL databases, particularly under high concurrency scenarios. \\nNoSQL databases consistently maintain low-latency response times, ensuring responsive and real-\\ntime transaction processing even at peak loads. In contrast, SQL databases experience escalating \\nlatency under high concurrency levels, potentially leading to user frustration and degraded \\nperformance. The findings of t his study have practical implications for e -commerce businesses \\nseeking to optimize their database architecture for scalability and performance. By leveraging \\nNoSQL databases, e-commerce platforms can enhance their ability to support growing transaction \\nvolumes, deliver responsive user experiences, and capitalize on peak demand periods without \\nexperiencing performance bottlenecks or downtime.  Moreover, the comparative analysis \\nunderscores the importance of aligning database technology choices with specific application \\nrequirements and performance objectives. While SQL databases may suffice for transactional \\nworkloads with predictable concurrency patterns and stringent consistency requirements, NoSQL \\ndatabases offer a compelling alternative for e -commerce pla tforms prioritizing scalability, \\nflexibility, and low -latency performance in dynamic, high -concurrency environments.  In \\nconclusion, the results of this study shed light on the critical role of database technology in shaping \\nthe performance and scalability of e -commerce platforms. By understanding the strengths and \\nlimitations of SQL and NoSQL databases and their implications for transaction processing and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n68 | P a g e  \\n \\nuser experience, e-commerce businesses can make informed decisions to optimize their database \\narchitecture and drive competitive advantage in the digital marketplace. \\nReferences: \\n1. Gadde, S. S., & Kalli, V. D. R. (2020). Descriptive analysis of machine learning and its \\napplication in healthcare. Int J Comp Sci Trends Technol, 8(2), 189-196. \\n2. Bommu, R. (2022). Advancements in Medical Device Software: A Comprehensive Review \\nof Emerging Technologies and Future Trends.  Journal of Engineering and \\nTechnology, 4(2), 1-8. \\n3. Gadde, S. S., & Kalli, V. D. (2021). The Resemblance of Library and Information Science \\nwith Medic al Science.  International Journal for Research in Applied Science & \\nEngineering Technology, 11(9), 323-327. \\n4. Gadde, S. S., & Kalli, V. D. R. (2020). Technology Engineering for Medical Devices -A \\nLean Manufacturing Plant Viewpoint. Technology, 9(4). \\n5. Bommu, R.  (2022). Advancements in Healthcare Information Technology: A \\nComprehensive Review. Innovative Computer Sciences Journal, 8(1), 1-7. \\n6. Gadde, S. S., & Kalli, V. D. R. (2020). Medical Device Qualification Use.  International \\nJournal of Advanced Research in Computer and Communication Engineering, 9(4), 50-55. \\n7. Bommu, R. (2022). Ethical Considerations in the Development and Deployment of AI -\\npowered Medical Device Software: Balancing Innovation with Patient Welfare. Journal of \\nInnovative Technologies, 5(1), 1-7. \\n8. Gadde, S. S., & Kalli, V. D. R. (2020). Artificial Intelligence To Detect Heart Rate \\nVariability. International Journal of Engineering Trends and Applications, 7(3), 6-10. \\n9. Brian, K., & Bommu, R. (2022). Revolutionizing Healthcare IT through AI and \\nMicrofluidics: From Drug Screening to Precision Livestock Farming. Unique Endeavor in \\nBusiness & Social Sciences, 1(1), 84-99. \\n10. Gadde, S. S., & Kalli, V. D. R. (2020). Applications of Artificial Intelligence in Medical \\nDevices and Healthcare.  International Journal  of Computer Science Trends and \\nTechnology, 8, 182-188.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16', 'file_type': 'pdf', 'file_name': '[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n69 | P a g e  \\n \\n11. Brandon, L., & Bommu, R. (2022). Smart Agriculture Meets Healthcare: Exploring AI -\\nDriven Solutions for Plant Pathogen Detection and Livestock Wellness \\nMonitoring. Unique Endeavor in Business & Social Sciences, 1(1), 100-115. \\n12. Gadde, S. S., & Kalli, V. D. (2021). Artificial Intelligence at Healthcare \\nIndustry. International Journal for Research in Applied Science & Engineering \\nTechnology (IJRASET), 9(2), 313. \\n13. Thunki, P., Reddy, S. R. B., Raparthi, M., Ma ruthi, S., Dodda, S. B., & Ravichandran, P. \\n(2021). Explainable AI in Data Science -Enhancing Model Interpretability and \\nTransparency. African Journal of Artificial Intelligence and Sustainable \\nDevelopment, 1(1), 1-8. \\n14. Gadde, S. S., & Kalli, V. D. An Innovative Study on Artificial Intelligence and Robotics. \\n15. Gadde, S. S., & Kalli, V. D. (2021). Artificial Intelligence and its Models.  International \\nJournal for Research in Applied Science & Engineering Technology, 9(11), 315-318. \\n16. Raparthi, M., Dodda, S. B., Redd y, S. R. B., Thunki, P., Maruthi, S., & Ravichandran, P. \\n(2021). Advancements in Natural Language Processing -A Comprehensive Review of AI \\nTechniques. Journal of Bioinformatics and Artificial Intelligence, 1(1), 1-10. \\n17. Gadde, S. S., & Kalli, V. D. R. A Qualitative Comparison of Techniques for Student \\nModelling in Intelligent Tutoring Systems. \\n18. Raparthi, M., Maruthi, S., Reddy, S. R. B., Thunki, P., Ravichandran, P., & Dodda, S. B. \\n(2022). Data Science in Healthcare Leveraging AI for Predictive Analytics a nd \\nPersonalized Patient Care. Journal of AI in Healthcare and Medicine, 2(2), 1-11. \\n19. Gadde, S. S., & Kalli, V. D. Artificial Intelligence, Smart Contract, and Islamic Finance. \\n20. Kalli, V. D. R. (2022). Human Factors Engineering in Medical Device Software Desi gn: \\nEnhancing Usability and Patient Safety. Innovative Engineering Sciences Journal, 8(1), 1-\\n7. \\n21. Kalli, V. D. R. (2022). Improving Healthcare Delivery through Innovative Information \\nTechnology Solutions. MZ Computing Journal, 3(1), 1-6.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='An Empirical Study on the Design and\\nEvolution of NoSQL Database Schemas\\nStefanie Scherzinger1 and Sebastian Sidortschuck2\\n1 OTH Regensburg, Regensburg, Germany\\nstefanie.scherzinger@oth-regensburg.de\\n2 OTH Regensburg, Regensburg, Germany\\n& SPARETECH.io, Stuttgart, Germany\\nsebastian.sidortschuck@sparetech.io\\nAbstract. We study how software engineers design and evolve their\\ndomain model when building applications against NoSQL data stores.\\nSpeciﬁcally, we target Java projects that use object-NoSQL mappers to\\ninterface with schema-free NoSQL data stores. Given the source code\\nof ten real-world database applications, we extract the implicit NoSQL\\ndatabase schema. We capture the sizes of the schemas, and investigate\\nwhether the schema is denormalized, as is recommended practice in data\\nmodeling for NoSQL data stores. Further, we analyze the entire project\\nhistory, and with it, the evolution history of the NoSQL database schema.\\nIn doing so, we conduct the so far largest empirical study on NoSQL\\nschema design and evolution.\\nKeywords: Schema Evolution · NoSQL Databases · Empirical Study.\\n1 Introduction\\nSchema-ﬂexible NoSQL data stores have become popular backends for building\\ndatabase applications. Systems like MongoDB allow for ﬂexible changes to the\\ndomain model during application development. In particular, they have proven\\nthemselves in settings where applications are frequently deployed to their pro-\\nduction environment, e.g., when web applications are built in an agile approach.\\nWhile the data stores do not enforce a global schema, the application code\\ngenerally assumes that persisted entities adhere to a certain (if loose) domain\\nmodel. Given that schema-ﬂexibility is one of the major selling points of NoSQL\\ndata stores, this raises the question how the domain model, and thereby the\\nimplied NoSQL database schema , actually evolves. We empirically study the\\ndynamics of NoSQL database schema evolution. Further, we investigate the\\nquestion whether the NoSQL database schema is denormalized, as commonly\\nrecommended in literature, e.g. [15].\\nUnfortunately, real-world data dumps of NoSQL data stores are hard to come\\nby. We therefore resort to analyzing the source code of applications hosted on\\nGitHub. We focus on the relevant software stack shown in Figure 1a, namely Java\\napplications that use an object-NoSQL mapper to store data in either Google\\narXiv:2003.00054v1  [cs.DB]  28 Feb 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='2 Scherzinger and Sidortschuck\\nCloud Datastore3 or MongoDB4, both popular and mature data stores. Among\\nover 1.2K open source GitHub repositories with this stack, we have identiﬁed\\nthe ten projects with the largest NoSQL schemas (a notion introduced shortly).\\nPrevious studies on schema evolution have primarily focused on schema-full,\\nrelational databases [5,11,13,18,20,25]. About NoSQL schema evolution in real-\\nworld applications, little is known that is based on systematic, empirical studies\\n(versus anecdotal evidence): Earlier studies have a diﬀerent focus (such as the\\nusage of certain mapper features [14]), or analyze a single project (c.f. [12]).\\nIn this paper, we introduce our notion of the NoSQL database schema,\\nwhich is implicit in object mapper class declarations, even though the under-\\nlying NoSQL data stores are schema-free. In this setting, this paper makes the\\nfollowing contributions:\\n– We formulate three research questions, namely (RQ1) whether the NoSQL\\ndatabase schema is denormalized (as recommended in literature), (RQ2)\\nwhich growth in complexity we can observe in NoSQL database schemas\\nover the project development time, and (RQ3) how the NoSQL database\\nschema evolves, thereby identifying the common changes.\\n– We analyze the ten projects with the largest NoSQL database schemas\\namong over 1.2K candidate projects, based on static code analysis and the\\ncommit history. We are able to conﬁrm that denormalization is common in\\nNoSQL database schemas. We are further able to show evidence of evolu-\\ntionary changes to the NoSQL database schema in all analyzed projects.\\n– We discuss our ﬁndings w.r.t. related studies on relational schema evolution\\nand ﬁnd that the churn rate of NoSQL schemas is comparatively high.\\nStructure. Next, we introduce preliminaries in Section 2. In Section 3, we describe\\nour methodology, and state our research questions. In Section 4, we present the\\nresults of our study, which we then discuss in Section 5. We point out threats to\\nthe validity of our results in Section 6, and give an overview over related work\\nin Section 7. We conclude with an outlook on future work.\\n2 Preliminaries\\nWe next introduce the software stack studied, as well as our terminology.\\nPhysical entities. We consider two popular NoSQL data stores: Google Cloud\\nDatastore (called Datastore hereafter) is commercial and hosted on the Google\\nCloud Platform, MongoDB is open source. Both data stores are schema-free\\n(however, MongoDB oﬀersoptional schema validation). Both manage document-\\nlike data, which we refer to as the (physical) entities. On an abstract level, an\\nentity is a collection of key-value pairs, or properties. Entities may be nested\\nand properties may be multi-valued. We sketch a Datastore entity representing\\na player and his or her missions in a role playing game in Figure 1a, in (simpliﬁed)\\nJSON notation, to abstract away from system-proprietary storage formats.\\n3 https://cloud.google.com/datastore/, available since 2009.\\n4 https://www.mongodb.com/, available since 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 3\\n(a) The software stack.\\n (b) Code changes to class Player.\\nFig. 1. (a) The object-NoSQL mapper separates the domain model from the NoSQL\\ndata store (adapted from [6]). (b) Not all code changes are actually schema-relevant.\\nDomain models. In principle, each entity in a schema-free data store may have\\nits very own, unique structure. However, in database applications, it is safe\\nto assume that the software engineers have agreed on some domain model , as\\nsketched in Figure 1a. In our setting, the domain model is captured by Java class\\ndeclarations, yet in the Figure, we use the more compact UML notation. (For\\nnow, we ignore the @-labeled annotations.) Class Player declares attributes for\\nan identiﬁer, a name, an amount of credits, and a list of missions. Each mission\\nalso has an identiﬁer, a title, a level of diﬃculty, and tracks its completion.\\nObject-NoSQL Mappers. Object mappers are state-of-the-art in building data-\\nbase applications [6]. Like object-relational mappers, the object-NoSQL map-\\npers Objectify 5 and Morphia6 map Java objects to entities. Objectify is tied to\\nDatastore, and Morphia to MongoDB. With object-NoSQL mappers, develop-\\ners merely specify their domain model as Java classes that are annotated with\\nthe keyword @Entity. Each entity-class has a unique key (annotated with @Id).\\nThe object mapper provides methods for saving and loading: In Figure 1a, the\\nclass name and the identifying attribute are mapped to the designated proper-\\nties _kind and _id. Objectify maps the player’s list of missions to an array of\\nnested entities. Yet at application runtime, an entity-class declaration may not\\nmatch the structure of all persisted entities, as discussed next.\\nLazy data migration. The data store may also store legacy versions of entities.\\nFigure 1b shows a new version of entity-class Player, with changes due to new\\nrequirements in the software development project. Attribute coins has replaced\\ncredits. Merely changing the entity-class in the application code does not af-\\nfect any existing entities. Instead, persisted entities are only migrated lazily,\\n5 https://github.com/objectify/objectify\\n6 https://github.com/MorphiaOrg/morphia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='4 Scherzinger and Sidortschuck\\nupon loading: The new version of entity-class Player in Figure 1b is backwards-\\ncompatible with Figure 1a. Once the legacy entity for Frodo has been loaded, the\\ncorresponding Java object will have an attributecoins, as annotation @AlsoLoad\\nlazily renames attributes.\\nThus, to obtain a summary of the structural variety of physical entities in\\nthe data store (based on code analysis alone, not having access to the data store\\ncontents itself), we need to consider the entire evolution history of entity-classes.\\nNoSQL database schema evolution. We base our notion of the NoSQL database\\nschema (or shorter, NoSQL schema) on the domain model. This idea of treating\\nentity-classes as schema declarations is re-current in literature, c.f. [4, 17]. Note\\nthat not all Java attributes are relevant for the NoSQL database schema: At-\\ntributes that are transient, e.g., carrying Objectify annotation @Ignore, are not\\nschema-relevant: The value of hoursSinceLastLogin in Figure 1b is not per-\\nsisted (it may be derived from lastLogin). Also, class methods are not schema-\\nrelevant. Thus, code changes that only aﬀect transient attributes or class meth-\\nods are part of software evolution, but not of schema evolution. Therefore, they\\nare not considered schema changes by us.\\nDenormalized entity-classes. The recommendation in working with Datastore\\nand MongoDB is to intentionally denormalize the schema. 7 This can be done\\nby either nesting entities, or by using multi-valued properties, such as the array\\nof Missions in Figure 5. There are various motivations for denormalization, one\\nbeing that traditionally, the query languages do not provide a join operator\\n(such is still the case in Google Cloud Datastore, and this also used to be the\\ncase for MongoDB), so joined data is materialized in the data store. Another\\nreason is that transactions between an arbitrary number of entities may not\\nbe supported 8. Consequently, transactionally safe updates are often realized by\\nupdates to a single, aggregate entity.\\nIn the following, we say an entity-class is denormalized if it does not declare\\nﬂat, relational-style tuples in ﬁrst normal form, i.e., with atomic attribute values\\nonly. So unless all schema-relevant attributes have Java primitive types (such as\\nInteger, String, Boolean, . . . ), we say the entity-class is denormalized. As we\\ndiscuss in Section 3.3, this is a practical yet conservative approach.\\nAs an example, the entity-class declarations for players, sketched in Figure 1,\\nis denormalized, due to the multi-valued attribute listOfMissions.\\n3 Methodology\\nIn the following, we describe our methodology, such as the context of our analysis,\\nthe research questions, and the analysis process. While our outline has strong\\nanalogies to Qiu et al. [13] and their analysis of relational schema evolution, our\\n7 E.g. “6 Rules of Thumb for MongoDB Schema Design” at https://www.mongodb.\\ncom/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-2, June 2015.\\n8 We point to the concepts such of entity groups and cross-group transactions in the\\nclassic Google Cloud Datastore [16], which is in the process of being deprecated.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 5\\nT able 1. Characteristics of the studied database applications.\\nProject Life Cycle # Commits # Entity-\\nclasses LoC (K)\\nObjectify\\nCryptonomica/cryptonomica 04/16 ∼ 09/18 185 0 ∼ 29 0 ∼ 526\\nFraunhoferCESE/madcap 12/14 ∼ 03/18 853 0 ∼ 82 0 ∼ 17\\ngoogle/nomulus 03/16 ∼ 09/18 2,025 51 ∼ 55 138 ∼ 224\\nnareshPokhriyal86/testing 01/15 ∼ 02/15 25 0 ∼ 79 0 ∼ 449\\nNekorp/Tikal-Technology 04/15 ∼ 11/15 59 0 ∼ 43 0 ∼ 49\\nMorphia\\naltiplanogao/tallyframework 06/15 ∼ 06/16 167 0 ∼ 24 0 ∼ 5\\nbujilvxing/QinShihuang 10/16 ∼ 12/16 154 0 ∼ 36 0 ∼ 21\\ncatedrasaes-umu/NoSQLDataEngineering 11/16 ∼ 09/18 711 0 ∼ 28 0 ∼ 280\\nGBPeters/PubInt 10/16 ∼ 02/18 69 0 ∼ 27 0 ∼ 5\\nMKLab-ITI/simmo 07/14 ∼ 02/17 142 0 ∼ 51 0 ∼ 5\\nprocess is rather diﬀerent: we cannot analyze schemas declared in a declarative\\ndata deﬁnition language, such as SQL. Rather, we need to parse raw Java code.\\n3.1 Context\\nWe used BigQuery9 to identify relevant open source repositories on GitHub, as\\nof September 4th, 2018. We consider a repository (which we synonymously refer\\nto as a project) relevant if it contains Java import statements for Objectify or\\nMorphia. We cloned over 1.2K candidate repositories and excluded any reposi-\\ntories that (1) have fewer than 20 commits (to exclude tinker projects), (2) are\\nthe Morphia or Objectify source code (or forks thereof), (3) or are ﬂagged as\\nforks from repositories already covered, with no schema-relevant code changes\\nafter the fork. We analyze the project history using git log 10. This allows us\\nto re-trace the development history of all entity classes. We parse and aggregate\\nthe log output using Python scripts.\\nAmong all projects analyzed, we determined the maximum number of entity-\\nclasses throughout the project history, and settled on the top-5 projects for\\nObjectify and Morphia respectively. Table 1 lists these projects with their life\\ncycles up to the latest commit at the time of our analysis. We also state the total\\nnumber of commits at the time. We state the minimum and maximum number\\nof entity-classes throughout the project history, as well as the total number of\\nlines of code between the ﬁrst and last analyzed commit (measured with cloc11\\nand reported in thousands).\\n9 Google BigQuery is a commercial cloud service. This data warehousing tool allows\\nfor querying the GitHub open data collection, mostly non-forked projects with an\\nopen source license: https://cloud.google.com/bigquery/.\\n10 We state the exact command pattern for reproducability: git log\\n--before=2018-09-04T00:00:00 --cherry-pick --date-order --pretty=format:\"%H;%aI;%cI;%P\" .\\n11 https://github.com/AlDanial/cloc'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='6 Scherzinger and Sidortschuck\\n3.2 Research Questions\\nRQ1: Are NoSQL schemas denormalized? We analyze the structure of entity-\\nclasses, whether they map to ﬂat tuples in ﬁrst normal form, or whether they\\nrepresent denormalized data.\\nRQ2: What is the growth in complexity of the NoSQL schema? We cap-\\nture schema complexity based on metrics recognized in literature.\\nRQ3: How does the NoSQL schema evolve? We automatically identify and\\nclassify evolutionary changes to the NoSQL schema.\\n3.3 Analysis Process\\nLocating entity-classes. We replay the commit histories and use the Java parser\\nQDox12 to parse class declarations. We identify entity-classes by the object map-\\nper annotation @Entity, which may also be inherited. 13\\nDenormalization. To determine whether an entity-class is denormalized, we parse\\nits Java declaration and strip away attributes that are not relevant to the NoSQL\\nschema. We then analyze the types of the remaining attributes. Unless all have\\nprimitive types (such as Integer, String, or Boolean), we assume that the\\nentity-class is denormalized.\\nIn most cases, we correctly recognize denormalization: (1) if the entity class\\ndeclaration contains container classes (e.g., a Java Collection), and therefore\\nan attribute is multi-valued. (2) Equally, the entity-class may contain nested\\nentity classes, giving it a hierarchical structure.\\nHowever, there are also cases where this approach is a conservative sim-\\npliﬁcation, and we might falsely categorize an entity-class as denormalized: an\\nattribute type may be declared in a third-party library, which is inaccessible\\nto us (see also our discussion in Section 6). Also, an attribute type may be a\\ncustom type that the developers declared. To realize that a custom type is just a\\nwrapper for a basic Java type, we would have to run more involved code analysis.\\nYet typically, polymorphic types are involved, and we are confronted with the\\ninherent limitations of static code analysis.\\nIdentifying schema changes. We identify commits with schema-relevant changes\\nby comparing succeeding versions of the application source code: We register\\nwhen (1) a new entity-class is added or an entity-class is removed, (2) a schema-\\nrelevant attribute is added or removed in an entity-class declaration, and (3) fur-\\nther, changes to schema-relevant attributes, such as to their types, default ini-\\ntializations, or even object mapper annotations. We only focus on changes which\\nwe can recognize programmatically. Recognizing renaming or splitting an entity-\\nclass, or renaming an attribute, are instances of the challenge of schema matching\\nand mapping [2], and cannot be fully automated.\\n12 https://github.com/paul-hammant/qdox\\n13 In earlier versions of the mapper libraries, this annotation was only optional, so it\\ncannot be relied upon. We therefore also search for the mandatory annotation @Id,\\nand thus reliably detect polymorphic entity-classes. (c.f. Section 6).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 7\\nCryptonomica/\\ncryptonomica\\nFraunhoferCESE/\\nmadcap\\ngoogle/\\nnomulus\\nnareshPokhriyal86/\\ntesting\\nNekorp/\\nTikal-Technology\\n(a) Objectify-based projects.\\naltiplanogao/\\ntallyframework\\nbujilvxing/\\nQinShihuang\\ncatedrasaes-umu/\\nNoSQLDataEngineering\\nGBPeters/\\nPubInt\\nMKLab-ITI/\\nsimmo\\nDenormalized entity-class\\nOther\\n(b) Morphia-based projects.\\nFig. 2. Visualization of denormalized NoSQL database schemas.\\n4 Results of the Study\\n4.1 RQ1: Are NoSQL schemas denormalized?\\nWe analyze the entity-class declarations in their most current version w.r.t. de-\\nnormalization. The results are visualized in Figure 2. For each analyzed project,\\nwe show a dot matrix chart. The number of dots represents the number of entity-\\nclasses. The brighter (orange) dots represent the entity-class declarations which\\nwe must assume to be denormalized, due to the limits of static code analysis.\\nThe darker (blue) dots represent the other entity-classes.\\nNotably, each project contains at least one denormalized entity-class, so all\\nschemas are denormalized. With the exception of two Objectify-based projects,\\ndenormalized entity-classes dominate the NoSQL database schemas. There are\\neven two Morphia-based projects where all entity-classes are denormalized.\\nResults. We ﬁnd that each project analyzed has denormalized entity-classes in its\\nNoSQL schema. This shows that developers make active use of denormalization.\\nHowever, without qualitative studies based on developer surveys, we do not\\nknow whether (1) the developers consciously chose a database which allows for\\na denormalized database schema, as this better suits their conceptual model.\\nHowever, it could also be that (2) they are actually forced denormalize their\\ndata model, due to the technological limitations of NoSQL data stores (brieﬂy\\ndiscussed in Section 2).\\n4.2 RQ2: What is the growth in complexity of the NoSQL schema?\\nIn empirical studies on relational schema evolution, the number of tables is\\nconsidered a simple approximation for schema complexity [7]. Accordingly, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='8 Scherzinger and Sidortschuck\\n0% 20% 40% 60% 80% 100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nCryptonomica/cryptonomica\\n0% 20% 40% 60% 80% 100%\\nFraunhoferCESE/madcap\\n0% 20% 40% 60% 80% 100%\\ngoogle/nomulus\\n0% 20% 40% 60% 80% 100%\\nnareshPokhriyal86/testing\\n0% 20% 40% 60% 80% 100%\\nNekorp/Tikal-Technology\\n# Entity-classes Schema-LoC\\n(a) Objectify-based projects.\\n0% 20% 40% 60% 80% 100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\naltiplanogao/tallyframework\\n0% 20% 40% 60% 80% 100%\\nbujilvxing/QinShihuang\\n0% 20% 40% 60% 80% 100%\\ncatedrasaes-umu/NoSQLD...\\n0% 20% 40% 60% 80% 100%\\nGBPeters/PubInt\\n0% 20% 40% 60% 80% 100%\\nMKLab-ITI/simmo\\n(b) Morphia-based projects.\\nFig. 3. Evolution trend of entity classes. The horizontal axes show the project progress,\\nin percentage of commits analyzed. The vertical axes show the complexity of the schema\\nw.r.t. its maximum, for two alternative metrics. (Visualization modeled after [13].)\\ntrack the number of entity-classes over time in Figure 3 (based on a visualization\\nidea from [13]). For each project, one chart is shown. On the horizontal axis, we\\ntrack the progress of the project, measured as the percentage of git commits\\nanalyzed. For the madcap project, this is based on 853 commits (c.f. Table 1).\\nOn the vertical axis, we track the size of the NoSQL database schema using two\\nmetrics. One is the number of entity classes (blue solid line). This metric is also\\nnormalized w.r.t. its maximum throughout the project history. So for madcap,\\nthe 100% peak corresponds to 82 entity classes, some of which were removed in\\nthe later phase of the project.\\nThe second line denotes a “proxy metric” [7] for approximating the size of the\\nNoSQL schema, where we count the lines of code of the entity-classes (including\\nsuperclasses, excluding comments and empty lines), and thereby compute the\\nSchema-LoC.14 There is shrinkage, yet overall, schema complexity increases.\\nResults. 1) As in the study by Qiu et al. on relational software evolution [13], we\\ncan conﬁrm that while the projects diﬀer in their life-spans and commit activity,\\nin nearly all projects, the NoSQL schema grows over time. However, there may be\\nphases of refactoring, leading to dips in the curves. 2) Apparently, Schema-LoC\\nlends itself nicely as a proxy-metric, and we obtain high correlation coeﬃcients\\n14 We ﬁnd this proxy-metric preferable over counting (schema-relevant) attributes, as\\nis common in studies on relational schema evolution: (1) Entity-classes with more\\nschema-relevant attributes have more lines of code accordingly. (2) In static code\\nanalysis, we cannot reliably count nested attributes: Abstract container classes and\\nthe use of polymorphism in general, make it impossible to know the number and na-\\nture of nested attributes at compile time. With Schema-LoC, we are able to abstract\\nfrom this issue.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 9\\nFig. 4. Visualizing relative schema sizes and churn. Each rectangle represents an entity-\\nclass, its area proportional to its size in lines of code (speciﬁcally Schema-LoC). The\\nhue represents the relative frequency of schema changes within the same project.\\nwhen comparing to the number of entity classes. As Schema-LoC depends on\\nthe number of attributes in an entity class, we can retrace an eﬀect reported\\nin [13], namely that entity-classes and their attributes (corresponding to tables\\nand columns) have largely analogous dynamics. 4) In general, the schema grows\\nmore than it shrinks. This is in line with studies on relational schema evolution.\\n5) One observation in [13] was that the schema stabilizes early: There, for 7 out\\nof 10 projects, 60% of the maximum number of tables is reached in the ﬁrst 20%\\nof the commits. Interestingly, in our study, the number of entity-classes reaches\\nthe 60% in only 4 projects. 6) In [13], less than 2% of all commits contain valid\\nschema changes (across all ten projects analyzed there). In our study, the share\\nof commits with schema-relevant changes is between 2.8% and over 30%, with 4\\nprojects reaching over 20%. Clearly, we observe higher churn rates.\\n4.3 RQ3: How does the NoSQL schema evolve?\\nWe ﬁrst investigate how often entity-classes undergo schema changes when com-\\npared to others inside the same project, and how large they are in terms of our\\nproxy metric Schema-LoC. Figure 4 visualizes the entity classes making up the\\nten NoSQL schemas as a tree map. This ﬁgure is best viewed in color. Each\\ncolored area represents one project. Inside, each rectangle represents one entity-\\nclass, the area proportional to its Schema-LoC. Darker hue indicates that an\\nentity-class has undergone more schema changes than the other entity-classes\\nin the same project. For instance, for nomulus, the darkest area represents 12\\nschema changes against the same entity-class. Thus, some entity-classes change\\nquite more often than others. However, there are also projects where schema\\nchanges aﬀect entity-classes quite uniformly.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='10 Scherzinger and Sidortschuck\\n0% 20% 40% 60% 80% 100%\\nNekorp/Tikal-Technology\\nnareshPokhriyal86/testing\\ngoogle/nomulus\\nFraunhoferCESE/madcap\\nCryptonomica/cryptonomica\\nObjectify\\nAdd entity-class\\nRemove entity-class\\nAdd schema-relevant attribute\\nRemove schema-relevant attribute\\nChange schema-relevant attribute\\n0% 20% 40% 60% 80% 100%\\nMKLab-ITI/simmo\\nGBPeters/PubInt\\ncatedrasaes-umu/NoSQLD...\\nbujilvxing/QinShihuang\\naltiplanogao/tallyframework\\nMorphia\\n(a) By project.\\nEntity-class Schema-relevant attribute\\nadd remove add remove change\\nObjectify 56.3% 8.4% 24.7% 4.0% 6.5%\\nMorphia 34.5% 16.2% 21.6% 10.4% 17.2%\\nOverall 34.8% 15.1% 27.3% 7.4% 15.4 %\\n(b) Objectify-based vs. Morphia-based projects.\\nProject Type Initialization Annotations\\nObjectify\\nCryptonomica/cryptonomica 2 0 3\\nFraunhoferCESE/madcap 9 0 6\\ngoogle/nomulus 11 2 58\\nnareshPokhriyal86/testing 0 0 0\\nNekorp/Tikal-Technology 0 0 3\\nMorphia\\naltiplanogao/tallyframework 7 3 54\\nbujilvxing/QinShihuang 32 33 15\\ncatedrasaes-umu/NoSQLDataEngineering 43 0 13\\nGBPeters/PubInt 0 2 2\\nMKLab-ITI/simmo 7 5 18\\n(c) Drill-down into the remaining changes to schema-relevant attributes.\\nFig. 5. Distinguishing diﬀerent kinds of schema changes: (a) and (b): Relative shares of\\nschema changes (by project and by mapper library). (c) Zooming in on the remaining\\nchanges in schema-relevant attributes mentioned in (a), showing absolute values.\\nIn Figure 5, we capture the distribution of schema changes according to\\nthe kind of change. In Subﬁgure 5a (after Qiu et al. in [13]), we break down\\nthe distribution of changes by project. Note that when a new entity-class is\\nadded, we do not count this as adding attributes at the same time. Notably, the\\ndistributions are project-speciﬁc. We now discuss two projects that stand out.\\nIn the fourth Objectify-based project, adding an entity-class makes up for\\nnearly all changes. Considering the project characteristics in Table 1 reveals that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 11\\nthis project is an outlier in several regards: With only 25 commits, it has barely\\nmade the bar for being considered in our analysis (see Section 3.1). At the same\\ntime, this project holds the second largest number of entity-classes in any project\\nconsidered in this analysis. Since the life cycle considered is only two months,\\nthis project is in a very early stage of development at the time of this analysis.\\nThus, it seems plausible that at this early phase, the developers kick start their\\ndata model by declaring the entity classes in bulk.\\nIn contrast, the second Morphia-based project stands out as the project with\\nthe least share of entity class additions. Since the git commit messages are in\\nChinese (which the authors of this paper do not master), we ﬁnd it diﬃcult to\\nretrace the developers’ motivation. What is noticeable in Subﬁgure 3b is that\\nwhile the number of entity classes increases in less than 10 distinct steps, the\\nproxy metric Schema-LoC changes in more ﬁne-granular steps. Thus, the entity-\\nclasses undergo more frequent changes. This matches the distribution plotted,\\nas then the share of entity-class creations is smaller by comparison. Subﬁgure 5b\\nsummarizes Subﬁgure 5a, and aggregates the changes by the mapper library.\\nWhile we see project-speciﬁc ﬂuctuations, when we group by mapper library, we\\nalso observe diﬀerences in the distribution. Overall, additions (whether of entity\\nclasses or of schema-relevant attributes) dominate.\\nIn Table 5c, we break down the schema-relevant attribute changes listed in\\nSubﬁgures 5a and 5b: (a) For some projects, types change. (b) For others, the\\ninitialization changes. A drill-down reveals that (as may be expected) adding\\nan initial value is the most frequent change, followed by changing the initializa-\\ntion value. (c) In other cases, mapper annotations that aﬀect the schema are\\nadded or removed. The most frequent annotations added are @PersistField\\nand @Reference. The ﬁrst is from a third-party framework. Since it is schema-\\nrelevant, we report it. The second supports referential constraints. Sporadically,\\nthird-party annotations are added to declare additional constraints, such as@Min.\\nResults. 1) We can conﬁrm the observations from related work on relational\\nschema evolution that schema changes are generally not distributed uniformly [13,\\n24]. 2) As already observed for RQ2, the trend is that entity-classes are added\\nmore frequently than they are removed. We see a similar pattern for schema-\\nrelevant attributes, in line with studies on relational schema evolution. Over-\\nall, in 9 out of 10 projects, additions collectively account for more than 50%\\nof the changes. In 5 projects, they even account for over 70% of the changes.\\n3) While additions are generally more frequent, there are also projects where\\nremovals of entity classes occur to a non-signiﬁcant degree. Related work on\\nrelational schema evolution has shown that there are what the authors call sur-\\nvivor tables [22], whereas there are that are more short-lived. The observation\\nthat entity-class removals are very project speciﬁc has also been made in [13].\\n4) Among all annotation changes, only 15 concern referential constraints (an-\\nnotation @Reference). The authors of two relate studies on relational schema\\nevolution, both [13] and [21], have observed that changes concerning referential\\nintegrity constraints are also rare in relational schema evolution. With NoSQL\\ndata stores, this is to be expected, as referential integrity is not supported to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='12 Scherzinger and Sidortschuck\\nthe same extent. 4) While Qiu et al. [13] found changes in attribute types to\\nbe the number one change for half of the projects analyzed (even outnumbering\\nadditions of either tables or columns), we do not see evidence of this eﬀect here.\\n5 Discussion\\nWe can reproduce the main results from related work on relational schema evo-\\nlution: There is strong evidence of NoSQL schema evolution, and additions are\\ndominant schema changes. However, we do not see the schema stabilizing in\\nthe early phases of all projects, which may partly be due to shorter project life\\nspans: The ten projects studied in [13] are PHP applications backed by rela-\\ntional databases, and have longer life cycles (two with ten years), more commits\\n(starting at nearly 5K), and more lines of code. This is to be expected with a\\nmuch older and thus more widely adopted stack.\\nStill, we do suspect that NoSQL developers evolve their schema more continu-\\nously. One indicator supporting this hypothesis is that we see higherchurn rates,\\nso a larger share of the commits contains code changes that aﬀect the schema.\\nThis calls for further study. Due to this churn, making sure that entity-class dec-\\nlarations are “backwards” compatible with legacy entities, persisted by earlier\\nversions of the application code, may become an overwhelming task. There are\\nﬁrst proposals for assisting tools, e.g., by type-checking versions of entity-class\\ndeclarations [3]. Clearly, more research is needed on systematic tool support.\\nThe fact that denormalization is common shows that solutions for managing\\nrelational schema evolution, managing ﬂat tuples, will not transfer immediately.\\nRather, when devising frameworks, we may want to turn to related work on\\nframeworks for handling schema evolution in XML (e.g. [9]) or object-oriented\\ndatabases (e.g. [26]) for inspiration on what has shown to be feasible.\\n6 Threats to Validity\\nConstruct validity. (1) With applications using older versions of Objectify and\\nMorphia, we cannot rely on the @Entity-annotation to identify entity-classes, so\\nwe also consider the @Id annotation. To be conﬁdent that this does not lead to\\nfalse positives, we performed manual checks. (With Objectify, we cross-checked\\nwhich entity-classes were registered with ObjectifyService, a mandatory pro-\\ngramming step.) (2) In static analysis, we encounter a limitation with attribute\\ntypes from third-party libraries. Tracking down these libraries is out of scope\\n(and not even possible in all cases). Thus, there are attributes that are not fully\\ncaptured by Schema-LoC. Yet as this is a proxy-metric to start with, we con-\\nsider this threat acceptable. Third-party libraries also aﬀect the recognition of\\nentity-classes as denormalized. Having sampled and inspected the entity-class\\ndeclarations, we are conﬁdent that – given the limitations of static code analysis\\n– the risk of false positives is acceptable. (3) We treat each single commit as\\ncontributing to a new version of the schema. There are software development\\nteams that operate by continuous deployment, so tested code is immediately'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 13\\nand autonomously deployed to the production environment. There, in theory,\\neach commit containing a schema change comprises a new schema version. Yet\\nrather often, a release to production comprises more than one commit. Unfor-\\ntunately, we are not able to tell in static code analysis which commits where\\nreleased when. There are development teams that tag release commits, but this\\nis project-internal culture, and not consistently the practice across all ten stud-\\nied projects. Therefore, we must go by the simplifying assumption that each\\ncommit declares a new NoSQL database schema.\\nExternal validity. We next discuss threats in generalizing our results to other\\nsoftware stacks. (1) It would be desirable to search additional code repositories,\\nand extend to further NoSQL data stores, object mapper libraries, and program-\\nming languages. (2) Extending our analysis to projects that do not use object-\\nmappers requires a diﬀerent kind of static code analysis, and was implemented\\nin a related study that involved a single MongoDB project [12]. At the same\\ntime, object mappers are state-of-the art in modern application development,\\nand by now, Objectify and Morphia are actually part of oﬃcial Datastore and\\nMongoDB tutorials (even though they started as independent projects). Thus,\\nwe do analyze a highly relevant stack. (3) There is the fundamental question\\nwhether studies on open source projects generalize to commercial projects.\\n7 Related Work\\nDatabase schema evolution is a timeless research area, with various proposals\\nhow to systematically manage schema changes. Providing tool support, however,\\nis not the scope of this paper. In the following discussion of related work, we\\ntherefore focus on empirical studies on schema evolution in open source projects.\\nIt is only natural that the availability of public code repositories has en-\\nabled empirical studies on relational schema evolution [5,11,13,18–20,22,23,25].\\nAmong their key ﬁndings, these studies show that the schema evolves. They\\nconﬁrm that adding tables or columns are frequent changes. In these settings,\\nthe schema is speciﬁed declaratively (usually in SQL). Accordingly, the term\\nschema modiﬁcation operations (SMOs) [5] does not transfer well to our stack.\\nRather than declarative DDL statements, we need to parse raw Java code: While\\nthe authors of [25] also parse application code, they do so to extract declarative\\nstatements embedded in code.\\nSo far, there are only few empirical studies on schema evolution in NoSQL\\ndata stores. Our work builds on an earlier analysis [14] on the adoption of map-\\nper annotations for lazy schema evolution, which is a diﬀerent focus. The au-\\nthors in [12] present an approach for identifying a schema evolution history in\\nMongoDB-based Java applications. Diﬀerent from us, the authors do not assume\\nthat an object-NoSQL mapper is used to access the data store. Rather, they an-\\nalyze direct calls to the MongoDB API. The schema derived is similar to our\\nnotion of the NoSQL schema, since it captures the perspective of the application\\ncode. The authors evaluated their approach for a single open source project,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='14 Scherzinger and Sidortschuck\\nwhereas our study has a broader basis, considering ten projects. Moreover, their\\ncontribution is to derive a visualization of the schema evolution history.\\nMeanwhile, there is a growing body of work on extracting schema descrip-\\ntions [1, 4, 8] from large collections of JSON data. While this a bottom-up ap-\\nproach, starting from the data, we proceed top-down, analyzing application code.\\nIn capturing schema complexity based on Java class declarations, we could\\nhave resorted to software metrics [10]. However, it is not clear how metrics\\nindicating an overly complex object-oriented design (e.g. classes with many at-\\ntributes) transfer. The practice of building aggregate models in NoSQL schema\\ndesign may actually be orthogonal.\\nWe refer to [7] for a high-level discussion on schema variety versus code\\nvariety, as well as metrics for programmatic schema analysis.\\n8 Conclusion and Outlook\\nIn this paper, we present the study on NoSQL schema evolution with the largest\\ndata basis so far, analyzing ten real-world, open source projects. We track the\\nschema growth as well as the nature of changes to the NoSQL schema. We are\\nable to reproduce most of the insights of related studies on relational schema\\nevolution, but we have also identiﬁed subtle diﬀerences.\\nSince this is a ﬁrst systematic study, many interesting questions remain unan-\\nswered. We remark on two. (1) Originally, we set out to compile detailed statis-\\ntics on the structure of denormalized entity-classes, such as their nesting depth.\\nHowever, we found that Java code written by experienced developers (e.g., as is\\nthe case with Google’s nomulus project) is highly polymorphic. This makes it\\nimpossible to compute reliable statistics based on the static analysis of entity-\\nclass declarations. However, more holistic analysis techniques, such as data ﬂow\\nanalysis of the entire application code, might reveal further insights. (2) We see\\nevidence that the schema evolves, but we do not know the factors that inﬂuence\\nNoSQL schema evolution. This calls for follow-up work, where we take the git\\ncommit messages into account, which often comment the reason for a schema\\nchange. What is also needed are qualitative studies, surveying developers who\\nroutinely deal with NoSQL schema evolution.\\nAcknowledgements This project was funded by the Deutsche Forschungsgemein-\\nschaft (DFG, German Research Foundation), grant number #385808805.\\nReferences\\n1. Baazizi, M., Colazzo, D., Ghelli, G., Sartiani, C.: Parametric schema inference for\\nmassive JSON datasets. VLDB J. 28(4), 497–521 (2019)\\n2. Bellahsene, Z., Bonifati, A., Rahm, E.: Schema Matching and Mapping. Springer\\nPublishing Company, Incorporated, 1st edn. (2011)\\n3. Cerqueus, T., Cunha de Almeida, E., Scherzinger, S.: Safely Managing Data Vari-\\nety in Big Data Software Development. In: Proc. BIGDSE’15 (2015)\\n4. Chilln, A.H., Ruiz, D.S., Molina, J.G., Morales, S.F.: A Model-Driven Approach\\nto Generate Schemas for Object-Document Mappers. IEEE Access 7, 59126–59142\\n(2019)\\n5. Curino, C.A., Tanca, L., Moon, H.J., Zaniolo, C.: Schema evolution in Wikipedia:\\nToward a Web Information System Benchmark. In: Proc. ICEIS’08 (2008)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf'}, page_content='Design and Evolution of NoSQL Database Schemas 15\\n6. Fowler, M.: Patterns of Enterprise Application Architecture. Addison-Wesley Long-\\nman Publishing Co., Inc., Boston, MA, USA (2002)\\n7. Jain, S., Moritz, D., Howe, B.: High variety cloud databases. In: Proc. ICDE Work-\\nshops 2016 (2016)\\n8. Klettke, M., St¨ orl, U., Scherzinger, S.: Schema Extraction and Structural Outlier\\nDetection for JSON-based NoSQL Data Stores. In: Proc. BTW’15 (2015)\\n9. Kl´ ımek, J., Mal´ y, J., Necask´ y, M., Holubov´ a, I.: eXolutio: Methodology for Design\\nand Evolution of XML Schemas Using Conceptual Modeling. Informatica, Lith.\\nAcad. Sci. 26(3), 453–472 (2015)\\n10. Lanza, M., Marinescu, R.: Object-Oriented Metrics in Practice: Using Software\\nMetrics to Characterize, Evaluate, and Improve the Design of Object-Oriented\\nSystems. Springer Publishing Company, Incorporated, 1st edn. (2010)\\n11. Lin, D.Y., Neamtiu, I.: Collateral Evolution of Applications and Databases. In:\\nProc. IWPSE-Evol’09 (2009)\\n12. Meurice, L., Cleve, A.: Supporting schema evolution in schema-less NoSQL data\\nstores. In: Proc. SANER’17 (2017)\\n13. Qiu, D., Li, B., Su, Z.: An Empirical Analysis of the Co-evolution of Schema and\\nCode in Database Applications. In: Proc. ESEC/FSE’13 (2013)\\n14. Ringlstetter, A., Scherzinger, S., Bissyand´ e, T.F.: Data Model Evolution Using\\nobject-NoSQL Mappers: Folklore or State-of-the-art? In: Proc. BIGDSE’16 (2016)\\n15. Sadalage, P.J., Fowler, M.: NoSQL Distilled: A Brief Guide to the Emerging World\\nof Polyglot Persistence. Addison-Wesley Professional, 1st edn. (2012)\\n16. Sanderson, D.: Programming Google App Engine with Java: Build & Run Scalable\\nJava Applications on Google’s Infrastructure. O’Reilly Media, Inc., 1st edn. (2015)\\n17. Scherzinger, S., Cerqueus, T., Cunha de Almeida, E.: ControVol: A framework for\\ncontrolled schema evolution in NoSQL application development. In: ICDE 2015.\\npp. 1464–1467 (2015)\\n18. Sjøberg, D.: Quantifying schema evolution. Information & Software Technology\\n35(1), 35–44 (1993)\\n19. Skoulis, I., Vassiliadis, P., Zarras, A.V.: Open-Source Databases: Within, Outside,\\nor Beyond Lehman’s Laws of Software Evolution? In: Proc. CAiSE 2014. pp. 379–\\n393 (2014)\\n20. Skoulis, I., Vassiliadis, P., Zarras, A.V.: Growing Up with Stability. Inf. Syst.\\n53(C), 363–385 (Oct 2015)\\n21. Vassiliadis, P., Kolozoﬀ, M., Zerva, M., Zarras, A.V.: Schema evolution and foreign\\nkeys: a study on usage, heartbeat of change and relationship of foreign keys to table\\nactivity. Computing 101(10), 1431–1456 (2019)\\n22. Vassiliadis, P., Zarras, A.V.: Survival in Schema Evolution: Putting the Lives of\\nSurvivor and Dead Tables in Counterpoint. In: Proc. CAiSE 2017. pp. 333–347\\n(2017)\\n23. Vassiliadis, P., Zarras, A.V., Skoulis, I.: How is Life for a Table in an Evolving\\nRelational Schema? Birth, Death and Everything in Between. In: Proc. ER 2015.\\npp. 453–466 (2015)\\n24. Vassiliadis, P., Zarras, A.V., Skoulis, I.: Gravitating to rigidity: Patterns of schema\\nevolution - and its absence - in the lives of tables. Inf. Syst. 63, 24–46 (2017)\\n25. Wu, S., Neamtiu, I.: Schema Evolution Analysis for Embedded Databases. In: Proc.\\nICDEW’11 (2011)\\n26. Xue Li: A survey of schema evolution in object-oriented databases. In: Proceedings\\nTechnology of Object-Oriented Languages and Systems. pp. 362–371 (1999)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Author’s Accepted Manuscript\\nData Modeling in the NoSQL World\\nPaolo Atzeni, Francesca Bugiotti, Luca Cabibbo,\\nRiccardo Torlone\\nPII:\\nS0920-5489(16)30118-0\\nDOI:\\nhttp://dx.doi.org/10.1016/j.csi.2016.10.003\\nReference:\\nCSI3149\\nTo appear in:\\nComputer Standards & Interfaces\\nReceived date:\\n25 March 2016\\nRevised date:\\n30 September 2016\\nAccepted date:\\n6 October 2016\\nCite this article as: Paolo Atzeni, Francesca Bugiotti, Luca Cabibbo and Riccardo\\nTorlone, Data Modeling in the NoSQL World, \\nComputer Standards &\\nInterfaces, \\nhttp://dx.doi.org/10.1016/j.csi.2016.10.003\\nThis is a PDF file of an unedited manuscript that has been accepted for\\npublication. As a service to our customers we are providing this early version of\\nthe manuscript. The manuscript will undergo copyediting, typesetting, and\\nreview of the resulting galley proof before it is published in its final citable form.\\nPlease note that during the production process errors may be discovered which\\ncould affect the content, and all legal disclaimers that apply to the journal pertain.\\nwww.elsevier.com'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Data Modeling in the NoSQL World ✩\\nPaolo Atzenia, Francesca Bugiottib,L u c aC a b i b b oa, Riccardo Torlonea\\naUniversit`aR o m aT r e\\nbCentraleSup´elec\\nAbstract\\nNoSQL systems have gained their popularity for many reasons, including the\\nﬂexibility they provide in organizing data, as they relax the rigidity provided by\\nthe relational model and by the other structured models. This ﬂexibility and\\nthe heterogeneity that has emerged in the area have led to a little use of tradi-\\ntional modeling techniques, as opposed to what has happened with databases\\nfor decades.\\nIn this paper, we argue how traditional notions related to data modeling\\ncan be useful in this context as well. Speciﬁcally, we propose NoAM (NoSQL\\nAbstract Model), a novel abstract data model for NoSQL databases, which ex-\\nploits the commonalities of various NoSQLsystems. We also proposea database\\ndesign methodology for NoSQL systems based on NoAM, with initial activities\\nthat are independent of the speciﬁc target system. NoAM is used to specify a\\nsystem-independent representation of the application data and, then, this inter-\\nmediate representation can be implemented in target NoSQL databases, taking\\ninto account their speciﬁc features. Overall, the methodology aims at support-\\ning scalability, performance, and consistency, as needed by next-generation web\\napplications.\\nKeywords: Data models, database design, NoSQL systems\\n✩This paper extends a short article appeared in the Proceedings of the 33rd International\\nConference on Conceptual Modeling (ER 2014) with the title Database Design for NoSQL\\nSystems [1].\\nPreprint submitted to Journal of L ATEX Templates October 15, 2016'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='1. Introduction\\nNoSQL database systems are today an eﬀective solution to manage large\\ndata sets distributed over many servers. A primary driver of interest in No-\\nSQL systems is their support for next-generation Web applications, for which\\nrelational DBMSs are not well suited. These are simple OLTP applications for5\\nwhich (i) data have a structure that does not ﬁt well in the rigid structure of\\nrelational tables, (ii) access to data is based on simple read-write operations,\\n(iii) relevant quality requirements include scalability and performance, as well\\nas a certain level of consistency [2, 3].\\nNoSQL technology is characterized bya high heterogeneity; indeed, more\\n10\\nthan ﬁfty NoSQL systems exist [4], each with diﬀerent characteristics. They can\\nbe classiﬁed into a few main categories [2], including key-value stores, document\\nstores, and extensible record stores. In any case, this heterogeneity is highly\\nproblematic to application developers [4], even within each category.\\nBeside the diﬀerences between the various systems, NoSQL datastores ex-\\n15\\nhibit an additional phenomenon: they usually support signiﬁcant ﬂexibility in\\ndata, with limited (if any) use of the notion of schema as it is common in\\ndatabases. So, the organization of data, and their regularity, is mainly hard-\\ncoded within individual applications and is not exposed, probably because there\\nis little need for sharing data between applications. Indeed, the notion of20\\nschema, and the need for a separation between data and programs, were moti-\\nvated in databases by the need for sharing data between applications. If this\\nrequirement does not hold any longer, many developers are led to believe that\\nthe importance of schemas gets reduced or even disappears.\\nAs the idea of data model is usually tightly related to that of schema, this\\n25\\n“schemaless” point view may lead to claim that the very notion of model and of\\nmodeling activities becomes irrelevant with respect to NoSQL databases. The\\ngoal of this paper is to argue that models and modeling do have an interesting\\nrole in this area. Indeed, modeling is an abstraction process, and this helps in\\ngeneral and probably even more in a world of diversity, as the analyst/designer30\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='can reason at a high level, before delving into the details of the speciﬁc systems.\\nInstead, given the variety of systems, it is currently the case that the design\\nprocess for NoSQL applications is mainly based on best practices and guide-\\nlines [5], which are speciﬁcally related to the selected system [6, 7, 8], with no\\nsystematic methodology. Several authors have observed that the development35\\nof high-level methodologies and tools supporting NoSQL database design are\\nneeded [9, 10, 11], and models here are deﬁnitely needed, in order to achieve\\nsome level of generality.\\nLet us recall the various reasons for which modeling is considered important\\nin database design and development [12]. First of all, beside being crucial\\n40\\nin the conceptual and logical design phases, it oﬀers support throughout the\\nlifecycle, from requirement analysis, where it helps in giving a structure to the\\nprocess,tocodingandmaintenance, wher eit givesvaluabledocumentation. The\\nmain point to be mentioned is that modeling allows the specialist to describe\\nthe domain of interest and the application from various perspectives and at45\\nvarious levels of abstraction. Moreover, it provides support to communication\\n(and to individual comprehension). Finally, it provides support to performance\\nmanagement, as physical database design is also based on data structures, and\\nquery processing eﬃciency is often basedon reference to the regularity of data.\\nConceptual and logical modeling, as they are currently known, were devel-\\n50\\noped in the database world, with speciﬁc attention to relational systems, but\\nfound applications also in other contexts. Indeed, while the importance of rela-\\ntional databases was clear since the Eighties, it was soon understood that there\\nwere many “non-business” application domains for which other modeling fea-\\ntures were needed: the advocates of object-oriented databases observed, more\\n55\\nor less at the same time, that some requirements were not satisﬁed, such as\\nthose in CAD, CASE, and multimedia and text management [13]. This led\\nto the development of models with nested structures, more complex than the\\nrelational one, and less regular, and so more diﬃcult to manage.\\nFlexibility in structures was also required in another area, which emerged a60\\ndecade later, and has since been very important: the area of Web applications,\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='where there were at least two kinds of developments concerned with models. On\\nthe one hand, work on complex object models for representing hypertexts [14,\\n15, 16], and on the other hand signiﬁcant development in semistructured data,\\nespecially with reference to XML [17].65\\nAnother recurring claim in the database world in the last ten or ﬁfteen\\nyears has been the fact that, while relational databases are ade factostandard,\\nit is not the case that there is one solution that works well for all kinds of\\napplications. As Stonebraker and C¸etintemel [18] argued, it is not the case that\\n“one size ﬁts all,” and diﬀerent engines and technologies are needed in diﬀerent\\n70\\ncontexts, for example OLAP and OLTP have diﬀerent requirements, but the\\nsame holds for other kinds of applications, such as stream processing, sensor\\nnetworks, or scientiﬁc databases.\\nThe NoSQL movement emerged for a number of motivations, including most\\nof the above, with the goal of supporting highly scalable systems, with speciﬁc\\n75\\nrequirements, usually with very simple operations over many nodes, on sets of\\ndata that have ﬂexible structure. Given that there are many diﬀerent appli-\\ncations and the speciﬁc requirementsvary, many systems have emerged, each\\noﬀeringadiﬀerentwayoforganizingdata andadiﬀerentprogramminginterface.\\nHeterogeneity can become a problem if migration or integration are needed, as\\n80\\nthis is often the case, in a world with changing requirements and new tech-\\nnological developments. Also, the availability of many diﬀerent systems, with\\ndiﬀerent implementations, has led to diﬀerent design techniques, usually related\\njust to individual systems or small families thereof.\\nIn this paper we argue that a model-based approach can be useful to tackle85\\nthe diﬃculties related to heterogeneity, and provide support in the form of\\nabstraction. In fact, modeling can be at the basis of a design process, at various\\nlevel; at a higher one to represent the features of interest for the application,\\nand at a lower one to describe some implementation features in a concrete but\\nsystem-independent way.\\n90\\nIndeed, we will present a high-level data model for NoSQL databases, called\\nNoAM (NoSQL Abstract Model) and show how it can be used as an interme-\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='diate data representation in the context of a general design methodology for\\nNoSQL applications having initial steps that are independent of the individual\\ntarget system. We propose a design process that includes a conceptual phase,95\\nas common in traditional application, followed (and this is unconventional and\\noriginal) by a system-independent logical design phase, where the intermediate\\nrepresentation is used, as the basis for both modeling and performance aspects,\\nwith only a ﬁnal phase that takes into account the speciﬁc features of individual\\nsystems.\\n100\\nThe rest of the paper is organized as follows. In Section 2, we illustrate\\nthe features of the main categories of NoSQL systems arguing that, for each\\nof them, there exists a sort of data model. In Section 3 we present NoAM,\\nour system-independent data model for NoSQL databases, and in Section 4 we\\ndiscuss our design methodology for NoSQL databases. In Section 5 we brieﬂy\\n105\\nreview some related literature. Finally, in Section 6 we draw some conclusions.\\n2. NoSQL data models\\nIn this section we brieﬂy present and compare a number of representative\\nNoSQL systems, to make apparent the heterogeneity (as well as the similarities)\\nin the way they organize data and in their programming interfaces. We ﬁrst110\\nintroduce a sample application dataset, and then we show how to represent\\nthese data in the representative systems we consider.\\n2.1. Running example\\nLet us consider, as a running example, an application for an on-line social\\ngame. This is indeed a typical scenario in which the use of a NoSQL database\\n115\\nis suitable, that is, a simple next-generation Web application (as discussed in\\nthe Introduction).\\nThe application should manage various types of objects, including players,\\ngames, and rounds. A few representative objects are shown in Figure 1. The\\nﬁgure is a UML object diagram. Boxes and arrows denote objects and relation-120\\nships between them, respectively.\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='mary : Player\\nusername = \"mary\"\\nfirstName = \"Mary\"\\nlastName = \"Wilson \"\\nrick : Player\\nusername = \"rick\"\\nfirstName = \"Ricky\"\\nlastName = \"Doe\"\\nscore = 42\\n2345 : Game\\nid = 2345\\nfirstPlayer secondPlayer\\n: GameInfo\\ngames [0]\\ngameopponent\\n: GameInfo\\ngames [0]\\ngame opponent\\n: Round : Round\\nrounds [0] rounds [1]\\n: Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 1: Sample application objects\\nmary : Player\\nusername = \"mary\"\\nfirstName = \"Mary\"\\nlastName = \"Wilson\"\\nrick : Player\\nusername = \"rick\"\\nfirstName = \"Ricky\"\\nlastName = \"Doe\"\\nscore = 42\\n2345 : Game\\nid = 2345\\nfirstPlayer secondPlayer\\n: GameInfo\\ngames [0]\\ngameopponent\\n: GameInfo\\ngames [0]\\ngame opponent\\n: Round : Round\\nrounds[0] rounds [1]\\n: Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 2: Sample aggregates (as groups of objects)\\nTo represent a dataset in a NoSQL database, it is often useful to arrange\\ndata in aggregates [19, 20]. Each aggregate is a group of related application\\nobjects, representing a unit of data access and atomic manipulation. In our\\nexample, relevant aggregates are players and games, as shown by closed curves125\\nin Figure 2. Note that the rounds of a game are grouped within the game itself.\\nIn general, aggregatescan be considered as complex-value objects [21], as shown\\nin Figure 3.\\nThedataaccessoperationsneededbyouro n-linesocialgamearesimpleread-\\nwrite operations on individual aggregates; for example, create a new player and130\\nretrieve a certain game. Other operations involve just a portion of an aggregate;\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 7, 'page_label': '8', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Player:mary : ⟨\\nusername : ”mary”,\\nﬁrstName : ”Mary”,\\nlastName : ”Wilson”,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:rick ⟩,\\n⟨ game : Game:2611, opponent : Player:ann ⟩\\n}\\n⟩\\nPlayer:rick : ⟨\\nusername : ”rick”,\\nﬁrstName : ”Ricky”,\\nlastName : ”Doe”,\\nscore : 42,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:mary ⟩,\\n⟨ game : Game:7425, opponent : Player:ann ⟩,\\n⟨ game : Game:1241, opponent : Player:johnny ⟩\\n}\\n⟩\\nGame:2345 : ⟨\\nid : ”2345”,\\nﬁrstPlayer : Player:mary,\\ns\\necondPlayer : Player:rick,\\nrounds : {\\n⟨ moves : ... , comments : ... ⟩,\\n⟨ moves : ... , actions : ..., spell : ... ⟩\\n}\\n⟩\\nFigure 3: Sample aggregates (as complex values)\\nfor example, add a round to an existing game. In general, it is indeed the\\ncase that most real applications require only operations that access individual\\naggregates [2, 22].\\n2.2. NoSQL database models135\\nNoSQL database systems organize their data according to quite diﬀerent\\ndata models. They usually provide simple read-write data-access operations,\\nwhich also diﬀer from system to system. Despite this heterogeneity, a few main\\ncategories can be identiﬁed according to the modeling features of these sys-\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='tems [2, 3]: key-value stores, document stores, extensible record stores, plus140\\nothers (e.g., graph databases) that are beyond the scope of this paper.\\n2.3. Key-value stores\\nIn general, in akey-value store, a database is a schemaless collection of key-\\nvalue pairs, with data access operations on either individual key-value pairs or\\ngroups of related pairs.145\\nAs a representative key-value store we consider hereOracle NoSQL[23]. In\\nthis system,keys are structured; they are composed of amajor keyand aminor\\nkey. The major key is a non-empty sequence of strings. The minor key is a\\nsequence of strings. Eachelement of a key is called acomponent of the key. On\\nthe other hand, eachvalue is an uninterpreted binary string.150\\nA sample key-value is the pair composed of key/Player/mary/-/username\\nand value ”mary”.I n t h e k e y , s y m b o l ‘/’ separates key components, while\\nsymbol ‘-’ separates the major key from the minor key. The distinction between\\nmajor key and minor is especially relevant to control data distribution and\\nsharding.155\\nIn a pair, the value can be either a simple value (such as the string”mary”)\\nor a complex value. In the former case, it is common to use some data inter-\\nchange format (such as XML, JSON, and Protocol Buﬀers [24]) to represent\\nsuch complex values.\\nOracle NoSQL oﬀers simple atomic access operations, to access and modify\\n160\\nindividual key-value pairs: put(key,value) to add or modify a key value pair\\nand get(key) to retrieve a value, given the key. Oracle NoSQL also provides\\nan atomic multiGet(majorKey) operation to access a group of related key-value\\npairs, and speciﬁcally the pairs having the same major key. Moreover, it oﬀers\\nan execute operation for executing multiple put operations in an atomic and\\n165\\neﬃcient way (provided that the keys speciﬁed in these operations all share a\\nsame major key).\\nThe data representation for a dataset in a key-value store can be based on\\naggregates. These are two common representations for aggregates:\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='•Represent an aggregate using a single key-value pair. The key (major key)170\\nis the aggregateidentiﬁer. The value is the complex value ofthe aggregate.\\nSee Figure 4(a).\\n•Represent an aggregate using multiple key-value pairs. Speciﬁcally, the\\naggregate is split in parts that need to be accessed or modiﬁed separately,\\nand each part is represented by a distinct but related key-value pair. The175\\naggregateidentiﬁer is usedasmajorkeyfor allthese parts, while the minor\\nkey identiﬁes the part within the aggregate. See Figure 4(b).\\nThe data access operations provided by key-value stores usually enable an ef-\\nﬁcient and atomic data access to aggregates with respect to both data repre-\\nsentations. Indeed, all systems support the access to individual key-value pairs\\n180\\n(useful in the former case) and most of them (such as Oracle NoSQL) provide\\nalso the access to groups of related key-value pairs (required inthe latter case).\\n2.4. Document stores\\nIn adocument store, a database is a set of documents, each having a complex\\nstructure and value.\\n185\\nIn this category, a widely used system isMongoDB [25]. It is an open-source,\\ndocument-oriented data store that oﬀers a full-index support on any attribute,\\na rich document-based query API and Map-Reduce support.\\nIn MongoDB, adatabase comprises one or more collections. Eachcollection\\nis a named group of documents. Eachdocument is a structured document, that\\n190\\nis, a complex value, a set of attribute-value pairs, which can comprise simple\\nvalues, lists, and even nested documents. Thus, documents are neither freeform\\ntext documents nor Oﬃce documents. Documents are schemaless, that is, each\\ndocument can have its own attributes, deﬁned at runtime.\\nSpeciﬁcally, MongoDB documents are based on BSON (Binary JSON), a\\n195\\nvariant of the popular JSON format. Values constituting documents can be of\\nthe following types: (i) basic types, such strings numbers, dates, and boolean\\nvalues; (ii) arrays, i.e., ordered sequences of values; and (iii) documents (or\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='key (/major/key/-) value\\n/Player/mary/- { username: ”mary”, ﬁrstName: ”Mary”, ... }\\n/Player/rick/- { username: ”rick”, ﬁrstName: ”Ricky”, ... }\\n/Game/2345/- { id: ”2345”, ﬁrstPlayer: ”Player:mary”, ... }\\n(a) Single key-value pair per aggregate\\nkey (/major/key/-/minor/key) value\\nPlayer/mary/-/username ”mary”\\nPlayer/mary/-/ﬁrstName ”Mary”\\nPlayer/mary/-/lastName ”Wilson”\\nPlayer/mary/-/games[0] {game: ”Game:2345”, opponent: ”Player:rick”}\\nPlayer/mary/-/games[1] {game: ”Game:2611”, opponent: ”Player:ann”}\\nPlayer/rick/-/username ”rick”\\nPlayer/rick/-/ﬁrstName ”Ricky”\\nPlayer/rick/-/lastName ”Doe”\\nPlayer/rick/-/score 42\\nPlayer/rick/-/games[0] {game: ”Game:2345”, opponent: ”Player:mary”}\\nPlayer/rick/-/games[1] {game: ”Game:7425”, opponent: ”Player:ann”}\\nPlayer/rick/-/games[2] {game: ”Game:1241”, opponent: ”Player:johnny”}\\nGame/2345/-/id 2345\\nGame/2345/-/ﬁrstPlayer ”Player:mary”\\nGame/2345/-/secondPlayer ”Player:rick”\\nGame/2345/-/rounds[0] {moves : ..., comments: ...}\\nGame/2345/-/rounds[1] {moves : ..., actions: ..., spell: ...}\\n(b) Multiple key-value pairs per aggregate\\nFigure 4: Representing aggregates in Oracle NoSQL\\nobjects): a document is a collection of zero or more key-value pairs, where each\\nkey is a plain string, while each value is of any of these types. Figure 5 shows a200\\nJSON representation of the complex value of a samplePlayer aggregate object\\nof Figures 2 and 3.\\nA main document is a top-level document with a unique identiﬁer, repre-\\nsented by a special attribute\\nid, associated to a value of a special typeObjectId.\\nData access operations are usually over individual documents, which are205\\nunits of data distribution and atomic data manipulation. The basic operations\\noﬀered by MongoDB are as follows:insert(coll,doc) adds a main documentdoc\\ninto collectioncoll;a n dﬁnd(coll,selector) retrieves from collectioncoll all main\\ndocuments matching document selector. The simplest selector is the empty\\ndocument {}, which matches with every document; it allows to retrieve all210\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 11, 'page_label': '12', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='[\\n\"username\" : \"mary\",\\n\"firstName\" : \"Mary\",\\n\"lastName\" : \"Wilson\",\\n\"games\" : {\\n[ \"id\" : \"Game:2345\", \"opponent\" : \"Player:rick\" ],\\n[ \"id\" : \"Game:2611\", \"opponent\" : \"Player:ann\"]\\n}\\n]\\nFigure 5: The JSON representation of the complex value of a sample Player object\\ncol lection\\n document id\\n document\\nPlayer\\n mary\\n {\"\\n id\":\"mary\", \"username\":\"mary\", \"firstName\":\"Mary\", ... }\\nPlayer\\n rick\\n {\"\\n id\":\"rick\", \"username\":\"rick\", \"firstName\":\"Rock\", ... }\\nGame\\n 2345\\n {\"\\n id\":\"2345\", \"firstPlayer\":\"Player:mary\", ... }\\nFigure 6: Representing aggregates in MongoDB\\ndocuments in a collection. Another useful selector is document{\\n id:ID},w h i c h\\nmatches with the document having identiﬁerID. There is also an operation to\\nupdate a document. Moreover, it is also possible to access or update just a\\nspeciﬁc portion of a document.\\nIn a document store, each aggregate is usually represented by a single main215\\ndocument. The document collection corresponds to the aggregate class (or\\ntype). The document identiﬁer ID is the aggregate identiﬁer. The content of\\nthe document is the complex-value of the aggregate, in JSON/BSON, including\\nalso an additional key-value pair{\\n id:ID} for the identiﬁer. See Figure 6.\\nAlso in this case, the data access operations oﬀered by document stores220\\n(such as MongoDB) provide an atomic and eﬃcient data access to aggregates.\\nSpeciﬁcally, they generally support both operations on individual aggregates, or\\nto speciﬁc portions of them, thereof.\\n2.5. Extensible record stores\\nIn an extensible record store, a database is a set of tables, each table is a225\\nset of rows, and each row contains a set of attributes (or columns), each with a\\nname and a value. Rows in a table are not required to have the same attributes.\\n11'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Data access operations are usually overindividual rows, which are units of data\\ndistribution and atomic data manipulation.\\nA representative extensible record store isAmazon DynamoDB [26], a No-230\\nSQL database service provided on the cloud by Amazon Web Services (AWS).\\nIn DynamoDB a database is organized in tables. Atable is a set of items. Each\\nitem contains one or moreattributes,e a c hw i t haname and a value (or a set\\nof values). Each table designates an attribute asprimary key. Items in a same\\ntable are not required to have the same set of attributes — apart from the pri-235\\nmary key, which is the only mandatory attribute of a table. Thus, DynamoDB\\ndatabases are mostly schemaless.\\nSpeciﬁcally, the primary key is composed of apartition keyand an optional\\nsort key. If the primary key of a table includes a sort key, then DynamoDB\\nstores together all the items having the same partition key, in such a way that240\\nthey can be accessed in an eﬃcient way.\\nDistribution is operated at the item level and, for each table, is controlled\\nby the partition key only.\\nSome operations oﬀered by DynamoDB are as follows:putItem(table,key,av)\\nadds (or modiﬁes) a new item in tabletable with primary keykey,u s i n gt h e245\\nset of attribute-value pairsav;a n dgetItem(table,key) retrieves the item of table\\ntable having primary keykey. It is also possible to access or update just a subset\\nof the attributes of an item. All these operations can be executed in an eﬃcient\\nway.\\nIn an extensible record store (such as DynamoDB), each aggregate can be250\\nrepresented by a record/row/item. The table corresponds to the aggregate class\\n(or type). The primary key (partition key) is the aggregate identiﬁer. Then,\\nthe item can have a distinct attribute-value pair for each top-level attribute of\\nthe complex value of the aggregate(or for each major part of the aggregatethat\\nneeds to be accessed separately). See Figure 7.\\n255\\nAgain, the data access operations provided by the systems in this category\\nsupport an eﬃcient data access to aggregates or to speciﬁc portions of them.\\n12'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='table Player\\nusername ﬁrstName lastName score games[0] games[1] games[2]\\n”mary” ”Mary” ”Wilson” { game: ..., opponent: ... }{ ... }\\n”rick” ”Ricky” ”Doe” 42 { game: ..., opponent: ... }{ ... }{ ... }\\ntable Game\\nid ﬁrstPlayer secondPlayer rounds[0] rounds[1] rounds[2]\\n2345 Player:mary Player:rick { moves : ..., comments: ... }{ ... }\\nFigure 7: Representing aggregates in DynamoDB (abridged)\\n2.6. Comparison\\nTo summarize, it is possible to say that each NoSQL system provides a num-\\nber of “modeling elements” to organize data, which can be considered the “data260\\nmodel” of the system. Moreover, the various systems can be eﬀectively classiﬁed\\nin a few main categories, where each category is based on “data models” that,\\neven though not identical, do share some similarities. In the next section we\\nshow that it is possible to pursue these similarities, thus deﬁning an “abstract\\ndata model” for NoSQL databases.265\\n3. The NoAM data model\\nIn this section we presentNoAM (NoSQL Abstract Data Model), a system-\\nindependent data model for NoSQL databases. In the following section we will\\nalso discuss how this data model can be used to support the design of NoSQL\\ndatabases.270\\nIntuitively, the NoAM data model exploits the commonalities of the data\\nmodeling elements available in the various NoSQL systems and introduces ab-\\nstractions to balance their diﬀerences and variations.\\nA ﬁrst observation is that all NoSQL systems have a data modeling element\\nthat is a data access and distribution unit. By “data access unit” we mean\\n275\\nthat the system oﬀers operations to access and manipulate an individual unit\\nat a time, in an atomic, eﬃcient, and scalable way. By “distribution unit” we\\nmean that each unit is entirely stored in aserver of the cluster, whereas diﬀer-\\nent units are distributed among the various servers. With reference to major\\n13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='NoSQL categories, this element is: (i) a group of related key-value pairs, in key-280\\nvalue stores; (ii) a document, in document stores; or (iii) a record/row/item, in\\nextensible record stores.\\nIn NoAM, a data access and distribution unit is modeled by ablock. Specif-\\nically, a block represents amaximal data unit for which atomic, eﬃcient, and\\nscalable access operations are provided. Indeed, while the access to an individ-285\\nual block can be performed in an eﬃcient way in the various systems, the access\\nto multiple blocks can be quite ineﬃcient. In particular, NoSQL systems do\\nnot usually provide an eﬃcient “join” operation. Moreover, most NoSQL sys-\\ntems provide atomic operations only over single blocks and do not support the\\natomic manipulation of a group of blocks. For example, MongoDB [25] provides\\n290\\nonly atomic operations over individual documents, whereas Bigtable does not\\nsupport transactions across rows [22].\\nA second common feature of NoSQL systems is the ability to access and\\nmanipulate just a component of a data access unit (i.e., of a block). This\\ncomponent is: (i) an individual key-value pair, in key-value stores; (ii) a ﬁeld,\\n295\\nin document stores; or (iii) a column, in extensible record stores. In NoAM,\\nsuch a smaller data access unit is called anentry.\\nFinally, most NoSQL databases providea notion of collection of data access\\nunits. For example, a table in extensiblerecord stores or a document collection\\nin document stores. In NoAM, a coll ection of data access units is called a300\\ncollection.\\nAccording to the above observations, the NoAM data model is deﬁned as\\nfollows.\\n•AN o A Mdatabase is a set ofcollections. Each collection has a distinct\\nname.305\\n•A collection is a set ofblocks. Each block in a collection is identiﬁed by a\\nblock key, which is unique within that collection.\\n•A block is a non-empty set ofentries. Each entry is a pair⟨ek,ev⟩,w h e r e\\nek is theentry key (which is unique within its block) andev is its value\\n14'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 15, 'page_label': '16', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Player\\nmary\\nusername\\n ”mary”\\nﬁrstName\\n ”Mary”\\nlastName\\n ”Wilson”\\ngames[0]\\n ⟨ game : Game:2345, opponent : Player:rick ⟩\\ngames[1]\\n ⟨ game : Game:2611, opponent : Player:ann ⟩\\nrick\\nusername\\n ”rick”\\nﬁrstName\\n ”Ricky”\\nlastName\\n ”Doe”\\nscore\\n 42\\ngames[0]\\n ⟨ game : Game:2345, opponent : Player:mary ⟩\\ngames[1]\\n ⟨ game : Game:7425, opponent : Player:ann ⟩\\ngames[2]\\n ⟨ game : Game:1241, opponent : Player:johnny ⟩\\nGame\\n2345\\nid\\n 2345\\nﬁrstPlayer\\n Player:mary\\nsecondPlayer\\n Player:rick\\nrounds[0]\\n ⟨ moves : ..., comments : ... ⟩\\nrounds[1]\\n ⟨ moves : ..., actions : ..., spell : ... ⟩\\nFigure 8: A sample database in NoAM\\n(either complex or scalar), called theentry value.310\\nFor example, Figure 8 shows a possible representation of the aggregates\\nof Figures 2 and 3 in terms of the NoAM data model. There, outer boxes\\ndenote blocks representing aggregates, while inner boxes show entries. Note\\nthat entry values can be complex, being this another commonality of various\\nNoSQL systems.315\\nPlease note that the same data can be usually represented in diﬀerent ways.\\nCompare, for example, Figure 8 with Figure 9. We will discuss this possibility\\nin the next section.\\nIn summary, NoAM describes in a uniform way the features of many NoSQL\\nsystems, and so can be eﬀectively used,as we show in the next section, for an\\n320\\n15'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 16, 'page_label': '17', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Player\\nmary\\n ϵ\\n⟨username:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:rick ⟩,\\n⟨ game : Game:2611, opponent : Player:ann ⟩\\n}⟩\\nrick\\n ϵ\\n⟨username:”rick”,\\nﬁrstName:”Ricky”,\\nlastName:”Doe”,\\nscore:42,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:mary ⟩,\\n⟨ game : Game:7425, opponent : Player:ann ⟩,\\n⟨ game : Game:1241, opponent : Player:johnny ⟩\\n}⟩\\nGame\\n2345\\n ϵ\\n⟨id : ”2345”,\\nﬁrstPlayer : Player:mary,\\nsecondPlayer : Player:rick,\\nrounds : {\\n⟨ moves :..., comments : ... ⟩,\\n⟨ moves :..., actions : ..., spell : ... ⟩\\n}⟩\\nFigure 9: Another NoAM sample database\\nintermediate representation in a NoSQL database design methodology.\\n4. System-independent design of NoSQL databases with NoAM\\nThe main goal of NoAM is to support a design methodology for NoSQL\\ndatabases that has initial activities that are independent of the speciﬁc tar-\\nget system. In particular, NoAM is used to specify an intermediate, system-325\\nindependent representation of the application data. The implementation in a\\ntarget NoSQL system is then a ﬁnal step, with a translation that takes into\\naccount its peculiarities.\\n16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='The motivations to consider database design for NoSQL systems are as fol-\\nlows. It is important to notice that despite the fact that NoSQL databases330\\nare claimed to be “schemaless,” the data of interest for applications do show\\nsome structure, which should be mapped to the modeling elements (collections,\\ntables, documents, key-value pairs) available in the target system. Moreover,\\ndiﬀerent alternatives in the organization of data in a NoSQL database are usu-\\nally possible, but they are not equivalent in supporting qualities such as perfor-335\\nmance, scalability, and consistency (which are typically required when a NoSQL\\ndatabase is adopted). For example, a “wrong” database representation can lead\\nto performance that are worse by an order of magnitude as well as to the in-\\nability to guarantee atomicity of important operations.\\nSpeciﬁcally, our design methodology has the goal of designing a “good” rep-\\n340\\nresentation of the application data in a target NoSQL database, and is intended\\nto support major qualities such as performance, scalability, and consistency, as\\nneeded by next-generation Web applications.\\nThe NoAM approach is based on the following main activities:\\n•conceptual data modeling and aggregate design, to identify the various345\\nentities and relationships thereof needed in an application, and to group\\nrelated entities into aggregates;\\n•aggregate partitioning and high-level NoSQL database design, where ag-\\ngregates are partitioned into smaller data elements and then mapped to\\nthe NoAM intermediate data model;350\\n•implementation, to map the intermediate data representation to the spe-\\nciﬁc modeling elements of a target datastore.\\nIn this approach, only the implementation depends on the target datastore.\\nWe will discuss the various steps of this approach in the rest of this section.\\n4.1. Conceptual modeling and aggregate design355\\nThe methodology starts, as it is usual in database design, by building a con-\\nceptual representation of the data of interest, in terms of entities, relationships,\\n17'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='and attributes. (This activity is discussed in most database textbooks, e.g.,\\n[12].) Following Domain-Driven Design (DDD [19]), which is a widely followed\\nobject-oriented methodology, we assume that the outcome of this activity is a360\\nconceptual UML class diagram deﬁning the entities, value objects, and relation-\\nships of the application. Anentity is a persistent object that has independent\\nexistence and is distinguished by a unique identiﬁer (e.g., a player or a game,\\nin our running example). Avalue objectis a persistent object which is mainly\\ncharacterized by its value, without an own identiﬁer (e.g., a round or a move).\\n365\\nThen, the methodology proceeds by identifying aggregates.\\nThe design of aggregates has the goal of identifying the classes of aggregates\\nfor an application, and various approaches are possible. After the preliminary\\nconceptual design phase, entities and value objects are grouped into aggregates.\\nEach aggregate has an entity as its root, and it can also contain many value370\\nobjects. Intuitively, an entity and a group of value objects are used to deﬁne an\\naggregate having a complex structure and value.\\nThe relevant decisions in aggregate design involve the choice of aggregates\\nand of their boundaries. This activity can be driven by the data access pat-\\nterns of the application operations, as well as by scalability and consistency\\n375\\nneeds [19]. Speciﬁcally, aggregates should be designed as the units on which\\natomicity must be guaranteed [20] (with eventual consistency for update op-\\nerations spanning multiple aggregates [27]). In general, it is indeed the case\\nthat most real applications require only operations that access individual aggre-\\ngates [2, 22]. Eachaggregateshould be large enough so as to include all the data380\\nrequired by a relevant data access operation. (Please note that NoSQL systems\\ndo not provide a “join” operation, and this is a main motivation for clustering\\neach group of related application objects into an aggregate.) Furthermore, to\\nsupport strong consistency (that is, atomicity) of update operations, each ag-\\ngregate should include all the data involved by some integrity constraints or\\n385\\nother forms of business rules [28]. On the other hand, aggregates should be as\\nsmall as possible; smallaggregates reduce concurrency collisions and support\\nperformance and scalability requirements [28].\\n18'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Thus, aggregate design is mainly driven by data access operations. In our\\nrunning example, the online game application needs to manage various collec-390\\ntions of objects, including players, games, and rounds. Figure 2 shows a few\\nrepresentativeapplication objects. (There, boxes and arrowsdenote objects and\\nlinks between them, respectively. An object having a colored top compartment\\nis an entity, otherwise it is a value object.) When a player connects to the\\napplication, all data on the player should be retrieved, including an overview395\\nof the games she is currently playing.Then, the player can select to continue\\na game, and data on the selected game should be retrieved. When a player\\ncompletes a round in a game she is playing, then the game should be updated.\\nThese operations suggest that the candidate aggregate classes are players and\\ngames. Figure 2 also shows how application objects can be grouped in aggre-400\\ngates. (There, a closed curve denotes the boundary of an aggregate.)\\nAs we mentioned above,aggregatedesign is alsodriven by consistency needs.\\nAssume that the application should enforce a rule specifying that a round can\\nbe added to a game only if some condition that involves the other rounds of the\\ngame is satisﬁed. An individual round cannot check, alone, the above condition;\\n405\\ntherefore, it cannot be an aggregate by itself. On the other hand, the above\\nbusiness rule can be supported by a game (comprising, as an aggregate, its\\nrounds).\\nIn conclusion, the aggregate classes for our sample application arePlayer\\nand Game, as shown in Figures 2 and 3.\\n410\\n4.2. Data representation in NoAM and aggregate partitioning\\nIn our approach, we use the NoAM data model (Section 3) as an intermedi-\\nate model between application aggregates (Section 4.1) and NoSQL databases\\n(Section 2). We represent each class of aggregates by means of a distinct col-\\nlection, and each individual aggregate by means of a block. We use the class415\\nname to name the collection, and the identiﬁer of the aggregate as block key.\\nThe complex value of each aggregate is represented by a set of entries in the\\ncorresponding block. For example, the aggregates of Figures 2 and 3 can be\\n19'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='represented by the NoAM database shown in Figure 8. The representation of\\naggregates as blocks is motivated by the fact that both concepts represent a420\\nunit of data access and distribution, but at diﬀerent abstraction levels. Indeed,\\nNoSQL systems provide eﬃcient, scalable, and consistent (i.e., atomic) opera-\\ntions on blocks and, in turn, this choice propagates such qualities to operations\\non aggregates.\\nIn general, an application dataset of aggregates can be represented in NoAM425\\ndatabase in several diﬀerent ways. Eachdata representationfor a datasetδis a\\nNoAMdatabase Dδrepresentingδ. Speciﬁcally, thevariousdatarepresentations\\nfor a dataset diﬀer only in the choice of the entries used to represent the complex\\nvalue of each aggregate. We ﬁrst discuss basic data representation strategies,\\nwhich we illustrate with respect to the example described in Figure 3. We then\\n430\\nintroduce additional and more ﬂexible data representations.\\nA simple data representation strategy, called Entry per Aggregate Object\\n(EAO), represents each individual aggregate using a single entry. The entry\\nkey is empty. The entry value is the whole complex value of the aggregate. The\\ndata representation of the aggregates of Figure 3 according to the EAO strategy435\\nis shown in Figure 9.\\nAnotherdatarepresentationstrategy,called Entry per Top-level Field(ETF),\\nrepresents each aggregate by means of multiple entries, using a distinct entry\\nfor each top-level ﬁeld of the complex value of the aggregate. For each top-level\\nﬁeld f of an aggregateo, it employs an entry having as value the value of ﬁeldf440\\nin the complex value ofo (with values that can be complex themselves), and as\\nkey the ﬁeld namef. Figure 10 shows the data representation of the aggregates\\nof Figure 3 according to the ETF strategy.\\nAs a comparison, we can observe that the EAO data representation uses a\\nblock with a single entry to represent thePlayerobject having usernamemary,445\\nwhile the ETF representation needs a block with four entries, corresponding to\\nﬁelds username, ﬁrstName, lastName,a n dgames. Moreover, blocks in EAO\\ndo not depend on the structure of aggregates, while blocks in ETF depend on\\nthe top-level structure of aggregates (which can be “almost ﬁxed” within each\\n20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 21, 'page_label': '22', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Player\\nmary\\nusername\\n ”mary”\\nﬁrstName\\n ”Mary”\\nlastName\\n ”Wilson”\\ngames\\n{⟨ game: Game:2345, opponent: Player:rick ⟩,\\n⟨ game: Game:2611, opponent: Player:ann ⟩}\\nrick\\nusername\\n ”rick”\\nﬁrstName\\n ”Ricky”\\nlastName\\n ”Doe”\\nscore\\n 42\\ngames\\n{⟨ game: Game:2345, opponent: Player:mary ⟩,\\n⟨ game: Game:7425, opponent: Player:ann ⟩,\\n⟨ game: Game:1241, opponent: Player:johnny ⟩}\\nGame\\n2345\\nid\\n 2345\\nﬁrstPlayer\\n Player:mary\\nsecondPlayer\\n Player:rick\\nrounds\\n{⟨ moves: ..., comments: ..., ⟩\\n⟨ moves: ..., actions: ..., spell: ... ⟩}\\nFigure 10: The ETF data representation\\nclass).450\\nThe general data representation strategies we just described can be suited in\\nsome cases, but they are often too rigid and limiting. For example, none of the\\nabove strategies leads to the data representation shown in Figure 8. The main\\nlimitation of such generaldata representations is that they refer only to the\\nstructure of aggregates, and do not take into account the data access patterns\\n455\\nof the application operations. Therefore, these strategies are not usually able to\\nsupport the performance of these operations. This motivates the introduction\\nof aggregate partitioning.\\nWe ﬁrst need to introduce a preliminary notion ofaccess path, to specify a\\n“location”in the structure ofa complexvalue. Intuitively, ifv is acomplex value460\\nand w is a value (possibly complex as well) occurring inv, then the access path\\n21'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='ap for w in v represents the sequence of “steps” that should be taken to reach\\nthe component valuew in v. More precisely, an access pathap is a (possibly\\nempty) sequence ofaccess steps, ap = p1 p2 ...p n, where each steppi identiﬁes\\nac o m p o n e n tv a l u ei nas t r u c tured value. Furthermore, ifv is a complex value465\\nand ap is an access path, thenap(v) denotes the component value identiﬁed by\\nap in v.\\nFor example, consider the complex valuevmary of the Player aggregate\\nhaving username mary shown in Figure 3. Examples of access paths for this\\ncomplex value areﬁrstNameand games[0].opponent. If we apply these access470\\npaths tovmary, we access valuesMary and Player:rick, respectively.\\nA complex valuev can be represented using a set of entries, whose keys are\\naccess paths forv. Each entry is intended to represent a distinct portion of the\\ncomplex valuev, characterized by a location in its structure (the access path,\\nused as entry key) and a value (the entry value). Speciﬁcally, in NoAM we475\\nrepresent each aggregate by means of apartition of its complex valuev,t h a ti s ,\\nas e tE of entries that fully coverv, without redundancy. Consider again the\\ncomplex valuevmary shown in Figure 3; a possible entry forvmary is the pair\\n⟨games[0].opponent, Player:rick⟩. We have already applied the above intuition\\nearlier in this section. For example, the ETF data representation (shown in480\\nFigure 10) uses ﬁeld names as entry keys (which are indeed a case of access\\np a t h s )a n dﬁ e l dv a l u e sa se n t r yv a l u e s .\\nAggregate partitioning can be based on the following guidelines (which are a\\nvariant of guidelines proposed in [12] in the context of logical database design):\\n•If an aggregate is small in size, or all or most of its data are accessed or485\\nmodiﬁed together, then it should be represented by a single entry.\\n•Conversely, an aggregate should be partitioned in multiple entries if it is\\nlarge in size and there are operationsthat frequently access or modify only\\nspeciﬁc portions of the aggregate.\\n•T w oo rm o r ed a t ae l e m e n t ss h o u l db e l o n gt ot h es a m ee n t r yi ft h e ya r e490\\nfrequently accessed or modiﬁed together.\\n22'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='Game\\n2345\\nϵ\\n⟨ id:2345,\\nﬁrstPlayer:Player:mary,\\nsecondPlayer:Player:rick ⟩\\nrounds[0]\\n ⟨ moves :. . . , comments :. . . ⟩\\nrounds[1]\\n ⟨ moves :. . . , actions :. . . , spell :. . . ⟩\\nFigure 11: An alternative data representation for games ( Rounds)\\n•Two or more data elements should belong to distinct entries if they are\\nusually accessed or modiﬁed separately.\\nThe applicationof the above guidelines suggests a partitioning of aggregates,\\nwhich we will use to guide the representation in the target database.495\\nFor example, in our sample application, consider the operations involving\\ngames and rounds. When a player selects to continue a game, data on the\\nselected game should be retrieved. When a player completes a round in a game\\nshe is playing, then the aggregate for the game should be updated. To support\\nperformance, it is desirable that this update is implemented in the database\\n500\\njust as an addition of a round to a game, rather than a complete rewrite of the\\nwhole game. Thus, data for each individual round is always read or written\\ntogether. Moreover, data for the various rounds of a game are read together,\\nbut each round is written separately. Therefore, each roundis a candidate to\\nbe represented by an autonomous entry. These observations lead to a data\\n505\\nrepresentation for games shown in Figure 8. However, apart from rounds, the\\nremaining data for each game comprises just a few ﬁelds, which can be therefore\\nrepresented together in a single entry. This further observation leads to an\\nalternative data representation for games, shown in Figure 11.\\n4.3. Implementation\\n510\\nWe now discuss how a NoAM data representation can be implemented in\\na target NoSQL database. Given that NoAM generalizes the features of the\\nvarious NoSQL systems, while keeping their major aspects, it is rather straight-\\nforward to perform this activity. We have implementations for various NoSQL\\n23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='systems, including Cassandra, Couchbase, Amazon DynamoDB, HBase, Mon-515\\ngoDB, Oracle NoSQL, and Redis. For the sake of space, we discuss the im-\\nplementation only with respect to a single representative system for each main\\nNoSQL category. Moreover, with reference to the same aggregate objects of\\nFigures 2 and 3 we will sometimes show only the data for one aggregate. Sim-\\nilar representations can be obtained for the other aggregates of the running\\n520\\nexample.\\n4.3.1. Key-value store: Oracle NoSQL\\nIn the key-value store Oracle NoSQL [23] (Section 2.3), a data representa-\\ntion D for an application dataset can be implemented as follows. We use a\\nkey-value pair for each entry⟨ek,ev⟩ in D. The major key is composed of the525\\ncollection name C and the block keyid, while the minor key is a proper cod-\\ning of the entry keyek (recall that ek is an access path, which we represent\\nusing a distinct key component for each of its steps). An example of key is\\n/Player/mary/-/ﬁrstName,w h e r es y m b o l/ separates components, and symbol\\n- separates the major key from the minor key. The value associated with this530\\nkey is a representation of the entry valueev; for example,Mary.T h ev a l u ec a n\\nbe either simple or a serialization of a complex value, e.g., in JSON.\\nThe retrieval of a block can be implemented, in an eﬃcient and atomic way,\\nusing a singlemultiGet operation — this is possible because all the entries of a\\nblock share the same major key. The storage of a block can be implemented535\\nusing variousput operations. These multipleput operations can be executed in\\nan atomic way — since, again, all the entries of a block share the same major\\nkey.\\nFor example, Figure 4(b) shows the implementation in Oracle NoSQL of the\\ndata representation of Figure 8. Moreover, Figure 4(a) shows the implementa-\\n540\\ntion in Oracle NoSQL of the EAO data representation of Figure 9.\\nAnimplementationcanbeconsidered eﬀectiveifaggregatesareindeedturned\\ninto units of data access and distribution.The eﬀectiveness of our implementa-\\ntion is based on the use we make of Oracle NoSQL keys, where the major key\\n24'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='controls distribution (sharding is based on it) and consistency (an operation in-545\\nvolving multiple key-value pairs can be executed atomically only if the various\\npairs are over a same major key).\\nMore precisely, a technical precaution is needed toguarantee atomic con-\\nsistency when the selected data representation uses more than one entry per\\nblock. Consider two separate operations that need to update just a subset of550\\nthe entries of the block for an aggregateobject. Since aggregatesshould be units\\nof atomicity and consistency, if these operations are requested concurrently on\\nthe same aggregate object, then the application would require that the NoSQL\\nsystem identiﬁes a concurrency collision, commits only one of the operations,\\nand aborts the other. However, if the operations update twodisjoint subsets\\n555\\nof entries, then Oracle NoSQL is unable to identify the collision, since it has\\nno notion of block. We support this requirement, thus providing atomicity and\\nconsistency over aggregates, by always including in each update operation the\\naccess to the entry that includes the identiﬁer of the aggregate (or some other\\ndistinguished entry of the block).\\n560\\n4.3.2. Extensible record store: DynamoDB\\nIn the extensible record store Amazon DynamoDB ([26], Section 2.5), the\\nimplementation of a NoAM database can be based on a distinct table for each\\ncollection, and a single item for each block. The item contains a number of\\nattributes, which can be deﬁned from the entries of the block for the item.565\\nA NoAM datarepresentationD can be representedin DynamoDB as follows.\\nConsider a blockb in a collectionC having block keyid. According toD,o n e\\nor multiple entries are used within each block. We use all the entries of a block\\nb to create a new item in a table forb. Speciﬁcally, we proceed as follows: (i)\\nthe collection nameC is used as a DynamoBD table name; (ii) the block key\\n570\\nid is used as a DynamoBD primary key in that table; (iii) the set of entries\\n(key-value pairs) of a blockb is used as the set of attribute name-value pairs\\nin the item forb (a serialization of the values is used, if needed). For example,\\nFigure 7 shows the implementation of the NoAM database of Figure 8.\\n25'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 26, 'page_label': '27', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='col lection Player\\nid\\n document\\nmary\\n{\\nid:”mary”,\\nusername:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames:\\n[ { game:”Game:2345”, opponent: ”Player:rick”},\\n{ game:”Game:2611”, opponent: ”Player:ann”} ]\\n}\\nFigure 12: Implementation in MongoDB\\nThe retrieval ofa block, given its collectionC and block keyid,c a nb ei m p l e -575\\nmented by performing a singlegetItem operation, which retrieves the item that\\ncontains all the entries of the block. The storage of a block can be implemented\\nusing a putItem operation, to save all the entries of the block, in an atomic way.\\nIt is worth noting that, using operationgetItem,i ti sa l s op o s s i b l et or e t r i e v ea\\nsubset of the entries of a block. Similarly, using operationupdateItem, it is also580\\npossible to update just a subset of the entries of a block, in an atomic way.\\nThis implementation is also eﬀective, since DynamoDB controls distribution\\nand atomicity with reference to items.\\n4.3.3. Document store: MongoDB\\nIn MongoDB ([29], Section 2.4), which is a document store, a natural imple-585\\nmentation for a NoAM database can be based on a distinct MongoDB collection\\nfor each collection of blocks, and a single main document for each block. The\\ndocument for a blockb can be deﬁned as a suitable JSON/BSON serialization\\nof the complex value of the entries inb, plus a special ﬁeld to store the block\\nkey id of b, as required by MongoDB,{\\n id:id}.590\\nWith reference to a NoAM data representationD, consider a blockb in a\\ncollectionC havingblockkey id.I fbcontainsjust anentry e, then the document\\nfor b is just a serialization ofe.O t h e r w i s e ,i fb contains multiple entries, we use\\nall the entries in blockb to create a new document. Speciﬁcally, we proceed by\\n26'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='col lection Player\\nid\\n document\\nmary\\n{\\nid:”mary”,\\nusername:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames[0]: { game:”Game:2345”, opponent: ”Player:rick” },\\ngames[1]: { game:”Game:2611”, opponent: ”Player:ann” }\\n}\\nFigure 13: Alternative implementation in MongoDB\\nbuilding a documentd for b as follows: (i) the collection nameC is used as the595\\nMongoDB collection name; (ii) the block keyid is used for the special top-level\\nid ﬁeld{\\n id:id} of d; (iii) then, each entry in the blockb is used to ﬁll a (possibly\\nnested) ﬁeld of documentd. See Figure 12.\\nTheretrievalofablock, givenitscollection C andkey id, canbeimplemented\\nby performing aﬁnd operation, to retrieve the main document that represents600\\nall the block (with its entries). The storage of a block can be implemented using\\nan insert operation, which saves the whole block (with its entries), in an atomic\\nway. It is worth noting that, using other MongoDB operations, it is also possible\\nto access and update just a subset of the entries of a block, in an atomic way.\\nAn alternative implementation for MongoDB is as follows. Each block b\\n605\\nis represented, again, as a main document forb, but using a distinct top-level\\nﬁeld-value pair for each entry in the NoAM data representation. In particular,\\nfor each entry (ek,ev), the document forb contains a top-level ﬁeld whose name\\nis a coding for the entry key (access path)ek, and whose value is either an\\natomic value or an embedded document that serializes the entry valueev.F o r610\\nexample, according to this implementation, the data representation of Figure 8\\nleads to the result shown in Figure 13.\\n4.4. Experiments\\nWe will now discuss a case study of NoSQL database design, with refer-\\nence to our running example. For the sake of simplicity, we just focus on the\\n615\\n27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='representation and management of aggregates for games.\\nData for each game include a few scalar ﬁelds and a collection of rounds.\\nThe important operations over games are: (1) the retrieval of a game, which\\nshould read all the data concerning the game; and (2) the addition of a round\\nto a game.620\\nAssume that, to manage games, we have chosen a key-value store as the\\ntarget system. The candidate data representations are: (i) using a single entry\\nfor each game (as shown in Figure 9, in the following calledEAO); (ii) splitting\\nthe data for each game in a group of entries, one for each round, and including\\nall the remaining scalar ﬁelds in a separate entry (as shown in Figure 11, called\\n625\\nRounds).\\nWe expect that the ﬁrst operation (retrieval of a game) performs better in\\nEAO, since it needs to read just a key-valuepair, while the second one (addition\\nof a round to a game) is favored byRounds, which does not require to rewrite\\nthe whole game.630\\nWe ran a number of experiments to compare the above data representations\\nin situations of diﬀerent application workloads. Each game has, on average, a\\ndozen rounds, for a total of about 8KB per game. At each run, we simulated\\nthe following workloads: (a) game retrievals only (in random order); (b) round\\nadditions only (to random games); and (c) a mixed workload, with game re-\\n635\\ntrieval and round addition operations, with a read/write ratio of 50/50. We ran\\nthe experiments using diﬀerent database sizes, and measured the running time\\nrequiredby the workloads. The targetsystem was Oracle NoSQL,deployed over\\nAmazon AWS on a cluster of four EC2 servers.\\n1\\nThe results are shown in Figure 14. Database sizes are in gigabytes, timings640\\nare in milliseconds, and points denote the average running time of a single op-\\neration. The experiments conﬁrm the intuition that the retrieval of games (Fig-\\nure 14(a)) is always favored by theEAO data representation, for any database\\nsize. On the other hand, the addition of a round to an existing game (Fig-\\n1 This activity was supported by A WS in Education Grant award.\\n28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 29, 'page_label': '30', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nGame Retrieval\\nEAO Rounds\\n(a) Game Retrieval\\n0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nRound Addition\\nEAO Rounds\\n(b) Round Addition\\n0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nMixed Load (50/50)\\nEAO Rounds\\n(c) Mixed Load\\nFigure 14: Experimental results\\nure 14(b)) is favored by theRounds data representation. Finally, the exper-645\\niments over the mixed workload (Figure 14(c)) show a general advantage of\\nRounds over EAO, which however decreases as the database size increases.\\nOverall, it turns out that theRounds data representation is preferable.\\nWe also performed other experiments on a data representation that does\\nnot conform to the design guidelines proposed in this paper. Speciﬁcally, a data650\\nrepresentation that divides the rounds of a game into independent key-value\\npairs, rather than keeping them together in a same block, as suggested by our\\napproach. In this case, the performance of the various operations worsens by at\\nleast an order of magnitude. Moreover, with this data representation it is not\\npossible to update a game in an atomic way.\\n655\\nOverall, these experiments show that: (i) the design of NoSQL databases\\nshould be done with care as it aﬀects considerably the performance and consis-\\n29'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='tency of data access operations, and (ii) our methodology provides an eﬀective\\ntool for choosing among diﬀerent alternatives.\\n5. Related works660\\nAlthough several authors have observed that there is a need for data-model\\napproaches to the design and management of NoSQL databases [9, 10, 11],\\nvery few works have addressed this issue,especially from a general and system-\\nindependent point of view. Indeed, most of them propose a solution to a speciﬁc\\nproblem in a limited scenario.\\n665\\nFor instance, Pasqualin et al. have recently shown how a document-oriented\\nmodel can be eﬃciently implemented in a NoSQL document store [30]. Sim-\\nilarly, Olivera et al. [31] and de Lima and Mello [32] have proposed a data-\\nmodel based methodology for the design of NoSQL document database [32],\\nwhereas Chevalier et al. have addressed the speciﬁc problem of leveraging on\\n670\\na document-oriented model for implementing a multidimensional database in a\\nNoSQL document store [33] and in a column-oriented NoSQL database [34].\\nMost of the other contributions to data modeling for NoSQL systems come\\nfrom on-line papers, usually published in blogs of practitioners, that discuss\\nbest practices and guidelines for modeling NoSQL databases, most of which\\n675\\nare suited only for speciﬁc systems. For instance, [5] lists some techniques for\\nimplementing and managing data stored in diﬀerent types of NoSQL systems,\\nwhile [35] discusses design issues for the speciﬁc case of key-value datastores.\\nSimilarly, Mior et al. [36] have recently proposed an approach to the problem of\\nschema design for the speciﬁc class of extensible record stores. On the system-\\n680\\noriented side, [6, 7, 8] illustrate design principles for the speciﬁc cases of HBase,\\nMongoDB, and Cassandra, respectively. However, none of them tackles the\\nproblem from a general perspective, as we advocate in this paper.\\nRecently, Ruiz et al. have proposed a reverse engineering strategy aimed at\\ninferring the implicit schema of NoSQL databases [37]. This approach supports685\\nthe idea that, even in this context, a model-baseddescriptionofthe organization\\n30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='of data is very useful during the entire life-cycle of a data set.\\nTo the best of our knowledge, this paper presents the ﬁrst general design\\nmethodology for NoSQL systems with initial activities that are independent of\\nthe speciﬁc target system. Our approach to data modeling is based on data690\\naggregates, a notion that is central in NoSQL databases where application data\\nare grouped in atomic units that are accessed and manipulated together [3].\\nThe notion of aggregate also occurs in other contexts with a similar meaning.\\nFor example, in Domain Driven Design [19], a widely followed object-oriented\\nsoftware development approach, an aggregate is a group of related application\\n695\\nobjects, used to govern transactions and distribution. Also Helland [20] advo-\\ncates the use of aggregates (there called entities) as units of distribution and\\nconsistency. In this framework, Baker et al. [38] propose the notion of entity\\ngroups, a set of entities that can be manipulated in an atomic way. They also\\ndescribe a speciﬁc mapping of entity groups to Bigtable [22], which however700\\nmakes the approach targeted only to a speciﬁc NoSQL system. Our approach is\\nbased on a more abstract database model, NoAM, and is system independent,\\nas it is targeted to a wide class of NoSQL systems.\\nThe issue of identifying data accessunits in database design shows some\\nsimilarities with problems studied in the past, such as: (i) the early works\\n705\\non vertical partitioning and clustering [39], with the idea to put together the\\nattributes that are accessed together and to separate those that are visited\\nindependently, and (ii) the more recent approaches to relational (or object-\\nrelational) storage of XML documents [40], where various alternatives obviously\\nexist, with tables that can be very small and handle individual edges, or very710\\nwide and handle entire paths, and many alternatives in between.\\nA major observation from [9] is that the availability of a high-level represen-\\ntation of the data remains a fundamental tool for developers and users, since it\\nmakes understanding, managing, accessing, and integrating information sources\\nmuch easier, independently of the technologies used. We have addressed this715\\nissue by proposing NoAM, an abstract data model that makes it possible to\\ndevise an initial phase of the design process that is independent of any speciﬁc\\n31'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 32, 'page_label': '33', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='system but suitable for each.\\nAlong this line, SOS [41] is a tool that provides a common programming\\ninterface towards diﬀerent NoSQL systems, to access them in a uniﬁed way.720\\nThe interface is based on a simple, high-level common data model which is\\ninspired by those of non-relational systems and provides simple operations for\\ninserting, deleting, and retrieving database objects. However, the deﬁnition of\\ntools for data access is complementary to data models and design issues.\\nFinally, Jain et al. discusses the potential mismatch between the require-725\\nments of scientiﬁc data analysis and the models and languages of relational\\ndatabase systems [42], whereas Alagiannis et al. [43] advocate a new database\\ndesign philosophy for emerging applications. This paper tries to provide a con-\\ntribution to these problems.\\n6. Conclusion\\n730\\nIn this paper we have argued how data modeling can be useful in the No-\\nSQL arena. Speciﬁcally, we have proposed a comprehensive methodology for\\nthe design of NoSQL databases, which relies on an aggregate-oriented view of\\napplication data, an intermediate system-independent data model for NoSQL\\ndatastores, and ﬁnally an implementation activity that takes into account the735\\nfeatures of speciﬁc systems.\\nReferences\\n[1] F. Bugiotti, L. Cabibbo, P. Atzeni, R. Torlone, Database design for NoSQL\\nsystems, in: Conceptual Modeling - 33rd International Conference, ER\\n2014, Atlanta, GA, USA, October 27-29, 2014. Proceedings, 2014, pp. 223–740\\n231.\\n[2] R. Cattell, Scalable SQL and NoSQL data stores, SIGMOD Record 39 (4)\\n(2010) 12–27.\\n[3] P. J. Sadalage, M. J. Fowler, NoSQL Distilled, Addison-Wesley, 2012.\\n32'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 33, 'page_label': '34', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='[4] M. Stonebraker, Stonebraker on NoSQL and enterprises, Comm. ACM745\\n54 (8) (2011) 10–11.\\n[5] I. Katsov, NoSQL data modeling techniques, Highly Scalable Blog,\\nhttps://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/,\\naccessed February 2016 (2012).\\n[6] A. Khurana, Introduction to HBase Schema Design, ;login: The Usenix750\\nmagazine 37 (5) (2012) 29–36.\\n[7] M. Hamrah, Data Modeling at Scale: MongoDB + Mon-\\ngoid, Callbacks, and Denormalizing Data for Eﬃciency,\\nhttp://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks -\\n(Accessed February, 2016) (2011).755\\n[8] A. Chebotko, A. Kashlev, S. Lu, A Big Data Modeling Methodology for\\nApache Cassandra, in: IEEE International Congress on Big Data, 2015,\\npp. 238–245.\\n[9] P. Atzeni, C. S. Jensen, G. Orsi, S. Ram, L. Tanca, R. Torlone, The rela-\\ntional model is dead, SQLis dead, and I don’t feel sogoodmyself, SIGMOD\\n760\\nRecord 42 (2) (2013) 64–68.\\n[10] A. Badia, D. Lemire, A call to arms: revisiting database design, SIGMOD\\nRecord 40 (3) (2011) 61–69.\\n[11] C. Mohan, History repeats itself: sensible and NonsenSQL aspects of the\\nNoSQL hoopla, in: EDBT, 2013, pp. 11–16.765\\n[12] C. Batini, S. Ceri, S. B. Navathe, Conceptual Database Design: An Entity-\\nRelationship Approach, Benjamin/Cummings, 1992.\\n[13] F. Bancilhon, Object-oriented database systems, in: Proceedings of the\\nSeventh ACM SIGACT-SIGMOD-SIGART Symposium on Principles of\\nDatabase Systems, March 21-23, 1988, Austin, Texas, USA, 1988, pp. 152–770\\n162.\\n33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 34, 'page_label': '35', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='[14] P. Atzeni, P. Merialdo, G. Mecca, Data-intensive web sites: Design and\\nmaintenance, World Wide Web 4 (1-2) (2001) 21–47.\\n[15] S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera,\\nDesigning Data-Intensive Web Applications, Morgan Kaufmann, 2003.775\\n[16] G. Mecca, A. O. Mendelzon, P. Merialdo, Eﬃcient queries over web views,\\nIEEE Trans. Knowl. Data Eng. 14 (6) (2002) 1280–1298.\\n[17] S. Abiteboul, P. Buneman, D. Suciu, Data on the Web: From Relations to\\nSemistructured Data and XML, Morgan Kaufmann, 1999.\\n[18] M. Stonebraker, U. C¸etintemel, “one size ﬁts all”: An idea whose time780\\nhas come and gone (abstract), in: Proceedings of the 21st International\\nConferenceonDataEngineering,ICDE2005,5-8April2005,Tokyo,Japan,\\n2005, pp. 2–11.\\n[19] E. Evans, Domain-Driven Design, Addison-Wesley, 2003.\\n[20] P. Helland, Life beyond distributed transactions: an apostate’s opinion, in:785\\nCIDR 2007, 2007, pp. 132–141.\\n[21] S. Abiteboul, R. Hull, V. Vianu, Foundations of Databases, Addison-\\nWesley, 1995.\\n[22] F. Chang, et al., Bigtable: A distributed storage system for structured\\ndata, ACM Trans. Comput. Syst. 26 (2).790\\n[23] Oracle,OracleNoSQLDatabase, http://www.oracle.com/us/products/database/nosql/,\\naccessed February 2016.\\n[24] J. Shute, et al., F1: A distributed SQL database that scales, PVLDB 6 (11)\\n(2013) 1068–1079.\\n[25] MongoDB Inc., MongoDB, http://www.mongodb.org, accessed February795\\n2016.\\n34'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 35, 'page_label': '36', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='[26] Amazon Web Services, DynamoDB, http://aws.amazon.com/dynamodb,\\naccessed February 2016.\\n[27] D. Pritchett, BASE: An ACID alternative, ACM Queue 6 (3) (2008)48–55.\\n[28] V. Vernon, Implementing Domain-Driven Design, Addison-Wesley, 2013.800\\n[29] K. Chodorow, MongoDB: The Deﬁnitive Guide, O’Reilly Media, 2013.\\n[ 3 0 ]D .P a s q u a l i n ,G .S o u z a ,E .L .B u r a t t i ,E .C .d eA l m e i d a ,M .D .D e lF a b r o ,\\nD. Weingaertner, A case study of the aggregation query model in read-\\nmostly NoSQL document stores, in: 20th Int. Database Engineering &\\nApplications Symposium (IDEAS ’16), IDEAS ’16, ACM, New York, NY,805\\nUSA, 2016, pp. 224–229.\\n[31] H. V. Olivera, M. Holanda, V. Guimarˆaes, F. Hondo, W. Boaventura, Data\\nmodeling for NoSQL document-oriented databases, in: 2nd Annual Int.\\nSymposium on Information Management and Big Data (SIMBig 2015),\\nVol. 1478 of CEUR Workshop Proceedings, 2015, pp. 129–135.\\n810\\n[32] C.de Lima, R.dos SantosMello, Aworkload-drivenlogicaldesignapproach\\nfor NoSQL document databases, in: 17th Int. Conference on Information\\nIntegration and Web-based Applications & Services (iiWAS ’15), iiWAS\\n’15, ACM, New York, NY, USA, 2015, pp. 73:1–73:10.\\n[33] M. Chevalier, M. E. Malki, A. Kopliku, O. Teste, R. Tournier, Implemen-\\n815\\ntation of multidimensional databases with document-oriented NoSQL, in:\\n17th International Conference on Big Data Analytics and Knowledge Dis-\\ncovery, (DaWaK 2015), Vol. 9263 of Lecture Notes in Computer Science,\\nSpringer, 2015, pp. 379–390.\\n[34] M. Chevalier, M. E. Malki, A. Kopliku, O. Teste, R. Tournier, Implementa-820\\ntion of multidimensional databases in column-oriented NoSQL systems, in:\\n19th EastEuropeanConference on Advances in Databases and Information\\nSystems (ADBIS 2015), 2015, pp. 79–91.\\n35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 36, 'page_label': '37', 'file_type': 'pdf', 'file_name': '[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf'}, page_content='[35] T. Olier, Database design using key-value tables,\\nhttp://www.devshed.com/c/a/mysql/database-design-using-key-value-tables/,825\\naccessed February 2016 (2006).\\n[36] M. J. Mior, K. Salem, A. Aboulnaga, R. Liu, Nose: Schema designfor nosql\\napplications, in: 32ndIEEEInternationalConferenceonData Engineering,\\nICDE 2016, Helsinki, Finland, May 16-20, 2016, 2016, pp. 181–192.\\n[37] D. S. Ruiz, S. F. Morales, J. G. Molina, Inferring Versioned Schemas from830\\nNoSQL Databases and Its Applications, in: 34th International Conference\\non Conceptual Modeling (ER 2015), 2015, pp. 467–480.\\n[38] J. Baker, et al., Megastore: Providing scalable, highly available storage for\\ninteractive services, in: CIDR 2011, 2011, pp. 223–234.\\n[39] T. J. Teorey, J. P. Fry, The logical record access approach to database835\\ndesign, ACM Comput. Surv. 12 (2) (1980) 179–211.\\n[40] D. Florescu, D. Kossmann, Storing and querying XML data using an\\nRDMBS, IEEE Data Eng. Bull. 22 (3) (1999) 27–34.\\n[41] P. Atzeni, F. Bugiotti, L. Rossi, Uniform access to NoSQL systems, Inf.\\nSyst. 43 (2014) 117–133.840\\n[42] S. Jain, D. Moritz, D. Halperin, B. Howe, E. Lazowska, SQLShare: Results\\nfromamulti-yearSQL-as-a-Servicee xperiment, in: Proceedingsofthe 2016\\nInternational Conference on Management of Data, SIGMOD Conference\\n2016, San Francisco, CA, USA, June 26 - July 01, 2016, 2016, pp. 281–293.\\n[43] I. Alagiannis, R.Borovica-Gajic,M.Branco, S.Idreos, A. Ailamaki, NoDB:845\\neﬃcient query execution on raw data ﬁles, Commun. ACM 58 (12) (2015)\\n112–121.\\n36'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nIEEE TRANSACTIONS ON Knowledge and Data Engineering,  manuscript ID 1 \\n \\nHaery: a Hadoop based Query System on Accumulative and  \\nHigh-dimensional Data Model for Big Data \\nJie SONG 1, Hongyan HE 1, Richard THOMAS 2, Yubin BAO 3, Ge YU 3 \\nAbstract——Column-oriented stores, known for their scalability and flexibility, are a common NoSQL database implementation \\nand are increasingly used in big data management. In column -oriented stores, a “full -scan” query strategy is inefficient and the \\nsearch space ca n be reduced if data is well partitioned or indexed, however there is no pre -defined schema for building and \\nmaintaining partitions and indexes at lower cost. We leverage an accumulative and high -dimensional data model, a \\nsophisticated linearization algori thm, and an efficient query algorithm, to solve the challenge of how a pre -defined and well -\\npartitioned data model can be applied to flexible and time -varied key-value data. We adapt a high-dimensional array as the data \\nmodel to partition the key -value dat a without additional storage and massive calculation; improve the Z -order linearization \\nalgorithm, which map multidimensional data to one dimension while preserving locality of the data points , for flexibility; efficiently \\nbuild a n expansion mechanism for the data model to support  time-varied data. The result is Haery, a column -oriented store, \\nbased on a distributed file system and computing framework. In experiments, Haery is compared with Hive, HBase, Cassandra, \\nMongoDB, PostgresXL and HyperDex in terms of query performance. With results indicat ing Haery on average performs 4.57x, \\n4.23x, 3.55x, 1.79x, 1.82x and 120.6x faster, respectively. \\nIndex Terms—Key-value data, Column-oriented store, Multi-dimensional data model, Linearization, Accumulation  \\n——————————   \\uf075   —————————— \\n1 INTRODUCTION\\nIn the big data era, traditional relational databases can no \\nlonger meet the requirements of query performance and \\nscalability [1]. Researchers are eager to find an effective way \\nto manage massive data. Column\\n-oriented stores, which are \\nnewly emerged NoSQL databases, are wildly accepted by \\nboth indu stry and academia. Column -oriented stores use \\ntables, wide columns or column families as their fundamen-\\ntal data models. In these models, a record is represented as a \\ncollection of key -value pairs (the column name is the key) \\nsuch that each possible key app ears at most once in the col-\\nlection. [2]. \\nIn this paper, we focus on partition based query optimiza-\\ntions of column -oriented stores. Generally, queries can be \\noptimized by partition pruning, i.e, if data is well partitioned \\nby keys, then the query can be op timized by only scanning \\nthe matched partitions. The advantage of a column -oriented \\nstore is flexibility; it relies on a free -formed and soft schema \\nso that arbitrary key -value pairs can be stored. Unfortunate-\\nly, because of this flexibility, column-oriented stores lack a \\npre-defined schema so key based partitions are difficult to \\nmaintain. For example  of composite partitioning, rows (rec-\\nords) do not contain same keys and new keys are freely in-\\ntroduced. Conversely, in a traditional database, the data fol-\\nlows a pre-defined and rigid schema so that it is well parti-\\ntioned, the cost of which is flexibility. Consequently a \\ntradeoff is made between normalization and flexibility of the \\nschema. \\nWe have built key -based partitions on massive key -value \\ndata without redu cing flexibility. These partitions improve \\nquery performance by greatly reducing the search space. In \\ncolumn-oriented stores, the key -value data is organized in \\ntables. Given two related key -value pairs, the relationships \\nbetween them are “different keys (columns) of the same rec-\\nord (row)” or “same key (column) of different records \\n(rows)”. If we segment each key by its value, and let keys be \\ndimensions, then rows can be categorized into a high -\\ndimensional data model (HDM for short). The HDM is a \\nlogical data model, by which the search space can be reduced \\nto some cells of the HDM when a query is performed. This \\ninstinctive solution challenges the flexibility of a key -value \\ndata model. The following issues need to be solved: \\n(1\\n) How to ensure the efficiency of query and storage on \\nHDM, especially given the sparsity of HDM due to the fact \\nthat keys in different rows are diverse; \\n(2\\n) Given the lack of well-formed schema, how to fit the \\nnewly imported keys; \\n(3) Considering the extremely large address space of \\nHDM, how to design the mapping mechanism, to linearize \\nthe HDM to storage, and to expand without changing the \\nexisting storage; \\n(4) The query algorithm and implementation. \\nIn this paper we propose Haery ( Hadoop query), a Ha-\\ndoop based query system that uses  an accumulative HDM \\nfor big data. As a query system, it highlights the query opti-\\nmization of column-oriented stores, and is based on Hadoop \\nHDFS as the storage and Hadoop MapReduce as the compu-\\nting framework  (it also support s other computing frame-\\nworks). Our contributions are as follows: \\n(1\\n) Drawing on the experience of partitions in relational \\ndatabase, we propose an accumulative HDM to partition \\nkey-value data. The HDM is \"accumulative\" in the sense that \\nnew keys and values can be dynamically introduced. \\n(2) We propose a linearization algorithm, as an extensible \\nand flexible improvement of the Z -order curve [3] and pro-\\nxxxx-xxxx/0x/$xx.00 © 200x IEEE        Published by the IEEE Computer Society \\n———————————————— \\n\\uf0b7 Jie SONG and Hongyan HE are with the Software College, Northeastern \\nUniversity, Shenyang, 110819, China. E -mail: songjie@mail.neu.edu.cn. \\nand  2322710332@qq.com. \\n\\uf0b7 Richard THOMAS is with School of Information Technology and Electrical \\nEngineering, The University of Queensland, Queensland, Australia  \\n\\uf0b7 Yunbing Bao and Ge YU are with the School of Computer Science and \\nEngineering, Northeastern University, Shenyang, 110819, China. E -mail: \\nbaoyb@mail.neu.edu.cn and yuge@mail.neu.edu.cn.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content=\"1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n2 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\npose an address tree, to map the logical data model to physi-\\ncal addresses. \\n (3) Haery, as a system, contributes to the implementation \\nexperience of NoSQL databases. \\nFrom the aspect of query optimization, the differences be-\\ntween Haery and other partitioned or indexed NoSQL data-\\nbases are as follows: \\n(1) Partitioning techniques face challenges of maintaining \\nflexibility and extensibility. Haery adopts a schema -\\ndependent partition approach which has a finer granularity \\nthan sharding techniques  [4], and is easy to maintain and \\nextend. \\n(2) Indexing techniques face challenges of large storage \\nand search cost in a big data environment. Compared with \\nindexing, Haery does not depend on any pre -computation \\nand materialization techniques, so the storage cost is very \\nlow. Haery also prefers calculation to sea rching, avoiding \\ncomplexity explosion in a big data environment.  \\n(3) HDM in Haery is easier to maintain than partitions \\nand indexes \\nThis paper evaluates the core algorithm of Haery, com-\\nparing query performance, loading performance and storage \\ncost of Haery with t hose of other data stores, such as Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nThe results show that Haery has a marked advantage in que-\\nry performance regardless of the data size and query com-\\nplexity. The extra calculation required during data loading is \\nless than these other stores, and the additional storage cost is \\nnegligible. \\nThe rest of this paper is organized as follows. Section 2 \\noverviews related work. Section 3 provides a detailed de-\\nscription of the HDM and section 4 explains the flat  Z-order \\nlinearization and addressing algorithms. Section 5 describes \\nthe accumulation strategy and section 6\\n describes the query \\napproaches. Section 7 briefly introduces the system architec-\\nture of Haery, explaining each component of the system. \\nSection 8 evaluates the loading and querying performance of \\nHaery, comparing these with of the performance of Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nFinally, conclusions and future works are summarized in \\nsection 9. \\n2 RELATED WORK \\nIn recent years, column-oriented stores which store arbitrary \\nkey-value pairs, ha ve attracted much attention in big data \\nresearch. Research on indexing techniques and on schema or \\nlogical data models of column -oriented stores are related to \\nHaery.  \\nWu et al. [ 5] proposes a two -level indexing framework \\nwhich contains local and global indexes. The local index \\nmaintains information on each node. The global index con-\\nsists of selected local indexes in order to save storage and \\nimprove query efficiency. The proposed Ef ficient B -tree \\nadopts a B+ tree to build the local index and a cost model -\\nbased adaptive strategy to select local indexes. The global \\nindex is organized as a BATON network [6]. However, it is \\noptimized for only querying single attributes, not for multi -\\ndimensional queries. Similar to Efficient B -tree, RT-CAN [7] \\nadopts an R-tree to build a local index and a CAN network to \\nbuild a global index. These optimizations are based on a \\npeer-to-peer structure. Centralized EMINC [8] and A-tree [9] \\nindexes are propose d in a master -sla\\nve structure which is \\nalso widely accepted in big data environments. While \\nproviding query performance optimization, the cost is huge \\nto maintain and rebuild indexes. \\nITHBase [10] and IHBase [11] employ secondary indexes. \\nBoth build an index table based on index keys. The key-value \\npairs in the index table are retrieved from the original data \\nset. When a query is performed, the index table is accessed \\nfirst, then the matched data is located by the mappings be-\\ntween the index table and indexe d keys. Secondary indexes \\nbenefit query performance when indexed keys are contained \\nin the query conditions. To reduce the cost of random query-\\ning, Zou et al. [12] proposed CCIndex which stores original \\ndata in an index table. When a query is performed, it  only \\nscans the index table without looking up the mappings be-\\ntween the index table and indexed keys. However, it is diffi-\\ncult to guarantee fault tolerance and consistency. A second-\\nary index is easier to maintain than two -level indexes. Both \\ntwo-level indexes and secondary indexes are based on specif-\\nic index structures, which are difficult to maintain when the \\ndata set is updated frequently. They are also not efficient \\nwhen querying high-dimensional data due to the indexes on \\nmany columns are extreme huge.  \\nMulti-dimensional indexes have been studied extensively. \\nCommon multi-dimensional indexes include Grid Index [13], \\nKD-trees [14] and R-Trees [15]. They divide the space into a \\nk-dimensional space which consists of grids, and allocate \\naddresses for each grid. Query results can be found by locat-\\ning specific grids, which is faster than the original query \\nstrategy. Based on this idea, MD- HBase [16] and SHG -tree \\n[17] were proposed. The storage of SHG\\n-tree is inefficient \\ndue to the sparsity of high-dimensional data, and MD-HBase \\nhas weaknesses in data consistency, and brings extra costs \\nand latency when data is unevenly distributed. Many re-\\nsearchers have come up with new index structures to satisfy \\ndifferent requirements. VAR-tree [18] aims to improve query \\nperformance by compression; CSA -tree [19] focuses on \\nmemory search; NV-tree [20] facilitates near inquiries, and so \\non. \\nThe optimization of schemas of non -relational databases \\nalso contributes to the query performance. Mior et al. [2 1\\n] \\npresented a system for re commending database schemas for \\nNoSQL applications. Automating the design process allows \\nthe proposed prototype, NoSQL Schema Evaluator (NoSE), \\nto produce efficient schemas and to examine more alterna-\\ntives than would be possible with a manual rule -based ap-\\nproach. They proposed a cost -based approach by a novel \\nbinary integer programming formulation to guide the map-\\nping from the application's conceptual data model to a data-\\nbase schema, while we propose  the logic data model map-\\nping the database schema  to the physical dataset for query \\noptimization.Vajk et al. [2 2] defined some problems of sche-\\nma design in NoSQL database by compared with material-\\nized views in relational databases, and proposed a cost driv-\\nen model, which can optimize the mapping from the applica-\\ntion data model to a physical schema. They start with a nor-\\nmalized data schema, then identified a column store schema \\nwhich can serve the queries with minimal cost. However, it\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 3 \\n \\nis to optimize for monetary cost (storage and query) only \\ninstead of co nsidering performance, also the cost model is \\nusually not the case, which maybe overvalue the inexpensive \\nschema options with poor performance. And the solution is \\nnot general enough, it is only efficient for predefined queries.  \\nHyperDex [23,24] is a cloud database. The key insight be-\\nhind HyperDex is the concept of hyperspace hashing in \\nwhich objects with multiple attributes are mapped into a \\nmultidimensional hyperspace. Haery’s initial idea is close to \\nHyperDex, however, the two systems have essential dif fer-\\nences. HyperDex relays on a predefined schema  of objects, \\nmodels the objects with same attributes into tables, divides \\nattributes of the table into different partitions, duplicat es \\nobjects of the table to every partition, builds the hyperspace \\nof each partition according to order ed hash values of its at-\\ntributes, tessellats the hyperspace into regions, assigns the \\nregions to servers, and stores the mappings as data in the \\ncorrdinator server. As the comparsion, Haery relays on a \\nflexible and accumulative schema of rows, models the rows \\nwith different keys into a table, builds the key-cube accord-\\ning to the segments on value range of each key, expands key-\\ncube to support new keys, divideds rows into cells, and cal-\\nculates addresses of cells. We compare the two systems in \\nthe Table 1. \\nIn Table 1, some concepts will be explained in \\nthe rest paper. \\nTABLE 1 DIFFERENCES BETWEEN HYPERDEX AND HAERY \\n \\nFacing the “volume”, “variety” and “variability” charac-\\nteristics of big data, index based solutions have three weak-\\nnesses: indexes require a large amount of storage, it is costly \\nto build indexes for various keys, and it is too complex to \\nmaintain and rebuild indexes when dealing with large scale \\nand frequently updated data sets. Haery solves these prob-\\nlems by an accumu lative and high-dimensional data model. \\nThere are three categories of multidimensional data models \\nfor NoSQL databases. \\n(1\\n) Basic multidimensional data models, which are a sim-\\nple technique, take the data sets as points in multidimen-\\nsional space, then divide data attributes (keys) as dimensions. \\nFor example, Chevalier et al. [25] propose an implementation \\nof traditional OLAP (On-Line Analytical Processing) systems \\nby column-oriented and document-oriented database;  \\n(2) Statistical multidimensional models, which are more \\ncomplicated than basic models, leverage specific aggregation \\nfunctions to achieve hierarchies of dimensions. For example, \\nIqbal et al. [26] propose a histogram method, which iterative-\\nly splits histogram buckets until the given space limit is \\nreached, to summarize multidimensional data represented as \\na tuple -independent probabilistic model. The proposed \\nmethod. \\n(3) Structured multidimensional data model s, which are \\nmore powerful to support hierarchies of dimensions. For \\nexample, we have prop osed a paper proposes a structured \\nand distributed MOLAP technique, named DOLAP (distrib-\\nuted OLAP), based on Hadoop distributed file system (HDFS) \\nand MapReduce program model, it can also support complex \\naggregation operations between hierarchical structures [27\\n]. \\nHaery adopts an improved basic multidimensional data \\nmodel. On one hand, all the above data models would be \\nrefactored when a new dimension is introduced, or current \\ndimension hierarchies are updated. For basic model s, new \\ndimensions cause updating index spaces and remapping \\nindexes to addresses. For statistical\\n models, new data causes \\nre-calculation of the existing aggregation. For structured \\nmodels, updating dimension hierarchies causes the rebuild-\\ning of the complex data structure. The cost of this refactoring \\nis unaffordable in a big data environment. Haery treats refac-\\ntoring in a distinct way, creating a new data structure instead \\nof modifying the existing one. In most column -oriented \\nstores, to reduce the cost of modification, data items  are not \\ndeleted or modified but replaced. Following this idea, the \\ndata model may also be replaced instead of being modified. \\nThe previously mentioned works aim to solve a specific \\nproblem to meet some specific requirements. Consequently \\nthey concentrate on query performance optimization. Haery \\naims to solve a broader range of problems and is a general -\\npurpose, high-dimensional data supported, query optimized, \\nflexible and extendable data store. \\n3 MODEL \\nIn this section, the critical definitions for key-cube, which are \\nfundamental to linearization, accumulation and query in the \\nrest of paper, are explained. The main symbols appearing in \\nthis paper are listed in Table 2. \\nTABLE 2 MAIN SYMBOLS IN THIS PAPER \\n \\n3.1 Key-values \\nIn column-oriented stores, data is accessed with a key -value \\ndata model. A table consists of rows, and a row consists of a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n4 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nunique row key and many objects. An object has key -value \\npairs associated with different timestamps, in which keys \\nwith different timestamps are the same but valu es with dif-\\nferent timestamps are different. Referred to by an entity -\\nrelationship conceptual model, a row stands for an entity \\nand objects are attributes of the entity, in which keys are at-\\ntribute names, and values are attribute values, while \\ntimestamps represent different versions of attributes. \\n3.2 Segments \\nIn a column -oriented store, the values of a key may be nu-\\nmerical or textual data, and values may belong to a certain \\nrange. To partition these values, we define the segment as a \\nspecial set of values which belongs to the same range. \\nDefinition 1 Segment. A segment s represents a range of values, \\ns is a two -tuple <α, β> representing the lower boundary and \\nupper boundary of the range (α and β are numbers) \\nThere are three special forms of segment. \\n(1\\n) s0 =<-∞,+∞>, also named ALL, represents all values; \\n(2) s1=<-∞, α> and sw=<β, +∞>, named lower overflow \\nsegment and upper overflow segment; \\n(3) se=null, also named NULL, represents no value.  \\nGiven a value u of the key, we define u belongs to the \\nsegment s, denoted as segment u≼s. s and ≼ may have fol-\\nlowing four forms: \\n(1) u≼s0 \\n(2) u≼se if u is null.  \\n(3) u is a numerical value, namely u≼s iff u∈(a, β]. \\n(4) u is textual value, namely u≼s iff hash(u)∈(a, β]. Func-\\ntion hash() is the hash value (integer) of a string. The func-\\ntions hash-1(-∞) and hash-1(+∞) are invalid. \\nFor segment sx and sy , their operations are defined as fol-\\nlows. \\n(1) Union of two segments, as sx∪sy ={u| u≼ sx or u≼ sy }, \\nand intersection of two segments, as sx∩sy ={u| u≼ sx and u≼ \\nsy } \\n(2) One segment is the sub-segment (proper subset) of the \\nother, as sx \\uf0cc sy iff ax <ay and βx >βy \\n(3) The special operations on se, such as sx ⊄se, se ⊄ sx, se \\uf0cc \\nse; and sx∩se =∅, se∩se = se \\n(4) The comparison between segments, as sx ≤ sy iff βx ≤ \\nay, we know \"≤\" is total order relation on {s} \\n(5) sub(s) are the non-overlapping and non-empty subsets \\nof s, which are in ascending order. \\nPer the function sub(s), a segment s can be divided into \\nsub-segments. We call this segment refinement, and sub -\\nsegments are in ascending order according to the \" ≤\" opera-\\ntor. \\nDefinition 2 Segment Refinement. Segment refinement means \\nsegment s is divided into w sub-segments, which are in ascend-\\ning order, and are non -overlapping and non-empty. Segment s \\nis refined to sub(s) according to the definition of function sub() \\nin definition 1. The number of sub -segments is a fixed value, \\ncalled the refinement factor and denoted as w (w>3). \\nThe segmentation algorithm divides a segment into sub -\\nsegments with an equal interval which is defined as Δ=(β- \\na)/w. It is a simple algorithm and is abbreviated here. \\nAmong the sub(s), lower overflow sub-segment is included if \\ns is also a lower overflow su b-segment, and the same for \\nupper overflow segments. For example: \\nFor the initial state: s0=<-∞,+∞>,Δ=(β- a)/w, w=3 \\nThe sub-segment of s0 is: sub(s0)={ s1, s2, s3,..., sw}= {<-∞, a \\n>, < a, a +Δ>, < a +Δ, a +2Δ>,...,<β-Δ, β>, <β, +∞> } \\nThe sub-segment of s1 is: sub(s1)={<-∞, a -wΔ>,...,< a -2Δ,  a \\n-Δ>, < a -Δ, a >} \\nThe sub -segment of sw is: sub(sw)={ < β, β+Δ>, < β+Δ, \\nβ+2Δ>,...,<β+wΔ, +∞>,} \\nSegment refinement disperses a massive amount of  data \\non the sub -segments, and because of equal intervals, seg-\\nments may map different volumes of data. But, as is ex-\\nplained later, the data volume of segments tends to be equal \\n(equal frequency) after many iterations of segment refine-\\nment. \\nDefinition 3 Segment Hierarchy and Levels. A segment and \\nits sub-segments are organized as a hierarchy. Segment hierar-\\nchy is initiated as two levels at least, then expend to τ\\n levels. \\nThe root level is level 0 with a single segment s 0 (ALL). The \\nnext level contains predef ined w segments, which are sub(s0), \\nincluding upper and lower overflow segments. A segment in \\nlevel 1 may expanded to w sub -segments, with each level in \\nturn potentially expanding further. Or, a level may not expand \\nat all. The segment hierarchy is a “w -ary” ordered tree because \\nsub(s) is in ascending order. \\nA consequence of definition 3 is that the segment hierar-\\nchy expands gradually. We introduce the concept of version-\\ning to distinguish the segment hierarchies of different ex-\\npanded stages in section 5. \\nDefinition 4\\n Segment Index. Segment index is the identity of \\neach segment in a segment hierarchy, and is denoted as the sub-\\nscript \"j\" of segment s j. The segment hierarchy which contains \\nτ+1 levels may have at most  segments, so the index of the root \\nsegment is 0 and the in dexes of th e rest are coded \\nfrom 1 to n level -wise, and from left to right in a level. Since \\nlevel l contains at most power(w, l) segments, some segments \\nmay be absent because the refinement has not happened yet, but \\ntheir corresponding indexes are reserved. \\nFunction f. According to decimal coding, a segment range \\ncan be calculated by giving a segment index j. Given s1=<-∞, \\nα> and sw=<β, +∞>, a function f(j) returns the segment by its \\nindex, that is,  sj=f(j) and sj =<aj, bj>=<fa(j), fb(j)>. Meanwhile \\nj= f-1(\\nsj)=f-1(<aj, bj>). If it exists, sj is the x-th ordered segment \\nin the level y of segment hierarchy, x, y, aj and bj are as fol-\\nlows (in the equation wy is “y-th power of w”, but not super-\\nscript): \\ny = logww+(w-1)j-1,              x=j- (wy-w)/(w-1) ; \\nwhen  j=(wy-1)/(w-1),         aj=-∞,  βj=2a-β+x·w1-y·(β-a) ; \\nwhen j=(wy+1-w)/(w-1),aj=2β-a+(x-1-wy)w1-y·(β-a), βj=+∞ ; \\notherwise aj=2β-a +(x-1-wy)·w1-y·(β-a), βj=2a-β+x·w1-y·(β-a) . \\nDefinition 5 Active Segment and Dormant Segment.  Given \\na segment hierarchy, a segment which has no sub -segments (a \\nleaf node) is an active segment; and segments which have sub -\\nsegments (non-leaf nodes) are dormant segments. \\n3.3 Key-cube \\nKey-cube, which is a replacement for the key -value data \\nmodel, is a high -dimensional data model whose dimensions \\nare keys and cells are links to the data files. The key, which \\n \\n\\uf0e5\\n\\uf03d\\n\\uf03d\\n\\uf074\\n0l\\nlw n'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 5 \\n \\nrepresents a dimension, is divided into segments according \\nto their values. Key-cube is not a fixed data model but accu-\\nmulatively changes according to the keys and their values. \\nDefinition 6 Dimension and Dimension Index . In a key -\\ncube, a key is modeled as a dimension which categorizes values \\nof the key into non-overlapping active segments and se, in order \\nto provide filtering, grouping and labeling. A dimension con-\\ntains segments of the expandable segment hierarchy. se is the \\nfirst segment of a dimension, and the rest of the segments are \\nactive segments of the corresponding segment hierarchy from \\nleft to right. Thus a dimension d contains at least w+1 segments \\nbecause the segment hierarchy is initiated with at least two lev-\\nels (see definition 3). A dimension index is the identity number \\nof the dimension. Let subscript i (1≤i≤n) represent the index of \\nd. And a table of dimension index, named as DTable, is used to \\nmap the keys (names) to the dimension indexes. \\nDefinition 7\\n Cell and Cell Index . A cell is specified by active \\nsegments (leaves) or  the null segment ( se) of a dimension. All \\nrows are partitioned into the cells by the following rules: Let a \\nrow contains a key -value pair <k i, ui>, and for the correspond-\\ning dimension di, if the value u i≼sij, then the row is mapped to \\nsij. Rows in column -oriented store are organized as data files, \\nthus a cell contains physical directories of the data files where \\nthe corresponding rows are stored. Cell index is calculated by \\nthe linearization algorithm and the mapped directories are re-\\ntrieved by the address tree (see section 4). \\nDefinition 8 Segment Map . A segment map is a HashMap \\nwhose keys are integers, representing dimension indexes, the \\ncorresponding values of a key are also integers, representing \\nsegment indexes. Both keys and values are sorted. Let m be a \\nsegment map, the array m.keys are dimension indexes, and the \\narray m[i].values are segment indexes of dimension di. \\nNormally, key value pairs are sparse, thus not all keys are \\nmodeled as dimensions, and a key -cube contains a large \\nnumber of empty cells. But the key -cube is a logical data \\nmodel, and as is explained in sections 4, the sparsity does not \\nwaste storage. A high -dimensional key-cube does not have \\nthe problem of storage explosion because most information \\nis calculated rather than stored. \\nDefinition 9 Dimensional Key and Trivial Key . Keys which \\nare selected as dimensions are dimensional keys, otherwise they \\nare trivial keys. Most keys are dimensional keys in a key -cube. \\nBy default, new keys are trivial keys, which may be upgraded to \\ndimensional keys. \\nDifferent from the selection of keys when building  an in-\\ndex, dimensional keys not only frequently occur in query \\nconditions, but also in query results. On the contrary, trivial \\nkeys are keys whose values are not queried often, neither in \\nquery conditions nor in query results. There are a number of \\nkeys in the key-value data model, and among which dimen-\\nsional keys dominate, so that the key -cube has a high num-\\nber of dimensions. When a key is selected as a dimension, \\nand its range is determined according to the context, then s0 \\nto sw, which are levels 0 and 1  of the segment hierarchy, are \\ninitialized first.  \\n4 LINEARIZATION \\nA key -cube is a high -dimensional data model. There are a \\nlarge number of cells, even though most of them are empty, \\nbut the large address space of cell indexes is reserved. Haery \\nneeds a sophisticated linearization algorithm and addressing \\nmechanism to encode and map the cells indexes to the file \\naddresses. A linearization algorithm is an algorithm which \\nmaps multi-dimensional data to one dimension. This section \\ndescribes a linearization algorithm that encodes the cells of a \\nkey-cube into a sequence. A space -filling curve (SFC) is a \\ncurve whose range contains the entire n -dimensional hyper-\\ncube, so that a curve is selected to design a linearization al-\\ngorithm. The cell index, which is the resul t of linearization, \\nmaps to physical directories by querying a tree -structure, \\ncalled an address tree.  \\nIn a key-cube, the adjacency of cells benefits the address-\\ning process because the matched cells for a query are normal-\\nly adjacent, thus they can be addr essed together. The ad-\\ndressing of one cell could reuse the addressing information \\nof its adjacent cells, therefore, the indexes of neighbor cells \\nshould be continuous. Based on the classification of Asano T. \\n[28], the most popular SFCs are recursive SFCs, such as Z -\\norder, Gray and Hilbert, and non -recursive SFCs, such as \\nSweep and Scan. Sweep and Scan are simple, their time \\ncomplexities and flexibilities are better than the others, but \\nthey only maintain the continuity of one dimension and \\nbreak that of the  other dimensions. Meanwhile, Z -order is \\nfair and scalable on each dimension, and is relatively simpler \\nthan Gray and Hilbert  [29]. By Z -order, cells, especially \\nneighboring cells are encoded continuously. Z-order curve is \\nmore suitable choice for its fairness in all dimensions. \\nHowever, Z-order can only expand synchronously in all \\ndimensions. It is too rigid and requires the same number of \\nactive segments in each dimension. To solve this problem, a \\nflat Z-order algorithm is proposed, which only requires th at \\nthe number of active segments in dimensions are integral \\nmultiples of each other. If the conditions are not satisfied \\nnaturally, some null \\nsegments (se) are artificially added to the \\ndimensions. \\n \\nFig. 1. Six space curves in two-dimensional space \\nFigure 1-(e) and (f) shows the flat Z -order in comparison \\nwith the original Z -order in two dimensional space. Let the \\ninterleaving ratio of two dimensions be 1:2. Thus, in Figure \\n1\\n-(f), when the curve extends one dimension for one cell, it \\nextends the other dimension for two cells, while the original \\nZ-order can expand all the dimensions by the same number \\n(a) Sweep (b) Scan (c) Gray (d) Hilbert\\n(e) Z-order (f) Flat-zorder'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n6 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nof cells (interleaving ratio is 1:1). \\nDefinition 10 Binary length. Binary length θi of dimension i is \\n⌊log2max(sij)⌋ in which max(s ij) is the maximum index of a seg-\\nment in dimension i. Without loss of generality, let θ be the \\nminimum binary length among those of all dimensions, then \\nθi/θ is a positive integer. \\nGiven the active segments of dimension 1 to n, the index \\nof a corresponding cell is calculated as the following three \\nsteps in which arrays are indexed starting with 1. For clarity, \\nsij is the j-th segment in i-th dimension (see Table 1), and Fig-\\nure 2 is an example of these steps. \\n(1) Binarization: Convert index (j) of segment sij into a se-\\nquence of binary numbers. Let the sequence be stored in ar-\\nray xij[]. If necessary, several “\\n0”s are complemented on to \\nthe head of the sequence to ensure that the length of xij[] is θi. \\nThen xij[p] is 0 or 1 ( pi ∈ [1, θi]). For example, θ1=2, x12=[1,0]; \\nθ2=8, x25=[0,0,0,0,0,1,0,1] . \\n(2) Grouping: Convert xij[] into yij[] whose length is θ. Let \\n“+” be the string concatenation operator, then ∀ p ∈ [1,θ], yij[p] \\n= xij[(p·θi /θ)] + xij[(p·θi /θ)+1]+ xij[(p·θi /θ)+2]+...+ xij[(p·θi \\n/θ)+ θi /θ], For example, let θ2= 8 and θ=2, then y25=[0000, \\n0101]. \\n(3) Interleaving: Interleave the segments of each dimen-\\nsion. Let array z[] contains n selected segments indexes for \\ndimension, respectively, then the index of a cell determined \\nby z[n] is { y1(z[1])[1]+ y2(z[2])[1]+…+ yn(z[n])[1]}+ {  y1(z[1])[2]+ \\ny2(z[2])[2]+… + yn(z[n])[2]}+…+{ y1(z[1])[θ]+ y2(z[2])[θ]+…+ y \\nn(z[n])[θ]}. Therefore： cell_index=  . \\nθ=2\\nθ1=2\\nθ2=8\\nS12\\nS25\\nx12= [1,0]\\nx25=[0,0,0,0,\\n        0,1,0,1]\\n(1)\\n(1)\\ny12=[1,0]\\ny25=[0000,0101]\\nz=[2,5]\\n(3)\\n(3)\\nindex=1000000101]\\n5 is 101\\n2 is 10\\n(1) Binarization (2 ) Grouping (3 ) Interleaving\\n(2)\\n(2)\\n \\nFig. 2. Example of (1) Binarization, (2) Grouping and (3) Interleaving \\nWe introduce the concept of b inary length  for dealing \\nwith an irregular n-dimensional key- cube, otherwise each \\ndimension should contain the same number of active seg-\\nments, and has to expand all the dimensions synchronously. \\nThe complemented “0” in the head of  sequence only con-\\nsumes the address space but not storage.  \\n \\nFig. 3. Example of flat Z-order \\nThe cell indexes for a 4× 8 key-cube (both in decimal and \\nbinary) are shown in Figure 3 as an example. A nd in this \\ncase, the interleaving ratio is 1:2. Interleaving the binary co-\\nordinate values yields binary Z-values, and connecting the Z-\\nvalues in their numerical order produces the recursively flat \\nZ-shaped curve. \\nFunction g(): Specifies segments of each dimension, then \\none or more cells are determined.  Let m be a segment map \\ncontaining active segment indexes for n dimensions. A func-\\ntion g(m) returns cell indexes c[] mapped by m. Function g() \\nis implemented according to the flat Z -order. The size of c[] \\nis \\nvalues i m\\nn\\ni\\n].[\\n1\\n0\\n\\uf02d\\n\\uf03d\\n\\uf050 . \\nDefinition 11 Cell Address and Address Tree . Given a cell, \\nits cell address is a group of physical directories in which the \\ncorresponding data is contained. An address tree is an ordered-\\ntree associated with a key-cube. It leverages the longest common \\nprefix naming scheme to map a cell index to a cell address. \\nA quad-tree is a tree data structure in which each internal \\nnode has exactly four children. An address tree has a maxi-\\nmum of θ levels, from level 0 to level θ-1. A θ-levels address \\ntree is similar to a quad -tree e xcept that each node in the \\nlevel (θ-2) has \\n) (\\n1\\n\\uf071 \\uf071i\\nn\\ni \\uf03d\\n\\uf050  children; then an internal node rep-\\nresents the longest prefix of its children’s indexes, and a leaf \\nrepresents a cell index. So that for an arbitrary address tree, \\nan internal node maps to the parent directories of the cell \\naddress, and a leaf maps to the cell address. An address tree \\nis initialized as θ levels at most, or less levels for reducing its \\nsize, according to the scale of key-cube. \\nFunction r(): Given an address tree and cell indexes, de-\\nnoted as array c[], function r(c[]) returns physical directories \\nthat contain the data files of the cells in c[]. In r(), the directo-\\nries are addressed by comparing the prefix of the cell index \\nwith nodes. Since the indexes of adjacent cells are continu-\\nous, they are addressed together. Assuming the indexes in c[] \\na\\nre sorted, then when c[i] is addressed, the traversal path \\n(node list from the root to the matched node) is cached, then \\nc[i+1] is addressed along the path inversely. The address tree \\ncontains θ levels at most, so if n cells are addressed one by \\none, the time complexity is O(θn), and the time complexity of \\nr() is no greater than O(θn). The algorithm is as follows:  \\n \\n\\uf0e5\\uf0e5\\n\\uf03d \\uf03d\\n\\uf071\\n1 1\\n])[ ( ] [\\np\\nn\\ni\\ni z i p y'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 7 \\n \\n \\n \\nFigure 4 shows the example of address tree of Figure 3. In \\nFigure 4, the selected cells in the left ma p to the node of ad-\\ndress tree in the center, and the node maps to the directories \\nin the right. I n Figure 4, a cell address contains only one di-\\nrectory, but in practice it would usually contain more. All the \\ndescendants of a node have a common prefix that is associat-\\ned with the node, and the root is associated with a computer.  \\n[0000]\\n...\\n[11]\\n[01]\\n...\\nPhysical Directories \\n(partly)\\n4\\\\\\nNode n\\nds\\\\\\nts\\\\\\n4\\\\\\n0\\\\\\n1\\\\\\n... ds\\\\\\nts\\\\\\n[10][00]\\n[0001]\\n[0010]\\n[0011]\\n[1000]\\n[1011]\\n[1100]\\n[1111]\\n...\\n...\\n[00001][00000] [10111][10110] [11111][11110]\\n...\\nCell with indexes\\nAddress tree (partly)\\nFig. 4. Example of address tree \\n \\nFig. 5. Example of segment refinement and key cube expansion  \\n5 ACCUMULATION  \\nHaery improves query performance by providing the pre -\\ndefined, well -formed and well -partitioned key -cube. Key -\\ncube has two features: high-dimensions for supporting a vari-\\nety of keys (section 4 explains why the high -dimensions do \\nnot cause performance problems); accumulation for perfor-\\nmance and flexibility. The later one is called key -cube expan-\\nsion and explained in this section. Figure 5 shows the accu-\\nmulation of key -cube expansion by segment refinement and \\ndimension introduction. \\nOn one hand, as defined in definition 2 , segment refine-\\nment means segment s is divided into w sub-segments. Obvi-\\nously,  a dimension initialized as a root with only w active \\nsegments is too coarse to categorize massive data, so that, the \\ndata volume in a segment gets larger and larger when new \\ndata is imported, and th en, the segment is refined to sub -\\nsegments for reducing data volume in a segment. By this \\nmeans, the key-cube is expanded by segment refinement, for \\nexample the step (1) and (3) in Figure 5, s22 is refined to s27, s28, \\ns29 in step (1) and s33 is refined to s311 and s312  in step (3). The \\ntrigger of segment refinement is that both the data volume \\nand data dispersion are larger than given thresholds. Data \\ndistribution is measured by a sampling approach and five -\\nnumber summary (minimum, Q1, median, Q3, maximum). \\nOn the other hand, new keys (by default trivial keys) are \\nintroduced with new data meanwhile the trivial keys may no \\nlonger be “trivial”, and new a dimension is accordingly add-\\ned to the key -cube. So that the key cube is expanded by di-\\nmension introduction, for example the step (2) and (4) in Fig-\\nure 5. Dimension introduction is triggered according to the \\nfrequency of a trivial key occurring in rows. \\nDefinition 12 Key-cube Expansion. A key-cube expansion is a \\nprocess of creating a new key\\n-cube based on a exsiting one, by in-\\ntroducing dimensions (new keys or trivial keys are upgraded as \\ndimensional keys), or by refining active segments of dimensions \\n(one or more active segments refine to w sub -segments), or by \\nboth. Expansion is the key feature of the accumu lative key-cube \\nwhich improves both flexibility and query efficiency. \\nDefinition 13 Key-cube Version. The version of a key-cube is for \\ndistinguishing different key -cubes by their historical features. \\nEvery time a key -cube is expended, a new one is defined.  These \\nkey-cubes are numbered in a sequence order from  earliest to lat-\\nest, and their orders are their versions. The version of the initial \\nkey-cube is 0. \\nDefinition 14 Key-cube Backtrace. A key-cube backtrace is the \\nprocess of traversing from a later versioned key-cube to an earlier \\nversioned one, for comprehensive query on data partitioned by \\ndifferent key-cubes. The key-cube backtrace can be treated as the \\ninverse operation of key -cube expansion, by dimensio n reduc-\\ntion, or by segments consolidation (replacing the active segments \\nwith its parent), or by both. \\nFor example in Figure 5, steps (6) and (8) are segment con-\\nsolidation, and step (5) and (7) are dimension reduction. \\nFunction h. The function h(cube) returns the previous  ver-\\nsioned key-cube of the current one by querying the key -cube \\nmetadata. Notice that many concepts defined in the previous \\nsections associated with the key -cube, such as dimensions, \\nsegments, function f(), g\\n() and r(), linearization algorithm and \\naddress tree, these concepts of different versioned key -cube \\nare different too. Especially the linearization algorithm, when \\na new key- cube is expanded, the linearization algorithm and \\naddress tree are all changed, mapping new data to different \\ncell addresses. \\nAs far as the query is concerned, every key-cube expansion \\nis a segmentation on the new data in the future, however, \\nexpansion does not affect the existing data. When a query is \\nissued, the targets are both the current data managed by the \\ncurrent key -cube, and “historical” data managed by earlier \\nversioned key- cubes. In these cases, Haery adopts a two -\\n \\n1\\nd1\\nd2\\ns11 s12 s13 s1e\\ns21\\ns22\\ns23\\ns2e\\nd1\\nd2\\nd3\\ns1es11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns31\\ns32\\ns33\\ns3e\\n78\\n3\\n5\\n4\\n6\\nd3\\nd1\\nd2\\ns1es11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns31\\ns32\\ns33 s3e\\ns310\\ns311\\ns312\\n2\\ns1e\\nd2\\nd1\\ns11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns22\\nσ0 σ1 σ2 σ3 σ4'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n8 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nphase query strategy: first query all key -cubes and locate all \\ndata files as the search space; second query data in these files. \\nThe key-cube backtrace ensures the data files containing both \\ncurrent data and historical data are selected. The query per-\\nformance is dominated by the size of the search space, which \\ncan be greatly reduced by querying key -cubes. Key-cube ex-\\npansion ensures that the optimization effect of later versioned \\nkey-cube is the same, or even better, than that of earlier ver-\\nsioned ones, and the optimization effect of earlier versioned \\nkey-cubes remains unchanged. Query performance is im-\\nproved by the accumulation strategy.  \\n6 QUERY \\nThe key-cube not only benefits queries on values (conditions) \\nby reducing the search space, but also benefits queries on \\nkeys (targets) by well -managed dimensional keys. And, due \\nto the accumulative key -cube, performance optimization is \\nstill in effect whe n new data is imported. In this section the \\nquery process is introduced. The query process of Haery in-\\ncludes query on key -cube and query on files, and the former \\nincludes backtrace on key-cube as mentioned in section 6. \\nDefinition 15\\n Query in Haery . A query in Haery includes two \\nparts, targets and conditions. Targets is a list of keys, and condi-\\ntions includes keys and query ranges. Both targets and condi-\\ntions contain dimensional keys and trivial keys. The key -cube \\nimproves query performance if keys, no matter if they are in con-\\nditions or in targets, are dimensional keys. To reduce the search \\nspace, a key, if it is not contained in conditions but only in tar-\\nget, is treated as a special condition whose range is < -∞,+∞>. \\nQueries are denoted as: \\nquery={<key, ran ge, is_target >}={<k 1, α1, β1,true>,<k2, α2, \\nβ2,true>, … , <kc, αc, βc, false >} \\nFor example, a query “\\nselect a, b from table where a∈(0, 8] \\nand c∈(0, 10] ” is represent as {< a, 0, 8, true>,< b, -∞, + ∞, \\ntrue>,<c, 0, 10, false>}. \\nThe querying process is explained by the following steps:  \\n(1) Initial query on the latest key cube; \\n(2) Backtrace and query each versions of key-cubes;  \\n(3) Query on chunk files in a distributed and parallel man-\\nner.  \\nSteps (1) and (2) are to reduce the search space (queried \\nfiles), which can greatly improve the performance. Algorithm \\n2 explains the details of steps (1) and (2). Haery support dif-\\nferent computing frameworks which have different machines \\nor grammars to implement step (3), such as Hadoop Map Re-\\nduce. Given the diversity of implementations, details of step \\n(3) are abbreviated. \\n \\n \\n7  ARCHITECTURE \\nIn this section, the software architecture of Haery is explained \\nas in Figure 6\\n. Haery is a query focused column-oriented store \\nwhich is based on two i nfrastructures: a distributed compu-\\nting framework and a distributed file system. Upon these, \\nseveral modules collaborate to implement the key -cube’s \\nmanagement and query engine, they are Cube -base, Proxy, \\nExpansion, Partition, Placement, and Calculation. T he source \\ncode of Haery is available on https://github.com/CloudLab-\\nNEU/Haery. \\nCube-base\\n Proxy\\nCalculation\\nSpark or MapReduce\\n1.cube metadata\\n3.cube \\ninformation\\n4. generate tasks\\nnew  cube\\n5. return\\nresults\\n      Haery core Computing \\nFramework\\nPartition\\nPlacement\\nHDFS\\nExpansion\\nData loading\\nData loading Data query\\n2. query  request \\nStorage\\n \\nFig. 6. Software architecture of Haery  \\n•Cube-base: The metadata is stored in the Cube-base module \\nwhich is implemented by an in -memory database on the \\nmaster node. \\n•Proxy: When Proxy receives a query from clients, it analyzes \\nand validates the query by referring to Cube -base. Proxy \\nthen submits the query statement to Calculation, and waits'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 9 \\n \\nfor the results. Finally, Proxy receives the data from Com-\\nputing Framework, generates the result view, and reports to \\nthe client. \\n•Calculation: Calculation is the core of Haery that imple-\\nments algorithm 2 in section 6. \\n•Expansion: Expansion is a trigger for key -cube expansion \\nand implements the algorithms and strategies mentioned in \\nsection 5. \\n•\\nPartition and Placement: Partition horizontally and vertical-\\nly partitions rows of a cell into several chunk files and \\nPlacement places the chunk files in nodes.  \\n• Computing Framework. Haery is technically compatible \\nwith many distributed computing frameworks such  as \\nMapReduce and Spark. Currently only MapReduce is sup-\\nported, however, it can support other computing frame-\\nwork if the algorithms  mentioned in section 6 are imple-\\nmented using the framework. \\n•Storage. Key-value data is stored in a distributed file system, \\nHDFS for example. \\nWe briefly introduce the storage mechanism of Cube-base \\nand Storage. In a traditional data cube, a multi -dimensional \\narray is the uniform form of storage and the storage cost is \\nexpensive. In Haery,  the cube is not stored but calculated.  \\nEach cell contains no data but links to the physical directories, \\nand cell indexes are calculated, not stored. An address tree is \\nan in-memory data structure. For each segment, only the ac-\\ntive segment indexes are stored. For each dimension, only the \\ndimension index, dimension name (for query conditions), and \\nrange of values represented as an ordered tuple <index, name, \\nα, β>, are stored. For each key-cube, only its version is stored. \\nAll the data mentioned above is Haery’ s key-cube metadata.  \\nIn a traditional a column-oriented store, storing data with \\nits schema provides flexibility but waste s space. In \\nHaery, \\nstorage is saved by separating the schema and data through \\nthe uniform key-cube, and flexibility is ensured by accumula-\\ntion. A  data set is horizontally partitioned into dimensional \\nchunk files and trivial chunk files , implemented as Hadoop \\nArrayFile and SequenceFile. A dimensional chunk file only con-\\ntains values of dimensional keys because names of which are \\nfixed, ordered, and modeled as dimensions in the key -cube. \\nThey are inferred by their orders. A trivial chunk file contains \\nboth trivial keys and their values. \\n8  EXPERIMENTS \\nIn this section, we evaluate the core algorithms of Haery, and \\ncompare Haery with some popular NoSQL and relational \\ndatabases using generated datasets and various query work-\\nload.  \\n \\n8.1 Setup \\nScope. Verify the core algorithms, function g\\n() for lineari-\\nzation and function r() for addressing, proposed in this paper \\nin a standalone environment. Compare Haery with other da-\\ntabases from the point of view s of loading performance, que-\\nry performance and storage cost. \\nExperiment environment. We execute our experiments on \\na 24 node physical machine cluster. Each node has the same \\nconfiguration with a 1TB hard disk, 8 GB memory, 64-bit plat-\\nform, and moderate I/O performance, except for Intel Core 12 \\nnodes are i5 and the other 12 nodes are i7. The gigabi t ether-\\nnet is connected by a Dell PowerConnect 5548.  \\nExperiment Content. We conducted two experiments. One \\nin a standalone environment to evaluate the algorithms on \\nkey-cube. The other in a cluster environment to compare the \\nloading, querying and storage performance of Haery with \\ncompetitors. In Haery, there are four main alg orithms: func-\\ntions f, g\\n, h and r. On one hand, function f calculates segment \\nrange by its index, and function h returns the key-cube of the \\nprevious version. These two function are lightweight algo-\\nrithms which contribute little to query performance. On the  \\nother hand, functions g and r are heavyweight algorithms. \\nThe effectiveness of function g is dominated by the lineariza-\\ntion algorithm, so we firstly compared the performance of Z -\\norder and flat Z-order. We also evaluated the performance of \\nbuilding and querying the address tree, which dominates the \\neffectiveness of function r.  \\nIn the cluster environment, the relationship between load-\\ning performance and loaded data volume are compared and \\nanalyzed. Query performance under different queries and \\ndatasets, an d the storage cost, are also compared and ana-\\nlyzed. An extend YCSB [30] is adopted as the query client. \\nSelection of competitors. We compare Haery with Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nThe above competitors are commonly used data s tores in big \\ndata environments. The main difference between them is their \\ndatabase models and data schema. Table 3 compares their \\ndifferences. \\nTABLE 3. DESCRIPTION OF SEVEN COMPETITORS \\n \\nExperiment data. To highlight Haery’s advantages of \\nhigh-dimensionality, accumulation and efficient querying, \\nand to support  the key/value building processes in the next \\ntwo paragraphs, we adopt the generated data. There are three \\nattributes for the dataset scale: number of dimensions, data \\nvolume and number of segments on dimensions. We create \\nfive datasets of increasing scales, as explained in Table 4\\n.  \\nTABLE 4. DESCRIPTION OF FIVE DATASETS \\n \\nLoading Process. The loading process of Haery is carefully \\nplanned because otherwise the accumulative mechanism does \\nnot work. The volume and number of dimensional keys of the \\ndatasets increase from S1 to S5 gradually, so that both values \\nand keys of the larger dataset contains the small er one, and \\nthe query workload is performed on all datasets. Haery ex-\\npends the key-cube when the data and dimensions increases. \\nThe loading processes of other system s follow their default \\nconfigurations. The reason that we use YCSB with a deeply'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n10 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nextended loading module is that the original one does not \\nsupport such an accumulative mechanism.  \\nKey Building. To highlight the accumulative key-cube, the \\nnaming rule of key s is carefully planned. First, datasets are \\naccumulated from S1 to S5, where the larger one contains keys \\nwhich have occurred in the smaller one. Second, as shown in \\nTable 4, let x be the number of dimensional keys, and there \\nare 0.2x trial keys, so that the dimensional key s are named as \\n“K0” to “K(x -1)”, while the trivial keys are named as “K(x)” \\nto “K(1.2x-1)”. By such naming convention, the trivial keys of \\na small dataset overlap the dimensional keys of a larger da-\\ntaset. For example, for S1, the dimensional keys are “K0” to \\n“K9”, and trivial keys are “K10” and “K11”. When the dataset \\nincreases in size to S2 whose dimensional keys are “K0” to \\n“K19”, and trivial keys are “K20” to “K23”, the trivial keys of \\nS1 expend to become dimensional keys in S2. For Hive and  \\nPostgresXL, a predefined schema is required before data is \\nloaded, so we create a table with columns “K0” to “K199” , \\nand let columns “K13” to “K199” be nullable. For HyperDex, \\nwe built five hyperspace for five datasets, each of them con-\\ntains keys of S1 to S5, respectively.  \\nValue Building. For key-value pairs of a row in any da-\\ntasets, there are only two distinct values, one is a random \\nfloat from 0 to 106, named as the numeric value, the other is a \\nrandom 10-characters sequence, named as the textual value. \\nThe values of any  keys are either the numerical one or the \\ntextual one, and the chance of being numeric or textual is fif-\\nty-fifty. Without losing generality,  K0 and K1 are numeric \\nand textual values generated by the YCSB default data gener-\\nation algorithms. According to these, for any datasets, t here \\nare only 2 independent dimensions  and the others are dupli-\\ncation, while equal-width segments on a dimension are also \\nequal-frequency due to  data being uniformly distributed \\nacross its range. Therefore, if the query ranges cover half of \\nthe numeric values and the textual values, respectively, \\nnamely half active segments on each dimension are selected, \\nas a result, 25% (1/22) rows are returned. \\nQuery workload. The query workload is generated by \\nYCSB using the customized Workload E (short ranges)  be-\\ncause Haery focus es on range query but not insert, update, \\nread-by-row-key, or other workload s. There are two attrib-\\nutes of a range query relevant to our experiment: number of \\nqueried dimensions and number of matched segments. In \\nHaery, dimensions which are not in the query conditions, are \\ntreated as ALL (all active segments are m atched). It means \\nthat querying on less dimensions does not means less calcula-\\ntion on key cube.  Therefore, all dimensions are contained in \\nthe query conditions. The matched segments could be more \\nor less. In the customized workload E of YCSB, we define that \\nEi is a query whose conditions covers 1/2i active segments of \\neach dimension, and due to the data distribution and value \\nduplication, the query rate of Ei is (1/2i)2. We implement E1 to \\nE6 because there are at least 64 segments in a dimension, and \\nthe query rates are from 0.25 (1/22) to approximate 0.00025 \\n(1/2\\n12). For other data  stores, the query workloads are the \\nsame as for Haery. \\n \\n8.2 Algorithms on key-cube \\nLinearization \\nWe compared the performance of flat Z-order and Z-order \\nusing different number s of dimensions. The datasets are \\nshown in Table 2 and the average linearization time of the \\ntwo algorithms are compared in Figure 7. As is analyzed in \\nsection 4, flat Z -order and Z -order have the same time com-\\nplexity; the experiment shows their performance is almost the \\nsame. \\n \\nFig. 7. Linearization time of flat Z-order and Z-order \\nFigure 7 shows that the performance of the proposed flat \\nZ-order is almost the same as that of Z -order. When more \\ndimensions are involved, the linearization time of flat Z-order \\nis slightly longer than that of Z -order. With the tiny cost, flat \\nZ-order improves flexibility by allowing the key-cube to be an \\nirregular cube, as mentioned in section 4. \\nAddressing \\nWe analyzed the performance of building an address tree, \\nand addressing ( querying) via an address tree. As per the \\nexplanation in section 4, it is not necessary for the addressing \\ntree to be a full -tree whose leaves are mapped to the finest -\\ngranularity cells. The level of the addressing tree also does \\nnot need to be the maximum  one, but according to the num-\\nber of directories. According to definition 11, an address tree \\nis a quad -tree li nked structure, and the θ-level address tree \\napproximately maps 22(θ-1) directories. A computer with Linux \\nmanaging about 2 15 \\ndirectories is suitable for quick storage \\nand search [31], so that a 9-level address tree, which means 216 \\nmapped directories, is enough for indexing all directories in a \\nnode. In this experiment, we evaluated the performance of \\nbuilding and addressing the address tree with 11,12,13,14 \\nlevels, which are suitable for clusters with 32 (25=220/215), 128, \\n512, 2048 nodes, respectivel y. A large scale data center con-\\ntains thousands of servers, and a middle-size data center may \\ncontain hundreds of servers [32]. Thus the experiment scale of \\na 2048-nodes cluster is sufficient. The experiment results are \\nshown in Figure 8. \\n \\nFig. 8. Time cost of building address tree and addressing  \\nFigure 8\\n-(a) shows that the build cost of an address tree \\n(logarithmic axis) is slight except when the number of levels \\n0\\n400\\n800\\n1200\\n1600\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\nLinearization  Time (s)\\nNumber of dimesions\\nZ-order\\nFlat z-order\\n1.8 \\n5.9 \\n32.5 \\n292.7 \\n1\\n10\\n100\\n11\\n 12\\n 13\\n 14\\nBuilding  Time (s)\\nNumber of Levels\\n(a) Buliding \\n0.039 \\n0.059 \\n0.075 \\n0.093 \\n0.026 \\n0.046 \\n0.056 \\n0.068 \\n0.00\\n0.05\\n0.10\\n11\\n 12\\n 13\\n 14\\nAddressing Time (s)\\nNumber of Levels\\nWithout locality\\nWith locality\\n(b) Addressing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 11 \\n \\nis up to 14. As the number of levels increases, the time of \\nbuilding the address tree increases exponentially because the \\nnumber of nodes in every level increases exponentially. Gen-\\nerally, it is definitely sufficient that the master node, on which \\nthe addr ess tree resides, manages 2048 nodes . Building an \\naddress tree in about 4 minutes is affordable. The cost of \\nbuilding the address tree only happens once, and is counter-\\nacted by the query optimization. \\nFigure 8-(b) describes the average addressing time unde r \\ntwo conditions, addressing without locality and addressing \\nwith locality. “ With locality” indicates that the next queried \\ncell is close to the previous one, and vice versa. In a ll condi-\\ntions, the addressing time is no more than 0.1 second s; it is \\nfast enough for big data queries. The addressing time is line-\\narly related to the level of the address tree. For comparison, \\nwhen the levels of the address tree grow, the time of address-\\ning without locality increases faster, and that with locality \\nincreases more s lowly. In terms of addressing with locality, \\nadjacent nodes can be addressed together since their indexes \\nare continuous. \\n \\n8.3 Performance and Cost of Haery \\nA. Loading \\nSince Hive writes data into HDFS directly without writing \\nmetadata during the loading process, its loading performance \\nis the same as that of HDFS, therefore we do not compare \\nloading performance of Hive with the others. Figure 9\\n shows \\nthe average loading speed of HBase, Cassandra, MongoDB, \\nPostgresXL and HyperDex  on datasets with different data \\nvolumes. In the loading experiment, Haery’s loading perfor-\\nmance on average is 1.5 , 9.8, 1.4 and 37.3  times higher than \\nHBase, Cassandra, MongoDB and HyperDex, respectively.  \\n \\nFig. 9. Comparison of loading speed among six stores \\nAll of these data stores, except HyperDex  employ batch \\nloading. The batch loading process consist s of four steps: (1) \\nuploading the original data files to the nodes; (2) reading the \\noriginal data files and writing  schema-related metadata; (3) \\nsharding\\n the data among nodes; (4) transfer ing original data \\nfiles to the system related file format. For these six data stores, \\nstep (1) is almost the same, Table 5 compares the remaining \\nthree steps and some details are emphasized as the following \\npoints. \\n(1) Compared to HBase, Cassandra and MongoDB, Haery \\nemploys a simpler loading approach, it transmits  the dataset \\nto the nodes, calculates the key-cube, and shards the original \\nfiles to chunk files on each node by a MapReduce task. The \\nformat of chunk files is lightweight because dimensional keys \\nare removed. Due to the metadata, key-cube and cell address, \\nare not stored data but are calculated results, there is no extra \\nwork required to maintain them. The other three stores have \\nto maintain more schema -related metadata . MongoDB also \\ntakes time to transfer TextFile format to BSON file format \\nwhich is complex. \\nTABLE 5. LOADING STEPS OF FIVE DATA STORES \\n \\n(2\\n) The loading speed of PostgresXL, a distributed RBDMS, \\nis faster and stable.  Not only the schema, but also the parti-\\ntion of the distributed table are predefined, and then the load-\\ning process is simpl y parallel insert operations on Postgres \\nnodes. Haery‘s loading speed is 0.834 times slower than Post-\\ngresXL. However, it is still comparable, and in the application \\nscenario of NoSQL database, the predefined schema is infea-\\nsible, while changing table schmea dynamically is not sup-\\nported by PostgresXL. \\n(3) As reported in Figure 4 in [24], HyperDex loads at least \\n4 times faster than Cassandra and MongoDB. However, it is \\nincomparable slow in our experiments. It loads 0.25 MB data, \\napproximate 2000 rows, per second. In [24], the loading per-\\nformance is not discussed, w e infer that the following two \\nreasons lead to the different results: First, a different dataset is \\nused. In [24], there are only 10 7 rows and each row only con-\\ntains 10 attributes; while in our experiments, there are a max-\\nimum 109 rows and e ach row contains 120 keys.  HyperDex \\nperforms an ordered hash function on every value of keys in \\neach row, and then hashes the row to the Hyperspace, finally \\nreplicating the object to each subspace. A massive dataset \\ndemonstrates how costly these calculation are. Second, Hy-\\nperDex does not provide batch loading tools so data is loaded \\nfrom the client machine to the servers row by row. An \\nacknowledgement of the previous load is required before the \\nnext one is executed.  \\n(4) The systems load data slower when more data is load-\\ned, but the downtrend is not significant. For Haery, it is be-\\ncause the cost of expansion and linearization when the data \\nvolume increases. For the other stores, it is because the cost of \\nstep (2) increases with the data volume. Besides, with HDFS, \\nthe cost of locating the file also increases with the data vol-\\nume. \\nB. Query \\nFigures 10 and 11 show the query time of each system on \\nthe test cases ( E1, E2, E3, E4, E5 and E6) using different datasets \\n(S1 to S5). Generally, the query performance of Haery is the \\nbest of the NoSQL database s, and is also better than Post-\\ngresXL in some cases. Query performance of HyperDex is the \\nworst. On an average of all cases, Haery is 4.57x, 4.23x, 3.55x, \\n1.79x, 1.82x and 120.6x faster than Hive, HBase, Cassandra, \\nMongoDB, PostgresXL and HyperDex, respectively.  \\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n24\\n 72\\n 120\\n 168\\n 216\\n 264\\n 312\\n 360\\nLoading \\nperformance\\n (MB/s)\\nData \\nvolume (GB)\\nHaery\\n HBase\\n MongoDB\\nCassandra\\n HyperDex\\n PostgresXL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n12 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\n \\nFig. 10. Query performance of the six data stores \\n \\nFig. 11. Query performance of HyperDex  \\nThe dataset size increases from 36 GB to 360 GB. And for \\nqueries with different query rates (E1 to E6), the query time of \\nmost stores increases with data volumes  except PostgresXL \\nand HyperDex, and also increases with the query rate s. It \\nmatches the common sense expectation of database queries: \\nthe query performce  is better when less data is queried, or \\nless data is returned.  \\nAfter carefully studying these stores, we believe some op-\\ntimizations of Haery benefits the query performance, such as, \\nthe linearization algorithm ensures that adjace nt cells are ad-\\ndressed together, and preferring calculation rather than look-\\nup table when mapping logical space (cell space) to the physi-\\ncal space.  Howerer, the dominant optimization is sharding \\neither by vertical partitions on keys, or by horizontal parti-\\ntions on rows, or both. We explain the results from this point \\nof view in Table 6. Some important points to consider are as \\nfollows: \\n(1) The query process of Haery is similar to Hive, but the \\nperformance is quite a bit better. Hive’s query performance is \\nclose to a “full scan” query. \\n(2) The query process of HBase and Cassandra are similar, \\ntheir vertical partitions benefit performance . However, the \\nhorizontal partitions do not provide a benefit due to the high \\ndimensional queries. \\n(3) Without vertical partitions, MongoDB partit ions data \\nK0 and K1 horizontally . For our test data set, partitions on \\nthese two keys provide a significant benefit  because the val-\\nues of the other keys are the same as these two, however, this \\nis not a general situation.  \\n \\nTABLE 6. COMPARING THE QUERY OPTIMIZATION \\n \\n (4) With b enfits of  predefined schema, PostgresXL skip s \\nthe dataset where the query keys are null, and then only the \\nincreased part of data is sca nned. Referring to Table 4, S1 has \\n36 GB data and 374M rows; S2 increases 36GB data and 187M \\nrows, S3 increases 72GB data and 187M rows due to new rows \\nhave more keys, S4 increases 144GB data and 187M rows, fi-\\nnally S5 increases 72GB data and 74M rows. From S2 to S4, the \\nnumber of increased rows are same. It is why query time on \\nS2 is less than that on S1, and query time on S2 to S4 are almost \\nthe same. So far we are not sure why query performance of S5 \\nis worse than that of S2 to S4 since only 74M rows in S5 are \\nscanned. Maybe the 120-keys table is too wide for PostgresXL. \\nThe performance advantage of PostgresXL is not a fair com-\\nparison to the NoSQL databases because the keys cannot be \\npredefined or predicted, and no mechanism indicates wheth-\\ner a key is absent or not ex pected in  the data files that are \\nscanned. Haery may kno w a key, as a dimensional one, is \\nabsent by querying a key -cube, but Haery does not know \\nwhether the absent one is a trivial key without scanning the \\ntrivial data files. \\n(5) HyperDex also rel ies on a predefined schema. In [24], \\nflexibility is explained to be supported by multiple hyper-\\nspaces in a table, but it does not explain how these hyper-\\nspaces are created automatically, namely schema expansion. \\nIn our experiments, we create five hyperspaces for S1 to S5, \\nwith 12, 24, 48, 96 and 120 keys , respectively. So essentially \\nqueries are performed on different “tables”. It may be the \\nreason why the query performance is related to the increased \\nrow number and query rate . For the same query rate, que ry \\nperformance of S1 is the worst, S2 and S4 are similar, and S5 is \\nthe best. Even so, its performance is not comparable to the \\nother stores. As we explain in Table 6, HyperDex cannot meet \\nthe requirements of high -dimensional queries on high -\\ndimensional data. \\nC. Storage \\nThe storage costs of each system are compared in Figure 12. \\nHaery costs the least storage and HyperDex costs the most. \\nThe storage cost s of Hive and Cassandra are similar to the \\nsize of the original data set. Taking dataset S5 (360 GB) for \\nexample, the storage cost of Hive, HBase, Cassandra, Mon-\\ngoDB, PostgresXL and HyperDex is respectively 2.9x, 4. 7x, \\n2.6x, 5.7x, 3.9x and 8.8x larger than that of Haery. \\n0\\n1000\\n2000\\n3000\\n4000\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(a) E1 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(e) E5 \\n0\\n400\\n800\\n1200\\n1600\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(c) E3 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(d) E4 \\n0\\n1000\\n2000\\n3000\\n4000\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(b) E2 \\nHaery\\nHive\\nHBase\\nMongoDB\\nCassandra\\nPostgresXL\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(f) E6 \\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\nE1\\n E2\\n E3\\n E4\\n E5\\n E6\\nQuery time (\\ns)\\nQuery Case \\nE\\nS1\\n S2\\n S3\\n S4\\n S5'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 13 \\n \\n \\nFig. 12. Storage cost performance on two cases of seven data stores \\nThe experimental results are explained as follows: \\n(1) As explained in the loading experiment, there is no ex-\\ntra data being stored in Hive. Hive directly adopts the origi-\\nnal file format of TextFile to store key -value pair, therefore, \\nthe storage cost remains the same as the original data set.  \\n(2) The storage strategy of Haery is as simple as that of \\nHive. Compared with Hive, Haery saves storage for two rea-\\nsons: First, dimensional keys are not stored because they are \\nfixed and can be inferred by their indexes; Second, ArrayFile \\nand SequenceFile save more storag e than TextFile. Therefore, \\nthe storage cost is even lower than the original dataset. \\n(3) HBase is column oriented. The keys and values are \\nstored as HFile, which contains data blocks and index blocks. \\nThe index block stores three levels of indexes: root index, in-\\ntermediate level index and leaf level index. Storing these ad-\\nditional indexes are a performance cost for HBase. Therefore, \\nthe storage cost of HBase is higher than the original data set. \\n (4) Cassandra is column oriented. The keys and values are \\nstored as SSTable, which contains data files, index files (key \\nindexes) and filter files. Cassandra does not introduce much \\nmetadata to SSTable, it also provides a compaction mecha-\\nnism which merges multiple SSTables into a new one. There-\\nfore, the storage cost of Cassandra is slight ly less than the \\noriginal data set. \\n (5) MongoDB is document oriented, using BSON as the \\nstorage structure. BSON is a binary -encoded serialization of \\nJSON-like documents. Not only are the same keys stored re-\\npeatedly, but also there are massive descriptors to be stored. \\nTherefore, the storage cost is approximately double that of the \\noriginal data set. \\n(6) The storage of PostgresXL on e ach node is a Postgres \\ndatabase. In Postgres, rows are stored in heap files. Heap files \\nare structured as a collection of blocks, each containing a row, \\nnamely a collection of values without keys. This saves storage \\ncompared to the original dataset. But, the system tables and \\nindex on \\nthe primary key take additional storage. Overall, the \\nstorage cost is larger than the original data set. \\n(7) HyperDex replicates the data for each subspaces of a  \\nhyperspace, also there are five  hyperspaces for a table corre-\\nsponding to five datasets . Therfore, HyperDex is the most \\nstorage consuming store. \\n9 CONCLUSION \\nThis paper presents the design, implementation, and evalua-\\ntion of Haery, a column oriented store for big data. Haery is \\nbuilt on Hadoop HDFS and a distributed compu ting frame-\\nwork, e.g. MapReduce. We proposed the following models \\nand algorithms:  \\n(1\\n) Key-cube, which is a high -dimensional data model as \\nthe logical schema of the keys and values;  \\n(2) An i mproved Z -order based linearization algorithm \\nand an address tree, to map the cells to directories;  \\n(3) Accumulation, which is a key-cube expansion approach \\nto maintain the efficiency of the key -cube when new data is \\nimported;  \\n(4) Query algorithms to imp lement queries on key -cubes \\nand physical storage;  \\n(5) The system architecture, components and implementa-\\ntion of Haery.  \\nOur experimental results show that the loading and query \\nperformance of Haery is the most stable and efficient. Due to \\nthe accumulative data models, the query performance in-\\ncreases slowly when the data volume increases. There is no \\nadditional storage cost in Haery, what’s more, the storage cost \\nof dimensional keys are also saved. There are some reasons \\nleading to these advantages.  \\n(1\\n) Haery adopts a pre-defined key-cube which both sup-\\nports high-dimensional data and partitions data into regions. \\nQuery performance improved by greatly red ucing the search \\nspace. \\n(2) With linearization and the address tree, no additional \\nstorage is required to store the key -cube, the mapping rela-\\ntionship between cells and directories are calculated rather \\nthan queried, and the locality of queried data is considered to \\nimprove the addressing performance.  \\n(3) Accumulation ensures that the well -partitioned key -\\ncube remains efficient when new keys and values are import-\\ned to Haery. \\nIn the future, we would optimize Haery as follows: dis-\\ncussing whether the indexes of cells should be compressed, \\nhow to compress them, and whether the compression affects \\nthe query performance. And propose more sophisticated data \\nstructures for the address tree. \\nACKNOWLEDGMENT \\nThis research is supported by the National Natural Science \\nFoundation of China (61672143, 61433008, U1435216, \\n61662057, 61502090, 61402090).  And the Fundamental Re-\\nsearch Foundations for the Central Universities  (N1616020 \\n03). We thank Shenqiang HU, a master student of Software \\nCollege, Northeastern University, for conducting experiments \\nin the revision that greatly improved the paper.  \\nREFERENCES \\n[1] Agarwal S, Priyusha M K. “A Transformation from Relational \\nDatabases to Big Data.” J. International Journal of Advanced Trends \\nin Computer Science & Engineering , 2015. \\n[2] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreos \\nand Samuel Madden, \"The Design and Implementation of Mod-\\nern Column-Oriented Database Systems\", J Foundations and \\nTrends® in Databases: Vol. 5: No. 3, pp 197-280. 2013 \\n[3] Morton, G.M.: “A computer oriented geodetic data base; and a \\nnew technique in file sequencing. ” Technical report, IBM Ltd., \\nOttawa, Canada.1966 \\n[4] Pramod J. Sadalage; Martin Fowler, \"4: Distribution Models\", \\nNoSQL Distilled, ISBN 0321826620, 2012 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 72\\n 108\\n 144\\n 180\\n 216\\n 252\\n 288\\n 324\\n 360\\nActual Storage\\n (GB)\\nData \\nvolume (GB)\\nHaery\\n Hive\\nHBase\\n MongoDB\\nCassandra\\n HyperDex\\nPostgresXL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14', 'file_type': 'pdf', 'file_name': '[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n14 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\n[5] Wu S, Jiang D, Ooi B C, et al. “Efficient B-tree Based Indexing for \\nCloud Data Processing ” J. Proceedings of the Vldb Endowment , \\n2010, 3(12):1207-1218.  \\n[6] Jagadish H V, Ooi B C, Vu Q H. “BATON: a balanced tree struc-\\nture for peer-to -peer networks ” Proc. International Conference on \\nVery Large Data Bases. VLDB Endowment, 2006:661-672. \\n[7] Wang J, Wu S, Gao H, et al. “Indexing multi-dimensional data in \\na cloud system ” Proc. ACM SIGMOD International Conference on \\nManagement of Data , SIGMOD 2010, Indianapolis, Indiana, Usa, \\nJune. 2010:591-602. \\n[8] Zhang X, Ai  J, Wang Z, et al. “An efficient multi -dimensional \\nindex for cloud data management ” Proc. International CIKM \\nWorkshop on Cloud Data Management , Clouddb 2009, Hong Kong, \\nChina, November. 2009:17 -24. \\n[9] Papadopoulos A, Katsaros D. “A-Tree: Distributed Indexing of \\nMultidimensional Data for Cloud Computing Environments ” \\nProc. IEEE Third International Conference on Cloud Computing \\nTechnology and Science. IEEE, 2011:407-414. \\n[10] Alternate indexed hbase implementation; speeds scans by add-\\ning indexes to regions rather secondary tables George L. Availa-\\nble on https://issues.apache.org/jira/browse/HBASE-2037. 2017  \\n[11] Indexed HBase. “An extnestion of HBASE core which support \\nfaster scans at the expense of larger RAM consumption. ” \\nhttps://github.com/gkulbak/ibase. 2017 \\n[12] Zou Y, Liu J, Wang S, et al. “CCIndex: A Complemental Cluster-\\ning Index on Distributed Ordered Tables for Multi -dimensional \\nRange Queries ” Proc. Ifip International Conference on Network and \\nParallel Computing. Springer-Verlag, 2010:247-261. \\n[13] Nievergelt J. “The Grid File: An Adaptable, Symmetric Multikey \\nFile Structure .” J. Acm Transactions on Database Systems , 1984, \\n9(1):38-71. \\n[14] Bentley J L. “Multidimensional binary search trees used for as-\\nsociative searching” J. Communications of the Acm, 1975, 18(9):509-\\n517. \\n[15] Guttman A. “R-trees: a dynamic index structure for spatial \\nsearching” J. Acm Sigmod Record, 1984, 14(2):47-57. \\n[16] Nishimura S, Das S, Agrawal D, et al. “MD-HBase: A Scalable \\nMulti-dimensional Data Infrastructure for Location Aware Ser-\\nvices” Proc. IEEE International Conference on Mobile Data Manage-\\nment. 2011:7-16. \\n[17] Liu Y, Liu Y, Kaikuo X U, et al. “SHG-Tree: An Efficient Index \\nStructure of Spatial Database ,“ J. Journal of Frontiers of Computer \\nScience & Technology, 2009. \\n[18] Dong Daoguo, Liang Liuhong, Xue Xiangyang. “VAR-Tree-A \\nNew High-Dimensional Data Index Structure .” J. Journal of Com-\\nputer Research & Development, 2005, 42(1):10 -17. \\n[19] Liang J J, Feng Y C. “CSA-Tree: an optimized high-dimensional \\nindex tree for main memory access ” J. Chinese Journal of Comput-\\ners, 2007, 30(3):415-423. \\n[20] Lejsek H, Asmundsson F H, Jonsson B T, et al. “NV-tree: an \\nefficient disk -based index for approximate search in very large \\nhigh-dimensional collections. ” J. IEEE Transactions on Pattern \\nAnalysis & Machine Intelligence, 2008, 31(5):869-883. \\n[21] Mior M J, Salem K, Aboulnaga A, et al. “NoSE: Schema design \\nfor NoSQL applications ” Proc. Data Engineering (ICDE) , 2016 \\nIEEE 32nd International Conference on. IEEE, 2016: 181-192. \\n[22] Vajk T, Deák L, Fekete K, et al. “Automatic NoSQL schema de-\\nvelopment: A case  study” Proc. Artificial Intelligence and Applica-\\ntions. 2013: 656-663. \\n[23] Escriva R., Bernard W., and Emin G . S. \"HyperDex: A distribut-\\ned, searchable key-value store.\" Proc. ACM SIGCOMM 2012 con-\\nference on Applications, technologies, architectures, and protocols for \\ncomputer communication. ACM, 2012. \\n[24] Escriva R., Bernard W., and Emin G. S. \"HyperDex: A distribut-\\ned, searchable key-value store for cloud computing.\" Computer \\nScience Department, Cornell University Technical Report \\n(2011). \\n[25] Chevalier M., El Malki M., Kopliku A., Teste O., Tournier R. \\nHow Can We Implement a Multidimensional Data Warehouse \\nUsing NoSQL?. In: Hammoudi S., Maciaszek L., Teniente E., \\nCamp O., Cordeiro J. (eds) Enterprise Information Systems. Lec-\\nture Notes in Business Information Processing, vol 241. Springer, \\nCham. 2015 \\n[26] Iqbal A, Wang H, Gao Q. “A Histogram Method for Summariz-\\ning Multi-dimensional Probabilistic Data ” J. Procedia Computer \\nScience, 2013, 19:971-976. \\n[27] Song J, Guo C, Wang Z, et al. “HaoLap: a Hadoop based OLAP \\nsystem for big\\n data.” J . Journal of Systems and Software, 2015, 102: \\n167-181. \\n[28] Asano T, Ranjan D, Roos T, et al. “Space Filling Curves and \\nTheir Use in the Design of Geometric Data Structures ” Proc. Lat-\\nin American Symposium on Theoretical Informatics. Springer -\\nVerlag, 1997:3-15 \\n[29] Mokbel M F, Aref W G, Kamel I. “Analysis of Multi-\\nDimensional Space-Filling Curves ” J. GeoInformatica, 2003, \\n7(3):179-209. \\n[30] Cooper, Brian F., et al. \"Benchmarking cloud serving systems \\nwith YCSB.\" Proc. the 1st ACM symposium on Cloud computing . \\nACM, 2010. \\n[31] Roger Nelson, “ How many sub-directories should be put into a \\ndirectory”. https: //stackoverflow.com/questions/494730/ how-\\nmany-sub-directories-should-be-put-into-a-directory. 2017  \\n[32] Kai H. , Jack D. , Geoffrey C. F.. “Cloud computing and distrib-\\nuted systems: from parallel processing to the Internet of things ” \\nM, Morgan Kaufmann Publishers Inc , 2011 \\n \\n \\n \\n \\nJie Song Ph.D, \\nassociate professor, \\nsongjie@mail.neu.e\\ndu.cn, his research \\ninterests include big \\ndata management  \\nand energy-efficient \\ncomputing. \\n  \\nHongYan HE  M.S \\ncandidate, her \\ncurrent research \\nfocuses on big \\ndata management. \\n \\n Richard Thomas  \\nPh.D., senior lec-\\nture, rich-\\nard.thomas@uq.edu\\n.au, his research \\ninterests include big \\ndata and software \\nengineering. \\n  \\nYubin BAO Ph.D., \\nprofessor, \\nbaoyb@mail.neu.e\\ndu.cn, his re-\\nsearch interests \\ninclude big data \\nmanagement. \\n \\n \\nGe Yu Ph.D., pro-\\nfessor, \\nyuge@mail.neu.edu.\\ncn, his research \\ninterests include \\ndatabase theory.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='Nhan Nguyen \\nBuilding an E-commerce Application \\nUtilizing Firebase Cloud service  \\nMetropolia University of Applied Sciences \\nBachelor of Engineering \\nMobile solutions \\nBachelor’s Thesis \\n28 March 2022'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='Abstract  \\nAuthor: Nhan Nguyen \\nTitle: Building an E-commerce Application Utilizing Cloud \\nservice \\nNumber of Pages: 33 pages + 1 appendices \\nDate: 28 March 2022 \\nDegree: Bachelor of Engineering \\nDegree Programme: Information of Technology \\nProfessional Major: Mobile Solutions \\nSupervisors: Antti Piironen, Principal Lecturer \\n \\nThe primary purpose of the thesis is to illustrate the way of establishing a serverless \\nbackend for the application by utilizing Firebase services. Besides, the thesis also \\ninvestigated the implementation and functioning of Firebase as well as its services. \\n \\nThe thesis is to establish a conceptual framework of Firebase services via an \\nexamination of Firebase documents and the implementation of a project utilizing \\nFirebase services and tools. Additionally, this research demonstrates how to create \\nFirebase project in the Firebase interface and how to integrate Firebase SDKs into the \\napplication. \\n \\nThe thesis project culminates in the creation of an e -commerce application prototype. \\nThe application’s objective is to enable individuals to purchase and sell things related \\nto their own demands. The E -commerce website exists as a practical tool in order to \\nhelp firms handle orders, receive payments and manage logistics. All key aspects of \\nthe working prototype application, in general, were successfully implemented, apart \\nfrom a few that were excluded for different causes. \\n \\nApplying serverless backend service such as Firebase brings a huge benefit to people, \\nespecially developers, in terms of the significant reduction of maintenance of the \\ntraditional backend. \\nKeywords: Firebase, React, web applications'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='Contents \\nList of Abbreviations \\n1 Introduction 1 \\n2 ReactJS 3 \\n2.1 React Virtual DOM 3 \\n2.2 Data Flow 5 \\n2.3 React Components 6 \\n2.4 JSX Syntax 6 \\n3 Firebase 8 \\n3.1 Overview 8 \\n3.2 Firebase Authentication service 9 \\n3.3 Cloud Function for Firebase service 11 \\n3.4 The Real-time Database services 12 \\n3.5 Firebase Hosting service 14 \\n3.6 Cloud Storage for Firebase service 16 \\n4 Implementation and result 18 \\n4.1 Project environment 18 \\n4.2 Project structure 19 \\n4.3 Elements of web application 20 \\n4.3.1 HTML 20 \\n4.3.2 Stylesheets CSS 18 \\n4.3.3 Javascript 19 \\n4.4 Firebase configuration 24 \\n4.5 Interface layout 25 \\n5 Deploying the website 29 \\n6 Testing 30 \\n7 Discussion 31 \\n8 Conclusion 33 \\nReferences 1'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='List of Abbreviations \\nAJAX: Asynchronous JavaScript and XML is a collection of client -side web \\ndevelopment tools for creating asynchronous web applications that \\nmay transmit and receive data without interfering with other \\ncomponents of the user interface. \\nAPI: The Program Programming Interface (API) is a collection of public \\nmethods and attributes that your application utilizes to communicate \\nwith other objects. \\nDOM: The Document Object Model (DOM) is an HTML and XML document \\nprogramming interface. It depicts the page so that programs may \\nalter the structure, style, and content of the document. It uses nodes \\nand objects to represent the document. \\nJSON: JSON stands for JavaScript Object Notation and is a lightweight data \\nstorage and transmission format. When data is transmitted from a \\nserver to a web page, this is often utilized. \\nNpm: Node package manager is a package manag er that manages the \\nexternal code dependencies of an application. \\nReact: It is a JavaScript framework and tool for creating user interface \\ncomponents. \\nRedux: It is a state container for JavaScript programs that is predictable. \\nREST: Representational State Transfer is a software architecture style that \\nestablishes web -based protocols for communication between \\ncomputer systems. \\nUI: User Interface refers to the area where people and machines \\ninteract.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='JSX: Javascript XML is a React syntax that enables HTML to be included \\nin JavaScript code. \\nMVC: Model–view–controller is a software design pattern that is often used \\nin the development of user interfaces.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"1 \\n \\n1 Introduction \\nThe web has progressed from serving static HTML content to serving application-\\nlike complex user interfaces that are sometimes as capable as their native \\nequivalents. By contrast, Web browsers have stayed relatively stable throughout \\ntime. \\nStatistics show that mobile devices accounted for 52.2% of total internet traffic in \\n2019. However, it takes an average of 22 seconds for a mobile landing page to \\ncompletely load. When you consider that 53% of users will abandon a mobile \\nwebsite if it takes mor e than 3 seconds to load, it  is no surprise that improved \\nmobile experiences are in high demand. \\nA huge percentage of web users have switched to native apps nowadays. This \\nsector has developed particularly rapidly in the e -commerce market, with a 54 \\npercent rise in 2017 alone. In exchange, spending time in browsers has declined \\ndramatically, and web developers are increasingly looking for methods to stay up \\nwith the experiences that native apps provide. \\nJavaScript is currently one of the most powerful web d evelopment programming \\nlanguages available. JavaScript allows programmers to create extremely \\nresponsive websites with dynamic functionality that responds to user requests, \\nenhancing the user experience. \\nReact generates a memory -based data structure cache.  When the code is \\namended, React detects the changes and updates the browser accordingly. This \\nunique feature will improve the webpage's efficiency since the React library only \\nrenders components that change, rather than loading the whole page as in other \\nmethods. That is why, in this thesis, React, an open-source JavaScript toolkit, is \\nutilized to create a user-friendly, scalable, and high-performance homepage.  \\nThe purpose of this thesis  is to create a working responsive e -commerce web \\napplication for an online retailer using React and Firebase t o assist businesses\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"2 \\n \\nin handling orders, receiving payments, and managing logistical processes and \\nprocedures. The React will handle the work in front -end, meanwhile, Firebase \\ncloud service helps manage the data and authentication. \\nThe thesis is divided into 8 sections. The sections of this thesis are organized in \\nthe same order as the phases of implementation of the project that was \\ndeveloped in this thesis. Sections 1 and 2 provide an overview of the technologies \\nthat will be used in the projects. The first section is devoted to ReactJs. The \\nsecond section delves deeper into Firebase. Additionally, the purpose of this part \\nis to analyze the advantages of using numerous services on Cloud Firebase. \\nFollowing the project's execution stages, Sections 4, 5, and 6 cover everything \\nfrom the design of the architecture to specific examples, all while discussing the \\ntechnologies that were employed to reach the ultimate result. Section 6 focuses \\non the testing procedure. Section 7 discusses the application's outcome and the \\nconclusion of the thesis. Finally, there are parts dedicated to observations on and \\ndebate of the whole endeavour.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='3 \\n \\n2 ReactJS \\nReact is a library for designing composable user interfaces. As a result, \\nreusable UI components are more likely to be created, and data that changes \\nover time may be shown. The V in MVC is often replaced with React. React \\ntakes away the DOM from you, enabling a simpler programming approach and \\ngreater performance. \\n2.1 React Virtual DOM \\nThe DOM is an acronym for Document Object Model. It is commonly referred to \\nplay an important role of the contemporary internet nowadays. It is basica lly an \\nabstraction of how online documents look and work. However, it is not as fast as \\nother JavaScript operations since most of frameworks update not the part but \\nwhole the DOM without requirement. Although they are  not essential, these \\nupdates are made automatically even if you do not ask for them. Assume that \\nnine goods have been placed in a shopping cart at an online web store. For \\nexample, changing a name in a list of (n) names would cost (n+1) times harder \\nfor a normal technology to recreate exact the same amount of names in that list. \\nThe Virtual DOM is existed before the creation of React, however, React makes \\nit accessible to developers for free. Virtual Dom is technically representation of \\nthe HTML Document Object  Model. For any DOM object, for instance a \\ncorrespondent or a light copy .  React has a virtual DOM object  of its own . The \\ncharacteristics of a virtual DOM are comparable to those of a real DOM. On the \\nother hand, it is unable to alter the perspective immed iately. It takes a long time \\nto manipulate the DOM. Update of Virtual DOM, on the other hand, is quicker \\ncomponent as well as does not alter the screen.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='4 \\n \\n \\nFigure 1. How the DOM working in React [1] \\nAs shown in Figure 1, the virtual DOM in React is a duplicate of the actual DOM \\nthat is used for testing. React employs a technique known as \"diffing,\" which \\nmeans that when a JSX element is rendered, every single Virtual DOM element \\nis changed. This may seem to be inefficient, however, it is not since Virtual DOM \\nis very quick to update and has no influence on the overall process. In order to \\ndetect which virtual DOM has been altered, React compares an updated state of \\nthe DOM with an earlier state of the DOM. When React recognizes that the DOMs \\nhave been modified, it only updates the objects that have been modified to the \\nactual DOM. \\nAs a result, React accelerates the updating process by using Virtual DOM. In the \\nabove example, React would have just updated the changed item, leaving the \\nother things unchanged. When changing a page in an application, this makes a \\ndifference since React may only make modifications to the areas of the DOM that \\nare necessary. A large part of the rationale for React increasing popularity within \\nthe developer community is due to this virtual DOM modification mechanism. \\nWhen utilizing React Virtual DOM, there are benefits and drawbacks to consider. \\nThe use of JSX and hyper script enables us to create multiple frontends for a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"5 \\n \\nsingle application; allow the creation of apps without having to consider state \\ntransitions; no need for React to be used simple and effective in raising output; \\nReact diffing techniques are very quick and efficient are just a few of the many \\nadvantages of the ReactJS library. Mean while, React has a few drawbacks, \\nincluding a lack of differentiation between static and dynamic components and \\nthe use of a substantial amount of memory. In memory, a full copy of the DOM is \\npreserved. \\n2.2 Data Flow \\nFramework, for example, like Angular use tw o-way data binding. When using \\nAngular's two -way data binding, for example, changing the model instantly \\nupdates the view and the other way around. An input field in the model may also \\nalter the model. It works well in the majority of cases, however switch ing to a \\ndifferent model may trigger cascade changes in other models. Again, because \\nthe state is modifiable by both view and controller, the data flow might be \\nunexpected in certain instances. Flux or Redux with React might be a better \\napproach to prevent such uncertainties as both designs follow one-way data flow. \\nCascading updates and modifications are not visible in a one-way data flow.  \\nAn application's states and models may be more tightly coordinated when data is \\nsent in a single path across the syst em. Additionally, a one -way data flow \\nsimplifies and clarifies the design. Flux architecture is a functional approach. In \\nthis case, the view is seen as a result of the current state of the application. \\nEventually, if the state receives some modifications the view also gets re -\\nrendered 16 automatically. In addition, the states provide a comparable \\nperspective, which aids in the application's comprehension and predictability. \\nData from parent to child in an application flow in a single path to increase \\npredictability. Any data may be updated from any view, anytime under this \\ntechnique. This also simplifies debugging if anything goes wrong.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='6 \\n \\n2.3 React Components \\nComponents are critical in React. To form the full user interface, these individual \\ncomponents are then layered within one another. When using components, it \\nbreaks up user interface into smaller, more manageable sections so that \\ndeveloper can focus on each one individually. The term UI refers to the user \\ninterface, or what is shown on the screen. Components  operate similarly to \\nJavaScript functions. They execute the exact same goal, but in quite distinct \\nenvironments and with very different techniques. They, like functions, accept \\nprops as inputs and return React components. These components define what \\nthe user sees on the screen while interacting with the interface. React \\ncomponents may be used to build a whole interface or simply parts of it. \\nReact components may be built in the same way that a JavaScript function is. \\nThis method takes props as arguments and returns text of html which normally is \\ncalled as JSX. These components are referred to as functional components. \\nAdditionally, a React component may be generated in a variety of various \\nmethods. Extending, inheriting, or deriving a class from  the primary component \\ntied to an object is another method of creating a component. [2] \\nAdditionally, stateless functional components are possible. By rendering each \\ncomponent, the user interface experience becomes more responsive and \\nefficient. \\n2.4 JSX Syntax \\nJSX is neither a string nor a hypertext markup language. It is a JavaScript \\nsyntactic extension with statically typed syntax. It is comparable to an object -\\noriented programming language that is optimized for use with current web \\nbrowsers. To design and develop the user interface, it is advised to utilize JSX in \\nconjunction with React. While it has all of the capabilities of JavaScript, it may'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"7 \\n \\nseem to be a template language at first look. JSX is used to generate the React \\nelement. It is also capable of being displayed to the React Virtual DOM. \\nTo begin, it is faster: Because the JSX source code is converted to JavaScript, \\nthe outcome is highly optimized. When compared to similar JavaScript code, JSX \\nproduced code is quicker. JSX has been shown to be 13 % quicker on iOS and \\n29% faster in Android. \\nSecond, it is more secure: JSX is statically coded and largely typesafe, in contrast \\nto JavaScript. When programs are created with JSX, their quality improves \\nsignificantly, since many problems are discovered dur ing the compilation \\nprocess. Additionally, it provides debugging capabilities at the compiler level.  \\nTo round things up, JSX provides a class structure that is extremely comparable \\nto Java, which means that developers no longer have to deal with JavaScrip t's \\nbasic prototype-based inheritance system. However, since the expressions and \\nstatements in JSX are almost comparable to those in JavaScript, programmers \\nalready familiar with JavaScript may begin using it right away. Additionally, there \\nare proposals f or language -services for editors / integrated programming \\nenvironments (IDEs), such as code completion to make coding easier. \\nTo assist with the presentation of nested components, JSX elements may be \\ngiven as children. Different kinds of children may be co mbined, allowing for the \\nusage of JSX children with string literals. This is another JSX attribute that \\ncorresponds to an HTML element. [3] \\nMultiple children may be added to a JSX expression. Therefore, it must be \\nincluded in a div if the component is requ ired to render several items. Within \\nenclosing, JavaScript expressions may be passed as children.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"8 \\n \\n3 Firebase \\nFirebase developed from Envolve, a former firm started by James Tamplin and \\nAndrew Lee in 2011 offer a backend development solution. The feature \\ncombination in Firebase speeds the integration of cloud databases into online \\nand mobile applications. Firebase is a Backend as a Service platform that \\nsignificantly lowers configuration and setup time. \\n3.1 Overview \\nFirebase is a Cloud -hosted, NoSQL database tha t employs a document -model. \\nWhile allowing for real -time data sharing and syncing across users, it can be \\nscaled horizontally. This is ideal for cross-platform apps, such as those for mobile \\nphones. In addition to server -based applications, Firebase's robu st user-based \\nsecurity makes it ideal for off-line usage. \\nFirebase is a self-scaling service built on top of Google's infrastructure. Firebase \\nadds analytics, authentication, performance monitoring, messaging, and crash \\nreporting to the features of a typic al NoSQL database. The integration is \\nextensive, given it is a Google product. Additionally, Google Ads, the Google Play \\nStore are also integrated with AdMob. \\nCreating a Firebase application from the ground up is not difficult. As seen in \\nFigure 2, the app lication may be classified as developing, growing, or earning. \\nDevelopers are able to make their own selections to use these pillars. Certain \\ntechnologies, such as A/B Testing, Analytics, App Distribution are provided free \\nof charge.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='9 \\n \\n \\nFigure 2. The overview of Firebase Cloud service [4] \\n3.2 Firebase Authentication service \\nCurrently, many online services rely on authentication in order to identify users \\nas well as safeguard data by limiting access. For example, Google requires an \\naccount and password to access some programs. The authentication process \\nhas been complicated by the addition of a third -party authentication mechanism \\n- APIs. Thus, A simple API that allows users to log in through federated providers \\nis provided by Firebase to complete the authentication process. It also works in \\nconcert with the real-time database to limit access, as demonstrated in Figure 3.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"10 \\n \\n \\nFigure 3. Firebase Authentication service [5] \\nGoogle, GitHub, Facebook, and Twitter are all popular federated providers. If \\ndevelopers alter Firebase, they do not need to sign in again, since it is already \\nconnected with these providers' authentication systems. A connection to the \\nauthentication system will be establ ished as soon as an account has been \\nvalidated in the database. \\nPassword recovery and user verification are both available via the Firebase user \\ninterface. Users may create a temporary account without revealing their identity, \\nwhich is then linked to their  federated provider -based account. Additionally, it \\nsupports a technology named Smart Lock in remembering and signing in \\nautomatically using the credentials. The Firebase authentication SDK requires \\nthe user's email address as a sign-in credential.  \\nFirebase Auth requires the least amount of time to implement into your code. The \\nreal Firebase authentication project takes more time to develop than Firebase \\nAuth. It is an online library that enables clients to alter typical Firebase situations \\nin order to give user authentication implementations in the form of user interface \\nscreens. Firebase Auth supports a variety of authentication methods on the web \\nand on mobile devices and is very simple to use. There are three primary stages \\nto integrating Firebase Auth user authentication into apps such as configuring\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"11 \\n \\nsign-in methods, configuring the sign-in UI, and completing the sign-in flow using \\nFirebase Auth. \\nThe Firebase Authentication SDK is somewhat more time -consuming than the \\nprevious one. Additionally, it enab les authentication process verification over \\ncomplete management of the outlook. Eventually, this option will maintain a \\ncomprehensive authentication process that includes several stages to \\nauthenticate the user's account. It is used to perform a variety o f functions, \\nincluding sign-in and sign-out, automated log-in, and data updating for new users. \\nAdditionally, the Firebase Authentication SDK facilitates connection with identity \\nproviders and anonymous authentication. Firebase Authentication is generally \\napproached via the Firebase Authentication SDK. There are the following steps \\nsuch as establish a sign -in method, create user interface flows for your login \\nmethods, and provide the user's credentials to the Firebase Authentication SDK. \\n3.3 Cloud Function for Firebase service \\nAs seen in Figure 4, Cloud Functions is a component of the Firebase and Google \\nCloud platforms that enables developers to handle fewer configurations, such as \\napp updates and general security for Google services and Firebase products. A \\nCloud function connects the Firebase application to the Google server, which \\nexecutes it. It minimizes boilerplate code by generating an admin SDK, third-party \\nservices, and a Cloud Function. The command is used to scale the design's \\nusage automatically depe nding on the computing data. As a result, human \\nmaintenance is not required for updating settings, credentials, or \\ndecommissioning. Cloud Function assumes users' security and privacy \\nobligations. Following the addition of a command line, the Cloud Function  is \\nisolated from the application logic, allowing the client to freely deploy code on the \\nprimary server. Since a result, developers choose to master their application, as \\nthese are delivered through Google and Firebase features.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"12 \\n \\n \\nFigure 4. Cloud Function for Firebase [6] \\nThe Firebase project is deployed with a specified provider to enable this \\ncapability. Then, using JavaScript, the Cloud Function and Firebase CLI are \\ninstalled to manage the desired services. Through the Firebase CLI, the Firebase \\nconsole may be used to control and see a function: create a Cloud Function, write \\nit, deploy it, and monitor it. \\n3.4 The Real-time Database services \\nTraditionally, real -time databases have been used to demonstrate how a \\ndatabase system may overcome the real -time limitation in order to create a \\nreliable database system. Due to the fact that it offers an easy API -based \\ncapabilities, this is Firebase's flagship service. When new information is added, it \\nis automatically synced as JSON across all devices. A real-time database is one \\nthat works with both web and mobile versions on the same device and exchanges \\ndata between devices in an extremely short period of time, whereas a batch \\ndatabase takes a long time to sync.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='13 \\n \\n \\nFigure 5. The example of Firebase real-time database. \\nIt restricts access to particular information by using a few codes provided by the \\nclient. No operation or maintenance server is necessary since the cloud service \\nprovides a real -time database that is ac cessible from anywhere (based on \\nNoSQL). The Real-time database, like the other products, is available for Android, \\niOS, Web, C++, Unity Platform, and JavaScript, in addition to the other platforms \\nmentioned above. There are up to 100 000 concurrent connec tions and 1,000 \\nconnections per second supported by each database in Real-time Database. \\nIn order to increase client access time, the data set is adjusted and improved due \\nto the use of a new database and integration of the real-time database with Cocoa \\nPods or Gradle, among other factors. Additionally, this technique allows for \\nwriting to take place even when the client is not connected, while still \\nguaranteeing client security and confidentiality. For the purpose of constructing a \\nFirebase Real-time Database, the following are the five basic stages: Configure \\nData and Listen for Changes, enable Offline Persistence, and safeguard your \\ndata by integrating the Firebase Realtime Database SDKs.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='14 \\n \\nIn order to do this, the Firebase SDKs platform, which communicates  with the \\nFirebase Cloud service, was established. Administrators have the ability to read, \\nprogrammatically send, access, and establish Firebase Auth via the \\nAdministration SDK. To understand how Cloud Storage works, consider the \\nfollowing steps: connect the Firebase SDKs for Cloud Storage, create a \\nreference, upload or download your files, and safeguard them. \\n3.5 Firebase Hosting service \\nWeb developers may use Firebase Hosting to host not just static websites, but \\nalso dynamic websites that are entirely built of HTML, CSS, and JavaScript files. \\nIn order to satisfy a range of needs, it is a software that maintains static web -\\nhosting services. Because HTTPS is based on SSL, consumers gain from faster \\nwebsite access, while developers profit from the ability to launch web applications \\nwithout registering an HTTPS connection certificate with the server. The Firebase \\nCommand Line Interface (CLI) allows for the creation and processing of Firebase \\nin a matter of seconds.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='15 \\n \\n \\nFigure 6. The flow of using Firebase Hosting [7] \\nWhen Firebase Hosting is used in conjunction with a contemporary website, the \\nmaterial is totally protected using zero -configure SSL. The chemicals are \\nreceived quickly since Firebase Hosti ng is cached on SSDs and CDN edges. \\nAdditionally, it has an undo feature in case of development errors, making \\nFirebase hosting a complete management solution. \\nTo run Firebase Hosting, the Firebase CLI is considered a full -featured solution \\nfor websites, a pplications, and Progressive Web Apps (PWA). The following \\nphases should be noted for front -end implementation of features, infrastructure, \\nand customized tooling: Install the Firebase command-line interface (CLI), create \\na project directory, and publish the site.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='16 \\n \\n3.6 Cloud Storage for Firebase service \\nVideos, images, and documents are stored online using Firebase Cloud Storage, \\nand they are backed up using Google Cloud Storage (which is free). The \\ninfrastructure for processing large files is created and mainta ined via the use of \\na clear API. As a result, the Realtime Database Rule enables for the distribution \\nof documents to certain individuals. This shows that since it is associated with \\nFirebase Authentication, the storage has been cached for faster access. \\nUploading and distributing files from mobile devices that are accessible to both \\nGoogle Cloud and Firebase is straightforward with the help of Firebase SDKs, \\nwhich are available on the Firebase website. Due to the automated scaling \\ncapabilities of Cloud Sto rage, there is no need to move the document to a \\ndifferent storage service provider. Depending on the file type, filename, or file \\nsize, declarative security for Firebase files may be given. Although the network is \\ndisqualified, Firebase SDKs continue to o perate in an unbroken manner; for \\nexample, if a video is not successfully uploaded, the movie will resume \\ntransmission from the point at which it was interrupted when a proper internet \\nconnection is obtained. \\n \\nFigure 7. An example of Cloud Storage for Firebase service'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='17 \\n \\nFor this purpose, the Firebase SDKs platform was created to communicate with \\nthe Firebase Cloud service, which was developed by Google. Firebase \\nAuthentication may be read, programmatically sent, and accessed via the Admin \\nSDK, which is available only in a privileged context. To demonstrate how Cloud \\nStorage works, the following procedures are taken: Creating a reference, \\nuploading or downloading your files, and safeguarding them are all possible using \\nthe Firebase SDKs for Cloud Storage.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='18 \\n \\n4 Implementation and result \\nIn this section, the essential phases of the development process and the results \\nproduced are described. A well performing website is constructed with five main \\npages. The Authentication page is the webs ite entrance that asks user \\nidentification, which is accomplished by provide username and password. \\nAuthentication is needed to access an entire site. The Homepage enables visitors \\nto do a variety of tasks, including seeing a list of goods with photographs, filtering \\nthe products by name as well as category, selecting the products which is \\ndisplayed in the Cart page. In general, the Order page summarizes all user -\\nplaced orders. Following that comes the Cart page. The Cart page enables \\nvisitors to browse the list of items. Administrator accounts enable users to control \\nall items, users, and orders from the admin page. \\n4.1 Project environment \\nPrior to developing the application, the environment must be configured. The \\nNodeJS version 13.12.0 is the first item on the list of the tools that are being \\nused here. Essentially, NodeJS is a tool that supports JavaScript on both the \\nclient and the server. Node.js is a ground-breaking platform that supports both \\nthe client and the server. \\nThe next tool is Visual Studio Code, which is one of the most popular software \\ndevelopment tools today since it is packed into a single graphical user interface \\nand includes certain snippets such as ES7 React/Redux to make writing easier \\nand more convenient. In addition, the Firebase CLI version 10.5.0 maintains, \\nviews, and deploys Firebase projects using a variety of tools, and from there, \\nwe can configure communication between the Firebase Cloud and a local \\nproject, which is the last step.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"19 \\n \\n4.2 Project structure \\nFigure 8 depicts the project 's structure, which is separated into three major \\nelements. The index.html file is the primary HTML file for our application, which \\ncontains your React code and serves as a container for React to render. The \\nproject's CSS is to style its apps and component s. The style property adds \\ndynamically calculated styles to React applications at render time. It takes a \\nJavaScript object instead of a CSS string. CSS files are divided into two major \\nfiles: index.css and App.css. App.js, the JavaScript file that contain s the other \\ncomponents, functions as a container for them, and redux is vital to the \\napplication's operation. The last portion contains the file fireConfig.js for \\nconfiguring Firebase.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='20 \\n \\n \\nFigure 8. The structure of project \\n4.3 Elements of web application \\n4.3.1 HTML/CSS and JavaScript  \\nThe HTML core section is seen in figure 9. This file includes the links to external \\nCSS sources, which is Bootstrap, in this project. The web application is assigned \\nthe “root” id.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='21 \\n \\n \\nFigure 9. The screenshot of HTML file \\nCSS specifies how HTML components should appear on screen, in print, or in \\nother media. CSS eliminates a great deal of labour. It is capable of simultaneously \\ncontrolling the layout of many web pages. In this appl ication, the stylesheets \\ninclude Index.css, App.css, authentication.css, layout.css as well as \\nproducts.css. \\n \\nFigure 10. The screenshots of product.css file'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='22 \\n \\nThe application was built by multiple parts which are Authentication, Homepage, \\nOrders and Cart. In terms of Authentication, there are two pages which are \\nrespectively Login and Register pages. They are the website entrance that asks \\nfor user identification, which is accomplished by providing username and \\npassword. Authentication is needed to access an entire site. The Homepage \\nenables visitors to do a variety of tasks, including seeing a list of goods with \\nphotographs, and filtering the products by name as well as category. Selecting a \\nparticular product link  to the product detail. The next one is the Order page. In \\ngeneral, the Order page summarizes all user placed orders. Following that comes \\nthe Cart page. The Cart page enables visitors to browse the list of items. \\nAdministrator accounts enable users to con trol all items, users, and orders from \\nthe admin page. Using the React -router-dom package ensures that pages are \\nnavigated accurately and quickly. \\nThe following list of additional components is shown in Figure 11. Considering \\nthe layout of the website, the re are three main parts which are respectively \\nFooter, Header and Layout. First, component Footer is used to display the footer \\nof the page. Secondly, component Footer is used to display the footer of the page. \\nLast, Component Layout is the custom componen t combine the Header, Footer \\ncomponents and the content to be passed to the Layout. Aside from that, the \\napplication was developed as part of a larger set of components. Component \\nLoader is simply the spinner which is displayed when loading resources. \\nComponent AdminPage, which is only visible to admin users, contains three \\nprimary sections: a list of users, a list of items that may be edited/removed, and \\na list of user orders.  Component CartPage displays the list of products ready to \\ncheck out. Component Homepage allows users to do a range of functions, \\nincluding seeing a list of items  with accompanying images and filtering products \\nby both name and category. Component OrdersPage is place that us er may \\nmanage their paid orders. Component ProductInfo, as its name implies, displays \\ninformation about the product, such as its name, category, and price. Component \\nLoginPage is the initial page of the website, user need a valid account with \\nusername and password. Component RegisterPage allows users register a new \\naccount with correct email, password and confirmation password.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='23 \\n \\n \\nFigure 11. The screenshot list of components \\n4.3.2 Redux \\nRedux is the one of most popular libraries to manage state of the application, with \\nfour pillars such Predictable, Centralized, Debuggable and Flexible. Predictable \\nmeans that Redux enables you to create apps that act consistently, are simple to \\ntest, and operate in a variety of settings. Centralized means t hat Adding \\nsophisticated features like undo/redo, state persistence, and other capabilities by \\ncentralizing your application\\'s data and logic is a great way to improve \\nperformance. Debuggable means that Redux DevTools, it is simple to see when, \\nwhen, why, and how your app\\'s state has changed in real time. Changes may be \\nlogged and debugged using \"time travel debugging\" in Redux\\'s design. Error \\nreports can also be sent to the server. Flexible means that Redux can be used \\nwith almost any UI layer and has a thriving add-on ecosystem.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='24 \\n \\nIn the application, we use redux to manage the state of products when loading, \\nadding and removing stuffs. \\n \\nFigure 12. The screenshot of redux file \\n4.4 Firebase configuration \\nIn the application, we use Firebase cloud services such as Authentication, \\nFirestore Database, Hosting. To set up the services in the application, we follow \\nthe following steps: \\n• Creating a Firebase project \\n• Installing the SDK and initialize Firebase \\n• Accessing the Firebase in the application'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='25 \\n \\n \\nFigure 13. The configuration file of Firebase in the application \\n4.5 Interface layout \\nThe screenshots below are of the application. The figure 14 is the homepage that \\nshows a list of goods and a search box for quickly finding things by name. \\nAdditionally, customers may filter items by category on the Homepage. By \\nchoosing certain goods, as seen in Figure 15, the detail page displays some \\nproduct information and a button to add the item to the basket. Once an order \\nhas been placed, the user may manage it via the Order page in figure 16. \\nFollowing the addition of products to the cart, figure 17 displays the list of chosen \\nitems and allows the user to check out the order. The Orders tab displays a list \\nof all previously placed orders in chronological order, allowing the user to quickly \\nexamine the total cost of the transaction as well as the pro duct details. For \\nauthentication, there are two pages. In figure 18, there is a login page, and there \\nis a register page. Register page is the only one where users are supposed to fill'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='26 \\n \\nin things like their username and password. The user can switch between  the \\nRegister and Login pages on their own. \\n \\nFigure 14. The Home page \\n \\nFigure 15. The product detail page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='27 \\n \\n \\nFigure 16. The Orders page \\n \\nFigure 17. The Cart page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='28 \\n \\n \\nFigure 18. The Login page \\n \\nFigure 19. The Register Page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"29 \\n \\n5 Deploying the website \\nIn order to get started, install the Firebase tools that will allow you to deploy \\nyour app, and then log into Firebase from your terminal to get things going. If \\nyou haven't already done so, you'll be prompted to enter your email address as \\nwell as your password to continue. It is also necessary to examine the root \\ndirectory of your React application. Your React application will now need the \\nuse of Firebase to be setup.  \\nAs we go, we configure through a series of questions and configuration options, \\nsuch as Select Hosting: Configure and deploy Firebase Hosting sites with a \\ngraphical user interface. Select From the drop-down menu, choose your \\nFirebase Project to work on. After that, you'll need to inform Firebase where to \\nlook for your assets after they've been deployed, which you can do here. Create \\nReact App creates a build folder that includes all the production assets by \\ndefault, and this is what you should expect. Assuming the default configuration \\nhas not been changed, this should be set to the build state. Finally, you have \\nthe option of having Firebase rebuild your existing build/index.html file with a \\nnew one created by itself. You will not be able to use this functionality until you \\nhave a fully functional version of your application. When presented with this \\nchoice, you should pick N. (No). You should see two new files on your hard disk \\nwhen the starting process is complete. firebase.json and firebaserc are two \\ndifferent types of firebase. Because these files include information about your \\nFirebase hosting setup, you should save and commit them to your git \\nrepository. Everything seems to be in working order, so you can continue with \\ndeploying your application.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"30 \\n \\n6 Testing \\nIt's also critical to do testing to guarantee that the application is functioning and \\ncompatible with the vast majority of browsers available. Moreover, the web text \\nis reviewed for grammatical errors in addition to being error-free. \\nEverything was checked, goods were seen with images, the list was sorted both \\nascending and descending, the products were filtered by name as well as \\ncategory.  Preferred things were put to the cart and deleted from the basket as \\nappropriate. The system functions just as it should in every respect. There were \\nno problems with the examination. \\nThe responsiveness test was conducted on different environments such as \\nAndroid, iOS, and Windows operating systems. There was a favorable conclusion \\nsince not all of the mobile phones that were tested were able to connect to the \\nwebsite without experiencing any difficulties. It seems like all of the visuals, \\nbuttons, and oth er parts of the design are being shown appropriately on the \\nscreen.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"31 \\n \\n7 Discussion \\nIt is now simpler than ever to develop a website utilizing a number of platforms \\nand computer languages, such as PHP, Vue, and JavaScript, as a result of recent \\ntechnological advancements . The majority of website builders, mostly \\nconventional ones such as WordPress, provide pre -designed website templates \\nfor users who do not have technical skills. These templat es, on the other hand, \\nare often incapable of being customized as readily as developers would want. \\nOne of the most significant disadvantages is the high initial and ongoing costs of \\ncreating and maintaining the process.  \\nAs a result, we chose React for th is project. JavaScript offers a number of \\nbenefits, including its speed, simplicity, server load, and interoperability with other \\nprogramming languages, to name a few. JavaScript is more efficient since it does \\nnot rely on external resources or a backend server. Because JavaScript is totally \\ninterwoven with HTML and CSS, it can be introduced to any web page in a matter \\nof seconds. \\nKnown for its effectiveness in the development of online applications, React is \\none of the most popular JavaScript lightweight frameworks for web development. \\nAs a form of cache, React retains a copy of its data structures in memory. Each \\ntime the code is changed, React automatically identifies and propagates the \\nchanges to the browser, ensuring that the user experience is uninterr upted. The \\nperformance of the website will be improved as a result of this unique feature. \\nThere are several advantages to using Firebase cloud services. The technique \\naids in the discovery of information by customers and increases the website's \\npresence o n the internet. Additionally, the rapid and secure hosting solution \\nfacilitates the deployment of web applications. Encryption without the need for \\nsetup Firebase Hosting use SSL encryption to protect the information. Domains \\nare protected from external attacks and data breaches thanks to the use of SSL \\ntechnology. Firebase Hosting makes use of SSD and content delivery networks \\n(CDNs) to deliver material quickly and improve application performance.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content=\"32 \\n \\nPayment and bills will not be the focus of this project due  to lack of time and \\nresources. However, additional development of these characteristics is required \\nin order to boost the website's economic efficiency. Customer service \\nrepresentatives should notify customers of any changes to the store's promotion \\nand discount policy through email on a regular basis. Another tip is to increase \\nyour website's search engine visibility and network marketing efforts, both of \\nwhich are becoming more important when running an online business. \\nCustomers who are not fluent in English should be able to place orders in a range \\nof languages, including but not limited to specific languages such as German and  \\nFinnish.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='33 \\n \\n8 Conclusion \\nThe goal of this thesis is to utilize React to create an e-commerce website that is \\nfast, flexible, and easy to use . The website should also be able to manage the \\nbackend effectively just like, for instance,  Firebase’s cloud services.  This thesis \\nwas a success since the website functions as expected. \\nThere are several features on this website that allow visitors to do a variety of \\ntasks such as seeing a list of goods with thumbnails and full -sized photographs, \\narranging the products by size and adding or deleting things from the shopping \\ncart. \\nThis project was made using React in the front-end and Firebase plays a role as \\nthe backend site. The project was put online after it had completed the \\ndevelopment phase. \\nDifferent platforms were used to co nduct the responsiveness, functionality, and \\nbrowser compatibility tests. We were pleased with the end outcome since our \\nwebsite suited all our needs. \\nThere are a lot of useful features in this web app, yet it is still basic a s well as  \\nappealing enough for an online business. At a modest cost, it gives customers a \\nway to grow their company.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='1 \\n \\n \\n \\nReferences  \\n1. What is Diffing Algorithm. Online. 02 February 2022.  <What is Diffing \\nAlgorithm>. Accessed 3 April 2022. \\n2. Horton Adam. Mastering React. Book. 23 February 2016. Accessed 3 April \\n2022. \\n3. Occhino, Tom. 2015. React Native: Bringing modern web techniques to \\nmobile. Online. 26 March 2015. < React Native: Bringing modern web \\ntechniques to mobile>. Accessed 5 April 2022.  \\n4. Sutch, Caelin. 2020. “What is Firebase?”. Online. 17 May 2020. <What is \\nFirebase>. Accessed 5 April 2022. \\n5. 2021. Firebase - Introduction. Online. 15 July 2021. < Firebase- \\nIntroduction>. Accessed 5 April 2022. \\n6. Ogbonna, Chizoba. 2022. Device to Device Push Notification using Cloud \\nFunctions for Firebase. Online. 27 March 2017. <Device to Device Push \\nNotification using Cloud Functions for Firebase>. Accessed 12 April 2022. \\n7. Firebase hosting. Online. <Firebase hosting>. Accessed 12 April 2022.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='Appendix 1 \\n1 (1) \\n \\nFigures \\nFigure 1. How the DOM working in React 4 \\nFigure 2. The overview of Firebase Cloud service. 9 \\nFigure 3. Firebase Authentication service 10 \\nFigure 4. Cloud Function for Firebase 12 \\nFigure 5. The example of Firebase real-time database. 13 \\nFigure 6. The flow of using Firebase Hosting. 15 \\nFigure 7. An example of Cloud Storage for Firebase service 16 \\nFigure 8. The structure of project 20 \\nFigure 9. The screenshot of HTML file 21 \\nFigure 10. The screenshots of product.css file 21 \\nFigure 11. The screenshot list of components 23 \\nFigure 12. The screenshot of redux file 24 \\nFigure 13. The configuration file of Firebase in the application 25 \\nFigure 14. The Home page 26 \\nFigure 15. The product detail page 26 \\nFigure 16. The Orders page 27 \\nFigure 17. The Cart page 27 \\nFigure 18. The Login page 28 \\nFigure 19. The Register Page 28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42', 'file_type': 'pdf', 'file_name': '[firebase]_Building an E-commerce Application.pdf'}, page_content='Appendix 2 \\n1 (1)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf'}, page_content='Academic Journal of Nawroz University (AJNU), Vol.10, No.3, 2022  \\nThis is an open access article distributed under the Cre ative Commons Attribution License  \\nCopyright ©2017. e-ISSN: 2520- 789X \\nhttps://doi.org/10.25007/ajnu.v11n3a14 80 \\n410 \\n \\nFirebase Efficiency in CSV Data Exchange Through PHP-\\nBased Websites \\nRidwan Boya Marqas1, Saman M. Almufti 2, Renas Rajab Asaad 3 \\n1,2,3Department of Computer Science, Nawroz University, Kurdistan Region  –  Iraq \\nABSTRACT \\nA database is a collection of data that may be organized into two distinct forms: relational databases, which use \\nSQL (structure query language), and distributed databases, which use No SQL (non-relational SQL). Both types of \\ndatabases are referred to as databases. The amount of information around the globe is increasing at an exponential \\nrate, leading to the development of data as the world gets  more technologically advanced and computerized. In \\nthe vast majority of studies, the NoSQL database is simpl y referred to by its acronym, \"NoSQL.\" In this \\ninvestigation, data is transferred from a website written in PHP t o a CSV file by way of a NoSQL database known \\nas Firebase. To import and export the experimental data, it was necessary to use two different CSV files, each of \\nwhich included 1,000 and 4,997 records, respectively. Exchangin g CSV files with a website that was built using \\nPHP was the method that this study used to test the performance of  Firebase.  \\nKEYWORDS: Database, Firebase, Website, PHP, NoSQL.  \\n1. INTRODUCTION\\nThe term \"database\" is mostly used to refer to a \\ncollection of data that has been structured, saved, and \\naccessible via a computer system (Ullman 2007). \\nToday\\'s digital systems are susceptible to data with \\nhuge dimensions because of the extent, diversity, and \\ncomplexity of the data universe. [citation needed] In \\naddition, enormous amounts of data must be saved, \\nmanaged, or analyzed using cloud and social media \\nstorage and management systems (Ramzan and Bajwa \\n2018). NoSQL databases are used for enterprise and \\nopen-source database management. These databases \\nstore vast amounts of data across a network of \\ncomputers. The name \"Not Just SQL\" was given to the \\nNoSQL database management system to dispel the \\nwidespread belief that SQL cannot be contained \\n(Ganesh Chandra 2015). \\nNoSQL makes it possible to scale horizontally; hence, \\nits many implementations are always kept on distinct \\nservers. The column-based NoSQL database stores \\ndata in a single huge table, as opposed to the relational \\ndatabase, which stores data in numerous tables. This is \\ndone to make autoscaling easier (Lee and Zheng 2015). \\nFirebase is a NoSQL database that is hosted on the \\ncloud and runs in that environment. Even when the \\nlocal cache network is off, it is still feasible to \\nsynchronize all the devices that are linked together. \\nThis database is driven by events, and in comparison, \\nto ordinary SQL databases, it operates considerably \\ndifferently (Moroney and Moroney 2017a). PHP was \\ninitially developed by Rasmus Lerdorf, a Canadian of \\nDanish ancestry, and the most recent version, PHP 7, \\nincorporates all the most recent enhancements (Jentsch \\n1997). \\nThis article looks at how well Firebase performs while \\nimporting and exporting CSV files with websites that \\nare based on the PHP programming language. \\n2. MATERIALS AND METHODS \\nA. Programming Hypertext Protocol (PHP)  \\nWeb pages are often built using a dynamic \\nprogramming language called PHP, which may be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf'}, page_content=\"Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n411 \\n \\n \\nfound on a broad variety of websites and web \\napplications that are constantly updated (Mon et al. \\n2019). To access and save data on a variety of \\nplatforms, the PHP code may be modified to operate \\nwith other web scripting languages. Comma Separated \\nValue (CSV) files may be read and written using \\nfunctions in PHP (Mon et al. 2019). \\nLarge volumes of data are frequently sent across \\ndisconnected applications using CSV files. Commas \\nare commonly used to separate spreadsheet fields in \\nCSV files, whereas system end- of-line characters are \\ncommonly used to separate CSV records in Microsoft \\nExcel or Text pad (Hapeez, Yassin, and Hamzah 2010). \\n2.1 Non-relational database (NoSQL) \\nNon-relational databases, such as NoSQL, do not \\nhave a defined structure and may be accessed using a \\nsimple query language. Data may be distributed and \\nreproduced in a less controlled environment using \\nNoSQL's huge database. For the foreseeable future, \\nthis database will maintain the ability to store data \\nindependently of any other databases. NoSQL \\ndatabases are organized hierarchically. It's also \\ncapable of handling large volumes of data at high \\nspeeds. The structure of a NoSQL database is \\nhorizontal. NoSQL databases like Cassandra and \\nHBase are among the few that may be used in a NoSQL \\napplication. \\nIn general, there are numerous Relational databases \\nand non-Relational databases. SQL databases include \\nMySQL, Oracle, Microsoft SQL Server, PostgreSQL, \\nand DB2, whereas NoSQL databases include Redis, \\nAmazon DynamoDB, Cassandra, Scylla, HBase, \\nFirebase, MongoDB, Couchbase, Neo4j, Datastax \\nEnterprise Graph, Elasticsearch, Splunk, and Solr. See \\nFig. 1. \\n \\nFig. 1.  Classes of SQL and NoSQL \\nThis table compares several aspects of SQL and \\nNoSQL Databases, such as their scalability and pricing \\nas well as their capacity to hold a large quantity of data \\nand how quickly they can be accessed. \\n3. PROPOSED METHOD \\nThis section describes the proposed technique for \\nreading CSV files and importing data into the firebase \\ndatabase, as well as exporting data from the firebase \\nservice. The execution time was calculated to assess the \\neffectiveness of importing and exporting PHP-based \\nwebsite files using firebase Information. \\n3.1 PHP & CSV \\nPHP needs the following functions for reading and \\nwriting data to and from CSV files: \\nI. FOpen operation \\nThis function is used to open a CSV file \\nTABLE 1 \\nComparison of SQL and NoSQL criteria  \\nCriteria RELATIONAL \\nDATABASE \\n(SQL) \\nNoSQL \\ndiversity Open and \\nclosed source \\nOpen source  \\nScalability Upgrade a \\nsingle server \\nwith devices \\nUsing standard servers scale \\nhorizontally \\nprice Costly data \\naccess \\nsolution \\ninexpensive than open source \\nand cheaper update \\nAmount of \\ndata \\nLimited Vast data hold \\naccessibility Affect by \\nsingle fail \\nUnaffected by one point of failure \\nthat’s distributed \\nExecution \\ntime \\nLong process \\ntime \\nShort process time \\nComplexity Complex \\ndata creation \\nLess complex data creation \\nimplementati\\non \\nSmall \\nimprovement \\noccurs \\nEach stage own improvement \\noccurs \\nuniformity Structured  Unstructured  \\nSecurity Strong \\nSecurity \\nSecurity not included is related to \\nother parts \\n   \\n   \\n.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n412 \\n \\n \\nusing open (file, mode), where the file \\nrepresents the target file and mode represents \\nthe access required for reading or writing in \\nthe CSV file. \\nII. Fgetcsv function \\nThis function is used to read data from a \\nCSV file by iteratively parsing an open file \\nline by line and checking for data fields. \\nfgetcsv (file, length, separator), where the file \\nis the target file, length is the maximum \\nlength of a CSV row, and separator is a \\ncomma to separate CSV data. \\nIII.  Fputcsv function \\nThis function is used to write data to a \\nCSV file fputcsv (file, fields), where the file is \\nthe destination file and fields are the data \\narray. \\nIV.  Fclose Method \\nThis function is used to open the CSV file fclose (file), \\nwhere the file represents the file to be opened. \\nThe features of Firebase are broken down into several \\ncategories and described in Table 2. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n3.2 PHP & FIREBASE \\nFirebase is a cloud-based, real-time database \\ndeveloped for mobile and web apps, although it \\ncannot be used directly with PHP to create websites. \\nFirebase stores data as JSON; hence, the Composer \\ndependency manager, which provides a standard \\nstructure for managing PHP and library dependencies, \\nis required for linking PHP with Firebase. \\n \\nI. Getreference function  \\nThis function is used to retrieve and push \\nvalues from the source database. \\ngetreference (DB) \\nwhere DB is the Database being \\nreferenced. \\nII. Push functionality \\nThis method inserts a list of data records \\ninto the Firebase database. When adding a \\nnode to a list of data, the Firebase database \\ngenerates a new unique key. \\nPush(Data) \\nData represents the list of data to be \\nuploaded into Database. \\nIII. The getValue method  \\nretrieves data from the Firebase database. \\ngetValue() \\nTABLE 2 \\nCharacteristics of firebase  \\nCriteria FIREBASE \\nType Cloud hosted \\nRealtime \\nDatabase \\nModel \\nDocument \\nStore \\nDevelop by Google \\ncompany \\nRelease 2012 \\nCommercial Yes \\nCloud based Yes \\nServer OS Hosted \\nScheme of \\nData \\nFree schema \\nXML support No \\nSQL No \\nAccess \\nmethods and \\nAPI’s \\nAndroid \\nSupport \\nprogram \\nlanguage \\niOS \\nServer-side \\nscripts \\nJavaScript \\nAPI \\nTriggers RESTful \\nHTTP API \\nConsistency Java \\nForeign keys JavaScript \\nIntegrity Objective-C \\nAuthenticatio\\nn \\nFunctionality \\nare limited \\nwith rules'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf'}, page_content=\"Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n413 \\n \\n \\n \\nFig. 2.  process of import & export. \\nFigure 2 depicts the data import and export processes \\nfor both the firebase databases. \\n4. EXPERIMENT & RESULT \\nThis section demonstrates the results of an \\nexperiment including the import and export of data \\nfrom the firebase databases, with download speeds of \\n34.65 Mbps and upload speeds of 36.06 Mbps. \\nIn the experiment, two hospital-related CSV files \\nwere employed; the first CSV file included 1000 \\nentries, while the second CSV file contained 4997 \\nrecords; both files had 11 columns, as seen in (figure 3). \\n \\nFig. 3.  columns name of CSV file \\n(Figure 4) depicts the full system connection \\nbeginning with the connection from the computer to \\nthe 000webhost server. The server hosts the website \\nthat connects to the distinct databases listed below: \\n• Firebase: in Google server Using NoSQL. \\n \\nFig. 4.  Connection System \\nA connection system in (figure 4) demonstrates the \\nconnection of the system used that import and export \\nprocess occurred through it which the connection \\ndistributes between firebase and CSV file by using \\n000webhost hosting server. \\nIn the Firebase online database, a table named hospital \\nwas created according to the CSV structure depicted in \\n(figure 5). \\n \\nFig. 5.  demonstrates the hospital table database \\nstructure online Firebase \\nTable 3 depicts the execution times for importing and \\nexporting data in the firebase database. \\nTABLE 3 \\nduration of Firebase's execution \\nprocess Record No. Firebase \\nimport 1000 138.649094 seconds \\n4997 735.564 seconds \\nexport 1000 0.495048046 seconds \\n4997 0.774774 seconds \\nTable 3 displays the import and export execution \\ntimes for 1000 and 4997 CSV entries in firebase.  \\n(Figure 6) depicts the import process execution time \\nfor 1000 and 4,997 CSV records into Firebase, while \\n(Figure 7) depicts the export process execution time for \\n1000 and 4,997 CSV records in the Firebase database.\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n414 \\n \\n \\n \\nFig. 6.  demonstrates the import process for Firebase \\n \\nFig. 7.  demonstrates the export process for Firebase \\nThe import and export process show the percentage \\nfor 1000 and 4997 CSV record which for import one \\nuse 18% for 1000 and 82% for 4997, while the export \\nprocess show 16% for 1000 and 84% for 4997 CSV \\nrecord. \\n5. CONCLUSION AND RECOMMENDATIONS \\nThis study presents a time performance assessment \\nfor the process of data transferring (import and export) \\nbetween CSV files and Firebase online databases with \\na PHP-based website. \\nThe experimental results for two CSV files containing \\n1000 and 4997 records indicate that the data exporting \\nprocess for Firebase in a PHP-based website is faster \\nthan the importing process. This is because, during the \\nimport process, the connection between the website \\nand Firebase is indirect and requires more time to \\nreach the database on an external server, whereas the \\nexport process directly reaches the database and \\nretrieves the data. \\nREFERENCES \\nBinani, Sneha, Ajinkya Gutti, and Shivam Upadhyay. \\n2016. “SQL vs. NoSQL vs. NewSQL- A \\nComparative Study.” 6(1):43–46. \\nDB-Engines.com. 2020. “Firebase Realtime Database \\nvs. Sqlite Comparison.” Retrieved June 2, 2020 \\n(https://db-\\nengines.com/en/system/Firebase+Realtime+Dat\\nabase%3BSQLite). \\nGanesh Chandra, Deka. 2015. “BASE Analysis of \\nNoSQL Database.” Future Generation Computer \\nSystems 52:13–21. \\nHapeez, Mohammad Shukri, Mohd Ihsan Mohd \\nYassin, and Mustafar Kamal Hamzah. 2010. \\n“Storage of Online HPLC Data for \\nPharmaceutical Research Applications Using \\nXML Database.” Proceedings of the 2010 5th IEEE \\nConference on Industrial Electronics and \\nApplications, ICIEA 2010 1605–9. \\nJentsch, Birgit. 1997.                             Gender Politics \\nand Post-Communism: Reflections from Eastern \\nEurope and the Former Soviet Union. Vol. 3. \\nLee, Chao Hsien and Yu Lin Zheng. 2015. “Automatic \\nSQL-to-NoSQL Schema Transformation over the \\nMySQL and HBase Databases.” 2015 IEEE \\nInternational Conference on Consumer \\nElectronics - Taiwan, ICCE-TW 2015 426–27. \\nMon, Cho Thet, Su Su Hlaing, Mie Mie Tin, Mie Mie \\nKhin, Tin Moh Moh Lwin, and Khin Mar Myo. \\n2019. “Code Readability Metric for PHP.” 2019 \\nIEEE 8th Global Conference on Consumer \\nElectronics, GCCE 2019 929–30. \\nMoroney, Laurence and Laurence Moroney. 2017a. \\n“An Introduction to Firebase.” The Definitive \\nGuide to Firebase 1–24. \\nMoroney, Laurence and Laurence Moroney. 2017b. \\n“The Firebase Realtime Database.” The Definitive \\nGuide to Firebase 51–71. \\nRamzan, Shabana and Imran Sarwar Bajwa. 2018. “SS \\nSymmetry An Intelligent Approach for Handling \\nComplexity by Migrating from Conventional \\nDatabases to Big Data.” \\nStephens, Jon and Chad Russell. 2004. Beginning \\nMySQL Database Design and Optimization. \\nUllman, J. D. 2007. “A First Course in Database \\nSystems.” Pearson Education India. Wang, K. C. \\n2018. Systems Programming in Unix/Linux.  \\n \\n \\n18%\\n82%\\nImport\\n1000\\n4997\\n16%\\n84%\\nexport\\n1000\\n4997'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/362539877\\nFIREBASE -OVERVIEW AND USAGE\\nArticle\\xa0\\xa0in\\xa0\\xa0Journal of Engineering and Technology Management · August 2022\\nCITATIONS\\n35\\nREADS\\n15,713\\n5 authors, including:\\nAnil Trimbakrao Gaikwad\\nBharati Vidyapeeth Deemed to be University\\n19 PUBLICATIONS\\xa0\\xa0\\xa039 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Anil Trimbakrao Gaikwad on 07 August 2022.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1178] \\nFIREBASE - OVERVIEW AND USAGE \\nPankaj Chougale*1, Vaibhav Yadav*2, Dr. Anil Gaikwad*3 \\n*1,2Student, Bharati Vidyapeeth Deemed To Be University, Pune. Institute Of Management,  \\nKolhapur, India. \\n*3Guide, Bharati Vidyapeeth Deemed To Be University, Pune. Institute Of Management,  \\nKolhapur, India. \\nABSTRACT \\nThe web application has relied heavily on large amounts of websites and random data such as videos, photos, \\naudio, text, files and other inappropriate content. Icon It is difficult for the Relational Database Management \\nSystem (RDBMS) to manage random data . Firebase is a new technology for managing large amounts random \\ndata. Very fast compared to RDBMS. This research paper focuses on us age of Firebase for Android and aims to \\nfamiliarize itself with its  functions, related concepts names, benefits and limitat ions. The paper also tries to \\nshow some nakedness Firebase features for building an Android app.  \\nKeywords: Cloud Based-Firebase, Cloud Based-Android. \\nI. INTRODUCTION \\nGoogle Firebase  may be a  Google-backed application development software  that allows developers to develop \\nIOS, Android and Web apps. Firebase provides tools for tracking analytics, reporting and fixing app crashes, \\ncreating marketing and products experiment. \\nII. METHODOLOGY \\nFirebase offers variety of services, including: \\nAnalytics – Google Cloud Analytics for Firebase offers free, unlimited reporting’s on as many as 500  or more \\nseparate events. Statistics present content on user behavior in IOS and Android app lications to enable best \\ndecision-making about improving an application performance and marketing. \\n \\n(source: https://i.ytimg.com/vi/8iZpH7O6zXo/maxresdefault.jpg) \\nAuthentication: Firebase authentication makes it easy for developers to protect authentication systems and \\nimproves login and boarding experience for users. This the feature offers an all -in-one identity solution, \\nsupporting email accounts and password, phone auth, moreov er like Google login, Facebook, GitHub, Twitter \\nand more.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1179] \\n \\n(source: https://firebase.google.com/images/products/auth/auth -3.png) \\nCloud Messaging: Firebase Cloud Messaging (FCM) may be various messaging tool allowing companies to trust \\nand receive messages on IOS, Android and therefore the web at no cost. \\n \\n(source: https://i.ytimg.com/vi/sioEY4tWmLI/maxresdefault.jpg) \\nRealtime Database: The Firebase Realtime Database may be a cloud -based NoSQL site that allows data to be \\nstored and synchronized between users in real time. Information is synced to all clients in real time and is \\nalways available when the app is offline. \\n \\n(source: https://media.geeksforgeeks.org/wp-content/uploads/20190421141241/gfg53.png)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1180] \\nCrashlytics: Firebase C rashlytics may be a real-time crash journalist assisting engineers  track, prioritize and \\nfix problems that diminish the quality of their applications. With Crashlytics, engineers spend less time \\nplanning and resolving crashes and long construction features for his or her apps. \\n \\n(source: https://firebase.google.com/docs/crashlytics) \\nPerformance: The Cloud-Firebase Performance Monitoring feature provides developers with an \\nunderstanding of app lication features of their IOS and Android apps to help them decide where to reach and \\nwhen the performance of their applications is improved. \\n \\n(source: https://miro.medium.com/max/1400/1*yevJMct9erdfRGH8YgJ1_g.png ) \\nTest Lab:  Firebase Test Lab  may be cloud-based application testing infrastructure. By one performa nce, \\ndevelopers can test their I OS or Android apps across all device devices as well device con figuration. They will \\nsee results, including videos, screenshots and logs, inside the Firebase console. \\n \\n(source:https://d1qmdf3vop2l07.cloudfront.net/unwavering-snake.cloudvent.net/hash-\\nstore/15f9baced6535d0042fc1d098c262d2c.png)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1181] \\nUse Cases: \\nConditions for using Firebase include: \\nCreate on boarding flows:  Developers can provide users with a fast, intuitive login process Firebase \\nVerification. allow users to sign in to their apps via Google, Twitter, Facebook or GitHub accounts in but five \\nminutes. Engineers can track each step their ride flow to enhance use r experience. Additionally, engineers can \\nuse it Google Analytics Firebase entry events for each step flow ride, create panels to see where users are \\nleaving and using the remote control adjusting to make changes to their operating systems to determine how  \\nthose changes affect them conversion. \\nCustomize a “welcome back” screen:  Engineers can use it to make it your own to stop everything user very \\neasy experience customizable first user -supported screen favorites, usage history, location or language. \\nEngineers can explain by audience, by section, in user behavior and display targeted content to all audiences.  \\nGradually unleash new features: Developers can launch new features . \\nProgressively roll out new features:  Developers can introduce new low -risk features by first testing those \\nfeatures in some users to determine how they operate and how users respond. Then, If the engineers are \\nsatisfied. \\nIn June 2018, mobile security company Ap pthority reported thousands of I OS and Android devices mobile \\napplications expo sing more than 113 GB of information about 2,271 unprepared Firebase database.  As of \\nJanuary 2018, Authority researchers are scanning Android applications that use Firebase systems to store users \\ndata or content  and updated communication patterns for applications made on Cloud-Firebase domains.  \\nAfter scanning mor e than 2.7 million Android and I OS apps, researchers identified 28,502 mobile apps (1,275 \\nIOS and 27,227 Android) that connected and stored data within t he Firebase background. Of these, 3,046 apps \\n(600 IOS and pear, 446 Android) store data within 2,271 Firebase poorly designed websites that give anyone \\nthe ability to view their content. \\nThe database revealed more than 100 million user data records, includ ing the actual LinkedIn, Firebase, \\nFacebook and company data store tokens; 25 million GPS location records; more than 4 million health \\ninformation records are protected, such as medical information and chat messages; 2.6 million User IDs and \\npasswords; and  50,000 financial records, including payments, banking and transactions for Bit coin . \\nPricing: \\nFirebase offers a free 1 GB real -time data storage system and two paid subscriptions plans: Flame Plan ($ 25 \\nper month with 2.5 GB of storage) and Blaze Plan (pa y-as-you-go, $ 5per GB per storage). All programs include \\nA / B testing, statistics, application identification, authentication (without phone auth), cloud messaging, \\nCrashlytics, dynamic links, invitations, functionality monitoring, forecasting and remote  preparation. The main \\ndifferences between programs include real -time shared storage  website, number of download tasks, Cloud \\nFirestone bandwidth, and more. Internet of Things Engineers share many common usage requirements across a \\nwide range of IoT  applications. Data collection, delivery of content with low latency, and to prevent \\ncommunication between devices and back -up services there are only three  of these common requirements.  \\nWhile meeting common needs  are often challenging from time to time, I oT development platforms  like Google \\nFirebase provide services and functionality that allows developers to satisfy many of those requirements. \\nBenefits of Google Firebase: \\nThe Firebase platform as a service includes a NoSQL document data store. Application s store data as JSON \\nobjects and interact with the database  employing a  JavaScript API. Mobile  or Android  developers have  the \\noption of using Android or I OS APIs, too.  The information store is intended  to scale with application demand, \\nso there\\'s no have to add servers, partition data or perform other database administration that comes  together \\nwith maintaining your own database  rear. The REST  full API includes queries  operations but not SQL \\noperations. The query API is ready-made to figure with channels of information. for instance, the \"on\" operation \\nlistens for changes in location and invokes a callback Function if a selected event occurs. There also are queries \\nand operators with SQL -equivalents, like order-By-Key, order-By-Value and order -By-Priority. The limit, limit -\\nTo-First and limit-To-Last query operations can restrict  the amount of JSON documents returned by the query.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1182] \\nIn addition to storing dynamic contents within the firebase store, developers can store static content  that it will \\nbe required within the app in a Firebase managed service. Way of providing static content storage is the same \\nas installing a website; Firebase will do verify domain ownership, provide an SSL certificate and use it \\nthroughout Firebase content delivery network. Finally, name service records must be updated to become a map  \\nthe name of your hosted site. \\nAnother advantage of Firebase is that it provides support for offline operation. Website  functions are locally \\nrecorded and synchronized with the Firebase site when a network connection is established.  Google Firebase \\nincludes mobile data protection controls at rest. Data is transmitted using SSL / TLS 2048 -bit encryption, and \\nwithin the site , users are authenticated and may be restricted to certain functions using a set of security rules. \\nFirebase authentication uses an existing login server or client side code. Firebase supports currently usernames \\n/ password login such as social login resou rces like Google, Twitter and Facebook. Custom code can be used if \\nyou would like to receive your tokens.  In the case of users or in the case of  IOT, devices - already authorized, \\nsecurity rules govern the activities they will perform and the data they wi ll access. Safety rules support reading \\n/ writing performance, integrated can expect, moreover as a guaranteed job. this can usually determine the \\ncorrect format of the information element and its data type. New apps automatically provide full read / write  \\naccess to the site; engineers should ensure that they review safety rules to limit the performance and breadth of \\ninformation the tool can use. Attackers can gain multiple or higher rights to compromise the integrity or \\nconfidentiality of information within a data store. \\nGoogle Cloud Firebase provides six price categories with its different services: - Free, Spark, Candle, Bonfire, \\nBlaze and Inferno. Sans Free tier, price from $ 5.00 / month on Spark to $ 1,499.00 / month on Inferno. Three to \\nsix stages al l include unlimited real -time internet connection support with users, additionally as 1 TB data \\ntransfer. The biggest difference between categories is reflected in the final volume of real -time data and \\ntransfer grants. Spark, for example, offers 1 GB of s torage and 10 GB of transmission while Inferno, the leading \\ntier, offers 300 GB of storage space and allows up to 1.5 TB of transfer data. the three most advanced categories \\nalso offer non-public backup options to customers. \\nDrawbacks  Of  Google-Firebase: \\nFirebase is very useful for IOT apps  that use data that can use the Javascript API to access Content and security \\nservices. However, developers should look to other tools to increase the back -end performance of their I OT \\nsystem. Basic server side processing is provided, but more advanced a nalysis may require uploading IO T data \\nto a different location, such as Hadoop or Spark. \\nAnother challenge is that the Firebase query function is limited. If you need more advanced query functions, \\nconsider sending data to an Elastic  Search server or collection for more search options. In addition, if visibility \\nand alerts are important when monitoring information distribution, consider tools such as Kibana, Elastic  \\nsearch data display. \\nIII. RESULTS AND DISCUSSION \\nOut-of-container notification response. you can use Firebase Notifications, a notification server with an Internet \\nconsole that allows anyone to send alerts to specific target audiences based on Firebase Analytics data.  \\nRealtime Database in Firestore uses NoSQL , a cloud -based website to store and sync real -time data between \\nusers and devices. In addition, Firebase Cloud Storage stores and stores user -generated content such as images,  \\naudio, and video more smoothly. Using Cloud Tasks, you can measure your applica tions at any time in your life \\ncycle without measuring the servers in use.  Hosting and verification is easily managed efficiently in Firebase \\nwith simple methods and effective tools. The ready -to-use Firebase ML Kit APIs carefully add advanced features \\nto machine learning in  the operating system. The best thing is that your limited knowledge and experience in \\nmachine learning would not be an obstacle to adding a little ingenuity to an advanced app.  \\nIV. CONCLUSION \\nThis paper highlights research on the Firebase API provided by Google and its unique features. This paper helps  \\nyou to learn how to utilize the Firebase in the Android app based on the Clients  requirements. This also helps \\nto make android apps faster and more efficient as there is no PHP required as a third party language to \\ncommunicate with the website. Provides a secure communication channel and website directly from JAVA.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7', 'file_type': 'pdf', 'file_name': '[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1183] \\nLearning materials are based on data provided online and refer to the examples provided. Google has been \\nconstantly updating Firebase, Ad Sense  is a beta section of Firebase. Not only for Android but also for cross -\\nplatform connectivity .The function can be expanded by adding new feature s and testing new possibilities to \\nAndroid apps. \\nV. REFERENCES \\n[1] https://firebase.google.com/ \\n[2] https://www.javatpoint.com/firebase \\n[3] https://www.tutorialspoint.com/firebase/index.html \\n[4] https://youtu.be/b1bGrWrx5Mo \\nView publication stats')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata[\"file_type\"] = \"pdf\"\n",
    "    doc.metadata[\"file_name\"] = doc.metadata.get(\"source\", \"\").split(\"\\\\\")[-1]\n",
    "\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c9fb8",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1740168",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    Split documents into chunks using RecursiveCharacterTextSplitter.\n",
    "\n",
    "    Args:\n",
    "        documents: List of LangChain Document objects\n",
    "        chunk_size: Maximum size of each chunk (in characters)\n",
    "        chunk_overlap: Number of characters to overlap between chunks\n",
    "\n",
    "    Returns:\n",
    "        List of chunked Document objects\n",
    "    \"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "\n",
    "    return splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b32a9164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 559 created from Total documents: 159\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_documents(docs, chunk_size=1000, chunk_overlap=200)\n",
    "print(f\"Total chunks: {len(chunks)} created from Total documents: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdc5c8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A\\nBenchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nRishi Kesav Mohan\\nrkmohan2@illinois.edu\\nRisheek Rakshit Sukumar Kanmani\\nrrs7@illinois.edu\\nKrishna Anandan Ganesan\\nkag8@illinois.edu\\nNisha Ramasubramanian\\nnr50@illinois.edu\\nABSTRACT\\nIn the era of big data, conventional RDBMS models have become\\nimpractical for handling colossal workloads. Consequently, NoSQL\\ndatabases have emerged as the preferred storage solutions for exe-\\ncuting processing-intensive Online Analytical Processing (OLAP)\\ntasks. Within the realm of NoSQL databases, various classifications\\nexist based on their data storage mechanisms, making it challenging\\nto select the most suitable one for a given OLAP workload. While\\neach NoSQL database boasts distinct advantages, inherent scalabil-\\nity, adaptability to diverse data formats, and high data availability\\nare universally recognized benefits crucial for managing OLAP'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='each NoSQL database boasts distinct advantages, inherent scalabil-\\nity, adaptability to diverse data formats, and high data availability\\nare universally recognized benefits crucial for managing OLAP\\nworkloads effectively. Existing research predominantly evaluates\\nindividual databases within custom data pipeline setups, lacking\\na standardized approach for comparative analysis across different\\ndatabases to identify the optimal data pipeline for OLAP workloads.\\nIn this paper, we present our experimental insights into how vari-\\nous NoSQL databases handle OLAP workloads within a standard-\\nized data processing pipeline. Our experimental pipeline comprises\\nApache Spark for large-scale transformations, data cleansing, and\\nschema normalization, diverse NoSQL databases as data stores, and\\na Business Intelligence tool for data analysis and visualization.\\nThe wide-ranging classifications of NoSQL databases include\\ndocument-oriented, key-value stores, columnar databases, and graph'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='a Business Intelligence tool for data analysis and visualization.\\nThe wide-ranging classifications of NoSQL databases include\\ndocument-oriented, key-value stores, columnar databases, and graph\\ndatabases. For our experiments, we selected MongoDB, Redis, Apache\\nKudu, and ArangoDB, each representing a distinct family of NoSQL\\ndatabases. Leveraging a standardized pipeline, we assessed the per-\\nformance of these databases using Koalabench, a popular NoSQL\\nbenchmarking dataset collection. Each pipeline we setup has a\\nunique NoSQL database and the performance of each pipeline has\\nbeen evaluated for data loading time and query execution time. We\\nhave also compared the performance of a standard SQL solution\\n(PostgreSQL) against the different NoSQL alternatives. Koalabench\\ngenerated datasets of varying sizes in our desired data model and\\nwe conducted a series of experiments using data belonging to two\\ndifferent data models - flat and snow. The insights gleaned from'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='generated datasets of varying sizes in our desired data model and\\nwe conducted a series of experiments using data belonging to two\\ndifferent data models - flat and snow. The insights gleaned from\\nthese experiments will facilitate the establishment of an optimal\\nOLAP data pipeline, pairing the ideal NoSQL database as the data\\nwarehousing solution.\\n1 INTRODUCTION\\nModern day data engineering has seen numerous enhancements.\\nEvery last ounce of data is being considered as a feature. With data\\nhaving immense significance in the modern world, the way in which\\nwe handle and process data should also evolve with requirements\\nand time. To develop a holistic data engineering pipeline, there are\\nthree crucial components:\\n• Data Loading and Processing\\n• Data Store\\n• Data Analysis\\n1.1 Data Loading and Processing\\nData loading and processing consists of the seamless ingestion,\\ntransformation, and preparation of data sourced from various ac-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='• Data Store\\n• Data Analysis\\n1.1 Data Loading and Processing\\nData loading and processing consists of the seamless ingestion,\\ntransformation, and preparation of data sourced from various ac-\\ncessible outlets. Given the enormous volume of data to be managed,\\nit requires underlying systems with enhanced processing capabili-\\nties to facilitate efficient handling. Apache’s Hadoop Distributed\\nFile System (HDFS) [1] emerges as one of the prominent solutions\\nbuilt for large-scale data storage. HDFS offers high-throughput ac-\\ncess to application data and is ideal for batch processing operations.\\nWith HDFS taking care of large storage, we would have to imple-\\nment a solution to handle data processing.Data processing is a key\\naspect of any data pipeline and it governs the process of converting\\nthe raw data into a database compatible format before loading it\\ninto the database. Apache Spark [2] stands out as a prime choice\\nfor swift and efficient data processing. Its seamless integration with'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='into the database. Apache Spark [2] stands out as a prime choice\\nfor swift and efficient data processing. Its seamless integration with\\nHDFS enables the seamless transfer of data from HDFS to Spark\\nfor processing or streaming to subsequent components within the\\npipeline. Employing these two tools in tandem establishes an opti-\\nmal platform for transforming data, rendering it ready for import\\ninto the designated data store.\\n1.2 Data Store\\nOnce the voluminous data has been stored and processed, we would\\nrequire a solution for storing the processed data. Ideally, this compo-\\nnent would be a database, which helps in storing data in a structured\\nformat and it can be accessed using query languages specific to\\nthe chosen database. The traditional relational database manage-\\nment system (RDBMS) [17] can store structured data effectively\\nbased on a predefined schema. RDBMS also requires complex query\\noperations such as joins in the event of requiring data from more'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='ment system (RDBMS) [17] can store structured data effectively\\nbased on a predefined schema. RDBMS also requires complex query\\noperations such as joins in the event of requiring data from more\\nthan one table and these complex query operations are generally\\nresource intensive.\\nWith the rise of semi-structured data, there is scope for exploring\\nthe possibility of OLAP workloads. NoSQL databases have been\\nadept at leveraging semi-structured data formats such as CSV and\\nJSON which has allowed for the incorporation of a flexible data\\nschema, ease of scalability and support for heterogenous datatypes.\\nNoSQL databases promote flexibility in schema and their data stor-\\nage mechanism allows for easy access of different data features\\nwithout the need for executing complex joins to derive insights.\\nDespite the benefits of NoSQL over SQL, prevailing architectures\\nhave predominantly relied on SQL databases as their primary data\\narXiv:2405.17731v1  [cs.DB]  28 May 2024'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\nstore. There exists a significant disparity in evaluating the perfor-\\nmance of NoSQL databases within an OLAP-intensive pipeline.\\n2 RELATED WORK\\nThe literature survey undertaken centered on evaluating prior re-\\nsearch within the realms of data processing and databases. These\\ndomains were identified as primary areas of interest for assessing\\nthe overarching structure of our envisioned pipeline.\\n2.1 HDFS and Spark for OLAP\\nOn the the topic of data processing, we looked far and wide to\\nunderstand the depth of research that had been performed in order\\nto gauge the progress from the perspective of data engineering\\npipelines. One of the key ideas, HDFS [22] has paved the way for\\nstoring large datasets and also provides an invaluable framework\\nfor carrying out analysis and transformation operations on the\\nlarge data using the MapReduce [10] algorithm. HDFS has paved'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='storing large datasets and also provides an invaluable framework\\nfor carrying out analysis and transformation operations on the\\nlarge data using the MapReduce [10] algorithm. HDFS has paved\\nthe way for several database and data streaming solutions to be\\nbuilt on top it. One such data streaming service that has become\\nwidely popular is Apache Spark [ 25]. Apache Spark helps in the\\nstreaming and processing of voluminous data from HDFS. HDFS\\nand Spark in tandem can handle the load of data processing on any\\nindustry-level data engineering pipeline.\\nResearch done based on leveraging HDFS and Spark for OLAP\\nworkloads is rather insignificant in the context of NoSQL databases.\\nThe standalone research of OLAP pipelines was limited in the pre-\\nvious years, however certain hybrid ideas were proposed. One such\\nidea called Hybrid Transaction/Analytical Processing (HTAP) [18]\\nproposed for a unified storage for both OLTP and OLAP workloads.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='vious years, however certain hybrid ideas were proposed. One such\\nidea called Hybrid Transaction/Analytical Processing (HTAP) [18]\\nproposed for a unified storage for both OLTP and OLAP workloads.\\nWhile this may seem feasible considering the flexibility of use that\\nsystems like HDFS and Spark provide in connecting with different\\ndatabase solutions, this idea does not account for the data rigidity\\nthat exists in transactional and analytical processing. RDBMS solu-\\ntions require a rigid schema and lack support for heterogeneous\\ndatatypes whereas NoSQL addresses both concerns. Hence, having\\na common store and unifying OLTP and OLAP is a little far-fetched\\nfor now.\\nWildfire by IBM [4] leveraged a HTAP like solution using HDFS\\nand Spark to perform data analysis and was one of the first solutions\\nto implement a data processing component in their pipeline. This\\nwork had stressed on the importance of having distributed data\\nstore and distributed data processing to make OLAP more effective.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='to implement a data processing component in their pipeline. This\\nwork had stressed on the importance of having distributed data\\nstore and distributed data processing to make OLAP more effective.\\nHowever, Wildfire was based on Spark SQL which is more similar to\\nRDBMS. Stream processing [21] became a popular research interest\\npost HDFS and Spark as many ideas revolved around bringing\\ntogether Spark and OLAP through the metaphorical ’cube’. This\\nprocess involved transforming raw data using Spark into an OLAP\\nfriendly format via a process called cubification. The resultant OLAP\\ncube, contained the features extracted from the provided data, in the\\nform of a cube on top of which data analysis could be performed to\\ngather insights. When distributed processing is employed, the use\\nof OLAP cubes becomes restricted to gathering insights from the\\nprocessed data and does not aid in determining a scalable strategy\\nfor processed data ingestion into the database.\\n2.2 Benchmarking for NoSQL-OLAP'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='processed data and does not aid in determining a scalable strategy\\nfor processed data ingestion into the database.\\n2.2 Benchmarking for NoSQL-OLAP\\nThe second broad topic we researched on is the availability of\\nliterature for NoSQL databases being used in OLAP/OLTP work-\\nloads. This exploration was an effort to understand whether there\\nwas a possibility to use NoSQL databases for OLAP. Based on the\\nNoSQL OLAP literature we identified, we also looked for previous\\nbenchmarking experiments performed for NoSQL OLAP works.\\nOne of the premier works in the NoSQL-OLAP domain involved\\nthe creation of OLAP cubes using distributed processing without\\nthe need of RDBMS [11]. Using the MC-Cube operator developed\\nin this paper, the idea is to perform transformations on the data\\nstored in columnar NoSQL databases and the data features get bun-\\ndled into \"cubes\" which can then be used for analytical insights.\\nThis idea came up short because of the use of just a columnar'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='stored in columnar NoSQL databases and the data features get bun-\\ndled into \"cubes\" which can then be used for analytical insights.\\nThis idea came up short because of the use of just a columnar\\nNoSQL database as other NoSQL databases were not tested in their\\nexperiments. Another idea on the contrary [8] used a document-\\noriented NoSQL database as the data store solution but employed\\na series of techniques such as shingling, chunck, minhashing, and\\nlocality-sensitive hashing MapReduce. These techniques do serve\\nthe purpose of extracting features from the available data but fall\\nshort when the data scales.\\nWhilst the initial literature stresses more on the usage of NoSQL\\ndatabases for OLAP workloads, there has been some research done\\non benchmarking NoSQL databases purely from a query processing\\nperspective for OLAP. The current evaluations of NoSQL databases\\nhave primarily focused on simplistic metrics, such as the loading\\ntime when used as a data warehouse. However, these comparisons'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='perspective for OLAP. The current evaluations of NoSQL databases\\nhave primarily focused on simplistic metrics, such as the loading\\ntime when used as a data warehouse. However, these comparisons\\nfail to capture the nuanced performance differences within a pro-\\ncessing pipeline. Extensive research [6, 7] has delved into NoSQL\\nbenchmarks, yet these studies often prove inadequate for com-\\nprehensive NoSQL database comparisons in a processing context.\\nThey emphasize the stark disparities between new benchmarks and\\ntraditional Relational benchmarks, illustrating the superiority of\\nthe former. For instance, Chevalier et al. ’s [6] introduction of Koal-\\naBench — a versatile benchmarking tool—enables the analysis of\\nboth relational and NoSQL databases, thereby addressing this gap.\\nNumerous studies explore different aspects of NoSQL databases or\\ncompare various NoSQL solutions. For instance, [13] highlights the\\nsignificance of Graph Databases in data analysis, while [7] provides'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='compare various NoSQL solutions. For instance, [13] highlights the\\nsignificance of Graph Databases in data analysis, while [7] provides\\na comparative analysis of three popular NoSQL databases in the\\ncontext of Big Data and Cloud Computing. Moreover, [12] details\\nthe evaluation of Columnar stores, particularly focusing on HBase\\nand utilizing the Star schema as a metric. Additionally, [13] con-\\nducts a comparative study between HBase and MongoDB using a\\nstar schema setup, further enriching our understanding of these\\ndatabase systems.\\n3 THE GENERIC PIPELINE\\nThe generic pipeline that we envisioned consists of three broad\\ncomponents - data processing, the data store and the data analytics\\nsolution. By feeding data to the data processor, we transform the\\ndata into a usable format for the data store. All the pipelines we have\\ncreated uses a different type of NoSQL database as its data store.\\nFinally, all the pipelines culminate with a data analytics solution'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='created uses a different type of NoSQL database as its data store.\\nFinally, all the pipelines culminate with a data analytics solution\\nthat reads off the processed data stored in the database to create'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nFigure 1: Schema of the dataset based off the Snow Logical\\nData Model generated by Koalabench\\nanalytical insights. The pipeline can be further explained in the\\nfollowing sections:\\n3.1 Data Processing\\nThe data processor section consists of two major components -\\nApache HDFS and Apache Spark. All the data files required for the\\npipeline are first loaded on to HDFS. Thanks to the distributed\\nstorage and distributed processing capabilities of using HDFS and\\nSpark in tandem, we were able to load data from HDFS and utilize\\nPySpark to load the transformed data into the database.\\nThe raw data we load into HDFS is provided to us by Koalabench\\n[14]. Koalabench can be broadly defined as a decision support bench-\\nmark for Big Data requirements. It is derived from the TPC-H\\nbenchmark, the reference benchmark in research and industry for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='[14]. Koalabench can be broadly defined as a decision support bench-\\nmark for Big Data requirements. It is derived from the TPC-H\\nbenchmark, the reference benchmark in research and industry for\\ndecision support systems. It has been adapted to support Big Data\\ntechnologies such as NoSQL databases and HDFS. It can generate\\ndata in different data formats and in different data models. There\\nare four primary data logical models supported by Koalabench, out\\nof which the two data models that we have used are the following :\\n3.1.1 Snowflake Data Model . This data model is very close to\\nthe one used in the TPC- H benchmark with small modifications.In\\na classic snowflake data model, there are three major components.\\nFact Tables are the prominent tables based on which most of the data\\nquerying is performed - ideally they are the centre of attraction in\\na snowflake data model. Component Tables are supporting tables to\\nthe fact table in the sense that they contain most of the required data'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='a snowflake data model. Component Tables are supporting tables to\\nthe fact table in the sense that they contain most of the required data\\nattributes to know more about the fact table records. Relationship\\ntables are used when multiple component tables need to be linked\\nto create a compound component table that aids in providing more\\ninformation about the fact table records.\\nAs per Figure 1, the element in green, Lineitem would be the only\\nfact table for our data model. All tables except PartSupplier will be\\ncomponent tables which aid in providing attributes to Lineitem via\\nsimple relationships. PartSupplier is a relationship table composed\\nof attributes from Part and Supplier component tables.\\n3.1.2 Flat Data Model . This is the simplest data model of all\\nthe supported ones. As per Figure 2, Lineitem carries over as the\\nFigure 2: Schema of the dataset based off the Flat Logical Data\\nModel generated by Koalabench\\npremier entity based on which all the data features are developed'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='Figure 2: Schema of the dataset based off the Flat Logical Data\\nModel generated by Koalabench\\npremier entity based on which all the data features are developed\\non. In the flat data model, the significant data parameters from\\ncomponent tables is included within the Lineitem fact table. All\\nrelationship tables removed as the flat data model promotes a sim-\\npler way of storing data without the need for establishing complex\\nrelationships.\\nThe common file format used by all the pipelines for their raw\\ndata is CSV and we have utilized the datasets generated using the\\nflat logical data model for the NoSQL databases and the dataset\\nbased on the Snowflake logical data model has been used in the\\nPostgreSQL pipeline. This helps us evaluate the performance of the\\ndatabases with the way in which they handle scalable data upon\\ndata ingestion.\\nThe data from HDFS is pushed to Spark for performing data\\ntransformations. As our test environment had limited hardware re-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='data ingestion.\\nThe data from HDFS is pushed to Spark for performing data\\ntransformations. As our test environment had limited hardware re-\\nsources to play with, we resorted to utilizing thenum_partitions fea-\\nture of Spark. In Apache Spark, num_partitions refers to the param-\\neter used to specify the number of partitions to divide an Resilient\\nDistributed Dataset (RDD) or dataframe into during processing.\\nProperly setting num_partitions can optimize performance by bal-\\nancing data distribution across the available computing resources.\\nIt is essential to choose an appropriate value for num_partitions to\\nensure efficient parallel processing and avoid resource contention\\nin Spark applications.\\n3.2 Data Store\\nOnce the processsed data is ready from Spark, we begin the process\\nof inserting the records into the data store component. For simplic-\\nity, we have taken up different NoSQL databases to act as the data\\nstore for the processed data from Spark. Spark supports interactions'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='ity, we have taken up different NoSQL databases to act as the data\\nstore for the processed data from Spark. Spark supports interactions\\nwith multiple popular SQL and NoSQL databases which has allowed\\nus to use four different NoSQL databases based on the four broad\\nclassifications of NoSQL databases. They are as follows:\\n3.2.1 MongoDB. MongoDB [15] is a popular NoSQL database\\nknown for its flexible document-oriented data model, which stores\\ndata in BSON (Binary JSON) format. It offers scalability and high per-\\nformance, with features like sharding and replication for handling\\nlarge-scale deployments. MongoDB’s expressive query language\\nand rich ecosystem of tools make it well-suited for a wide range of\\napplications, from web development to analytics.\\n3.2.2 ArangoDB. ArangoDB [24] is a multi-model database sys-\\ntem supporting document, key/value, and graph data models in\\na single database core. It offers a versatile query language, AQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\n(ArangoDB Query Language), and boasts features like distributed\\ngraph processing and geo-spatial indexing. With its flexible data\\nmodel and powerful querying capabilities, ArangoDB is suitable for\\ndiverse use cases including social networks, recommendation en-\\ngines, and real-time analytics. For the purposes of our experiments,\\nwe have leveraged the graph data models of ArangoDB owing to\\nthe Graph databases family of NoSQL solutions.\\n3.2.3 Apache Kudu. Apache Kudu [3] is an open-source columnar\\nstorage engine designed for fast analytics on rapidly changing data.\\nIt combines the performance of traditional columnar databases with\\nthe ease of use of Hadoop. With features like automatic partition-\\ning and fault tolerance, Apache Kudu is ideal for real-time data\\ningestion and interactive analytics applications.\\n3.2.4 Redis. Redis [19] is an in-memory data store known for its'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='ing and fault tolerance, Apache Kudu is ideal for real-time data\\ningestion and interactive analytics applications.\\n3.2.4 Redis. Redis [19] is an in-memory data store known for its\\nhigh performance and versatility in caching, session management,\\nand real-time analytics. It supports various data structures like\\nstrings, hashes, lists, sets, and sorted sets, making it suitable for a\\nwide range of use cases.\\nApart from the mentioned NoSQL databases, we have performed\\none additional experiment using PostgreSQL [23] as the RDBMS\\nsolution for the data store. This experiment has helped us compare\\nand contrast the performance of SQL and NoSQL databases in\\nthe generic pipeline. The processed data has been saved on the\\nrespective databases thanks to the versatility provided by PySpark\\nin developing seamless interaction modules between Spark and the\\ndatabases.\\nOnce the data has been loaded, we have executed a subset of\\nthe queries belonging to the TPC-H benchmark. As Koalabench'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='in developing seamless interaction modules between Spark and the\\ndatabases.\\nOnce the data has been loaded, we have executed a subset of\\nthe queries belonging to the TPC-H benchmark. As Koalabench\\nderives its existence from TPC-H, it has allowed us to reuse the\\nsame template of the documented benchmark queries. However,\\nbased on how the data is stored in the different databases, we have\\ncome up with equivalent queries for each of the NoSQL databases\\nand executed them on the databases respectively.\\n4 ARCHITECTURE\\nFor the evaluation of each proposed data pipeline as part of our\\nbenchmarking process, we plan to execute datasets of varying scale\\nfactors generated from the Koalabench suite. These datasets will\\nbe run through each pipeline within our standardized local testing\\nenvironment, which consists of a system running MacOS Sonoma\\n14.4.1, powered by an Apple M2 chip with 8GB of RAM and 128GB\\nof storage. To support distributed data processing capabilities for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='environment, which consists of a system running MacOS Sonoma\\n14.4.1, powered by an Apple M2 chip with 8GB of RAM and 128GB\\nof storage. To support distributed data processing capabilities for\\nHadoop and Spark, we intend to create a dockerized environment\\nwithin the local system. This environment will comprise a master-\\nworker setup facilitating the execution of distributed tasks.\\nThe architecture shown in Figure 3 consists of one master node\\nconcurrently acting as a worker, along with three dedicated worker\\nnodes. The master node maintains bidirectional communication\\nroutes with each worker node, enabling seamless coordination\\nfor distributed processing across HDFS and Spark clusters. The\\ndockerized environment is built on Debian Linux v11 across all\\nnodes. To facilitate distributed data processing capabilities, each\\nnode is equipped with Hadoop-3.3.6 and Spark-3.4.1.\\nWe have incorporated the latest docker images for all the databases'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='nodes. To facilitate distributed data processing capabilities, each\\nnode is equipped with Hadoop-3.3.6 and Spark-3.4.1.\\nWe have incorporated the latest docker images for all the databases\\nunder evaluation, including Postgres-alpine-3.19, MongoDB-7.0.8,\\nFigure 3: Dockerized experimental setup consisting of one\\nmaster and three worker nodes\\nArangoDB-3.12.0, Redis-7.2.4, and Apache Kudu-1.17. These database\\nimages have been instantiated within the same docker network\\nas the experiment nodes, enabling efficient measurement of time-\\nbased metrics without introducing potential biases stemming from\\nnetwork latency. The data analysis component of our pipeline will\\nleverage a Business Intelligence (BI) tool connected to the respective\\ndatabase docker image for accessing and analyzing stored data. The\\ntentative BI tools identified for this purpose areMetabase v0.49.3 and\\nTableau 2024.1. The BI container will reside within the same docker'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='tentative BI tools identified for this purpose areMetabase v0.49.3 and\\nTableau 2024.1. The BI container will reside within the same docker\\nnetwork as the experimental setup to ensure seamless integration\\nand communication.\\n5 EXPERIMENTS\\nThe following section elaborates on the dataset that we have used\\nto evaluate the performance of the various pipelines and documents\\nthe findings from the individual experiments we have carried out\\nin each data pipeline we developed.\\n5.1 Dataset\\nTo evaluate our pipelines across varying data scales, we have used\\ndifferent datasets generated from Koalabench using the flat data\\nmodel based on varying scale factor(sf). The primary objective of\\nutilizing different sf values is to maximize the dataset size, enabling\\ncomprehensive testing of our pipelines’ performance and scalability\\nacross a broad range of data volumes. We have conducted exper-\\niments on all the pipelines using sf1, sf2, sf3, sf4 and sf5 datasets'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='across a broad range of data volumes. We have conducted exper-\\niments on all the pipelines using sf1, sf2, sf3, sf4 and sf5 datasets\\nwith the data format set as CSV for all datasets. The minimum\\nsize of the collection belongs to the sf1 dataset at 2.38 GB and the\\nmaximum size of the collections belongs to sf5 which scales up to\\napproximately 11GB. For the PostgreSQL pipeline, we have lever-\\naged the snow data model to generate datasets ranging from sf1 to\\nsf5 in the CSV format. The least size is with sf1 at 1.4GB and the\\nmaximum size is with sf5 at approximately 6GB.\\n5.2 Data Loading Time\\nTable 1 portrays the load times for different datasets across the five\\ndifferent databases.\\nApache Kudu has shown the smallest load times across all five\\ndatasets. One of the primary reasons for this observation is the\\ncolumnar-storage mechanism that Apache Kudu employs coupled\\nwith its superior in-memory management that flushes out records'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nTable 1: Data Loading times for all databases across the five\\ndatasets (time in seconds)\\nDatabases SF1 SF2 SF3 SF4 SF5\\nPostgreSQL 37s 375s 857s 1089s 1481s\\nMongoDB 90s 1250s 1701s 2275s 2810s\\nArangoDB 295s 2249s 3964s 12169s 15162s\\nRedis 1495s 3245s 5023s 7748s 10289s\\nApache Kudu 42s 95s 146s 192s 240s\\nas and when the threshold is reached. This allows Kudu to han-\\ndle scaling data efficiently and keep the overall loading time to\\nagreeable levels.\\nMongoDB seems to have progressively higher data ingestion\\ntimes. MongoDB’s document-oriented storage model and lack of\\nnative sharding capabilities may lead to increased data ingestion\\ntimes due to increased indexing and write operations overhead.\\nAdditionally, as the dataset size grows, MongoDB’s reliance on\\nmemory-mapped files for storage may result in slower write per-\\nformance and increased disk I/O operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Additionally, as the dataset size grows, MongoDB’s reliance on\\nmemory-mapped files for storage may result in slower write per-\\nformance and increased disk I/O operations.\\nRedis displays the poorest performance on all 5 datasets. The\\nvery high data loading times observed in Redis can be attributed\\nprimarily to the schema we used for the processed data. In order\\nto keep the schema uniform across all NoSQL databases, we opted\\nfor having the linenumber_id as the only key and all the remaining\\ndata features were aggregated into a single list and attributed to\\nthe corresponding linenumber_id. In general, we ended up creating\\na record that had a single key and 40 values. Additionally, redis\\nbeing a single threaded solution meant that for every insertion\\noperation had two call operations to make. With data scaling, this\\nwould increase exponentially and in the end be double of what an\\nideal PostgreSQL implementation would execute.\\nArangoDB was implemented using its document model and its'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='would increase exponentially and in the end be double of what an\\nideal PostgreSQL implementation would execute.\\nArangoDB was implemented using its document model and its\\ndata insertion times are exorbitant owing to the document model\\nthat was implemented. ArangoDB is definitely not an ideal solution\\nshould the data scale as for sf5, the data insertion time is almost\\n50x what it was for sf1.\\nPostgreSQL shows a gradual increase in data insertion times from\\nsf1 all the way upto sf5. It is able to handle scaling data much more\\nefficiently has compared to some of its NoSQL counterparts.\\nBased on this experiment, it is clear that with a NoSQL database\\nas your data store, it is preferrable to choose a columnar storage\\nsolution in case the pipeline has to be robust enough to handle\\nscaling data.\\n5.3 Query Processing Time\\nAs Koalabench is based off the TPC-H benchmark, we selected five\\nout of the seventeen benchmark queries that TPC-H has to offer. The'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='scaling data.\\n5.3 Query Processing Time\\nAs Koalabench is based off the TPC-H benchmark, we selected five\\nout of the seventeen benchmark queries that TPC-H has to offer. The\\nTPC-H benchmark is meant for running benchmark experiments\\non RDBMS solutions. Hence, the queries can be directly applied\\nto the PostgreSQL data pipeline as the schema is same as the one\\nproposed by TPC-H. For all NoSQL data pipelines that leverage\\nthe flat data model, we prepared queries in the native querying\\nlanguage of the NoSQL database present in the pipeline.\\nThe five queries we selected were different from one another\\nbased on the significant operation that was being performed by the\\nFigure 4: Query Execution time (in seconds) for PostgreSQL\\nacross the five datasets\\nquery. For context, Query 1 was aggregation intensive in which\\nalmost eight out of the ten values that were being fetched were\\nbased off aggregation. Query 2 had a good balance of aggregation'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='query. For context, Query 1 was aggregation intensive in which\\nalmost eight out of the ten values that were being fetched were\\nbased off aggregation. Query 2 had a good balance of aggregation\\nand join operations. Query 3 had an aggregation along with a sub-\\nquery based filter. Query 4 had five join operations and Query 5\\nwas the simplest query off them all with just a single aggregation\\noperation.\\n5.3.1 PostgreSQL. Figure 4 shows a graph depicting the query\\nexecution times for the PostgreSQL data pipeline. Across the five\\ndatasets and for the five queries, we see a steady increase by a factor\\nof 2x for almost all queries except Queries 3 and 4. The reason for\\nthe observation of exponential rise in query execution times for\\nthose two queries alone is because of the inherently costly join\\noperations of RDBMS solutions which tend to compare every row\\nof the two tables. For filtering and aggregation queries, the increase\\nin query execution time is 0.33x for every 2x scale in data which is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='of the two tables. For filtering and aggregation queries, the increase\\nin query execution time is 0.33x for every 2x scale in data which is\\npretty commendable.\\n5.3.2 MongoDB. Figure 5 shows a graph displaying the query\\nexecution times for the MongoDB pipeline for the five selected\\nbenchmark queries across the five datasets. MongoDB uses the\\ntranslated version of the benchmark queries and the major change\\nbetween the original and translated versions are that all join related\\noperations become filter related operations. This is because of the\\nnon-availability of support for joins in NoSQL databases.\\nMongoDB handles most of the queries easily with scaling data\\nwith a maximum increase in execution times by about 0.8x for every\\n2x increase in data. This can be attributed to MongoDB’s inbuilt\\nsharding capabilities, that promote horizontal scaling approach that\\nallows it to handle larger data sets and provide high throughput\\noperations by distributing data across multiple shards. One aspect'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='allows it to handle larger data sets and provide high throughput\\noperations by distributing data across multiple shards. One aspect\\nwhere MongoDB struggles is with intense aggregation queries\\nsuch as Query 1. With data scaling, MongoDB is forced to look\\nup multiple document objects and perform compute operations on\\nthem which are CPU intensive and tend to consume more time\\nthan rest of the query operations.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian\\nFigure 5: Query Execution time (in seconds) for MongoDB\\nacross the five datasets\\nFigure 6: Query Execution time (in seconds) for ArangoDB\\nacross the five datasets\\n5.3.3 ArangoDB. Figure 6 shows a graph displaying the query\\nexecution times for the ArangoDB pipeline for the five selected\\nbenchmark queries across the five datasets. ArangoDB uses the\\nArango Query Language (AQL) version of the benchmark queries.\\nArangoDB sees a 1.5x scale in query execution times for every\\n2x scale in data size. Whilst ArangoDB does not selectively out-\\nperform other data pipelines in certain scenarios, it does show a\\nsteady increase in query execution time with scaling data. This can\\nbe attributed to the dynamic query optimizer that selects the per-\\nfect query execution plan depending on the query being executed.\\nArangoDB employs efficient memory management techniques to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='be attributed to the dynamic query optimizer that selects the per-\\nfect query execution plan depending on the query being executed.\\nArangoDB employs efficient memory management techniques to\\nminimize disk I/O and maximize query performance. It utilizes\\nmemory caches for frequently accessed data and implements buffer\\nmanagement strategies to optimize disk access.\\nFigure 7: Query Execution time (in seconds) for Redis across\\nthe five datasets\\nFigure 8: Query Execution time (in seconds) for Apache Kudu\\nacross the five datasets\\n5.3.4 Redis. Figure 7 portrays a graph containing information on\\nthe query execution times of the five benchmark queries across the\\nfive datasets on Redis. Irrespective of the size of data, Redis takes\\nlonger than all the other databases to execute the queries. For sf1,\\nthe smallest dataset, Redis displays a best case of 3x increase in\\nterms of query execution times. Redis suffers primarily from the\\ndata schema we had selected for the flat data model. By assigning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='the smallest dataset, Redis displays a best case of 3x increase in\\nterms of query execution times. Redis suffers primarily from the\\ndata schema we had selected for the flat data model. By assigning\\nall features to a single key, linenumber_id, a single row of record\\nbecomes huge for redis to retrieve from memory. By default, redis\\nis an in-memory intensive data store and hence, holding such a big\\nrecord in memory is bound to make the query execution slow and\\nCPU intensive.\\n5.3.5 Apache Kudu. Figure 8 shows the recorded query execu-\\ntion times for the five benchmark queries across the five datasets.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB\\nBased on these readings, it is clear that Apache Kudu performs\\nthe best amongst all the selected databases in terms of query exe-\\ncution. Kudu stores data in a columnar format and this is highly\\nefficient for analytical workloads because it allows queries to read\\nonly the columns needed for the query, minimizing I/O operations\\nand improving query performance. This can be visibly seen from\\nthe very minimal increase in query execution times across datasets\\nfor queries 2 to 5. Even when data scales at 2x, query execution\\ntimes have tended to stay flat and do not see a proportional or\\nexponential increase like the rest of the databases. The reason why\\nquery 2 sees an exponential increase with scaling data is related\\nto a potentially incorrect time measurement on behalf of Apache\\nKudu. When fetching data records based on given filter conditions,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='query 2 sees an exponential increase with scaling data is related\\nto a potentially incorrect time measurement on behalf of Apache\\nKudu. When fetching data records based on given filter conditions,\\nKudu tends to write the retrieved records onto the impala shell.\\nThe write operation time is also included in the overall execution\\ntime of the query and thus it seems beefed up with scaling data\\ncompared to the rest of the queires.\\n6 CONCLUSION\\nThrough this paper, we envisioned our idea of building the ideal data\\nengineering pipeline, that would be take care of data processing and\\ndata store. We experimented with datasets of varying scale from the\\nKoalabench dataset where data was generated in two different data\\nmodels. Our experiments measured the query execution times for\\n5 benchmark queries and recorded the data ingestion time for all\\ndatabases. The effect of the data schema was evident in the varied\\ndata ingestion times. Certain databases did not handle the scaling'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='5 benchmark queries and recorded the data ingestion time for all\\ndatabases. The effect of the data schema was evident in the varied\\ndata ingestion times. Certain databases did not handle the scaling\\nlevels of data and ended up seeing exponential increase in the query\\nexecution times of complex queries. A definitive finding from our\\nexperiments is the prominence of Columnar data storage options\\nin future OLAP research. Columnar storages tend to load data 2x\\nfaster than other SQL and NoSQL options and the same goes for\\nquery execution as well.\\nThese experiments also highlight the immense scaling and dis-\\ntributed processing capabilities that modern-day NoSQL solutions\\nhave and how these can be put to use to solve complex data analyt-\\nical problems. We believe this idea would be a pioneer towards the\\nadoption of more NoSQL-based data store solutions for evaluating\\nOLAP workloads. As part of future work, we do recommend the in-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='ical problems. We believe this idea would be a pioneer towards the\\nadoption of more NoSQL-based data store solutions for evaluating\\nOLAP workloads. As part of future work, we do recommend the in-\\ntegration of a data analysis tool to commensurate the data pipeline.\\nResearch can be extended across various columnar storage options\\nto identify the ideal columnar storage option to evaluate OLAP\\nworkloads.\\nREFERENCES\\n[1] Apache. 2006. Hadoop Distributed File System. (2006). https://hadoop.apache.\\norg/\\n[2] Apache. 2014. Apache Spark. (2014). https://spark.apache.org/\\n[3] Apache. 2022. Apache Kudu. (2022). https://kudu.apache.org\\n[4] Ronald Barber, et.al. 2017. Evolving Databases for New-Gen Big Data Applica-\\ntions.\\n[5] Zane Bicevska and Ivo Oditis. 2017. Towards NoSQL-based Data Warehouse\\nSolutions. Procedia Computer Science 104, 104–111. https://doi.org/10.1016/j.\\nprocs.2017.01.080 ICTE 2016, Riga Technical University, Latvia.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Solutions. Procedia Computer Science 104, 104–111. https://doi.org/10.1016/j.\\nprocs.2017.01.080 ICTE 2016, Riga Technical University, Latvia.\\n[6] Max Chevalier, Mohammed El Malki, Arlind Kopliku, Olivier Teste, and Ronan\\nTournier. 2015. Benchmark for OLAP on NoSQL technologies comparing NoSQL\\nmultidimensional data warehousing solutions. In 2015 IEEE 9th International\\nConference on Research Challenges in Information Science (RCIS) . 480–485. https:\\n//doi.org/10.1109/RCIS.2015.7128909\\n[7] Max Chevalier, Mohammed El malki, Arlind Kopliku, Olivier Teste, and Ronan\\nTournier. 2015. Implementing Multidimensional Data Warehouses into NoSQL.\\nICEIS 2015 - 17th International Conference on Enterprise Information Systems,\\nProceedings 1. https://doi.org/10.5220/0005379801720183\\n[8] Farnaz Davardoost, Amin Babazadeh Sangar, and Kambiz Majidzadeh. 2020.\\nExtracting OLAP Cubes From Document-Oriented NoSQL Database Based on\\nParallel Similarity Algorithms. Canadian Journal of Electrical and Computer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Extracting OLAP Cubes From Document-Oriented NoSQL Database Based on\\nParallel Similarity Algorithms. Canadian Journal of Electrical and Computer\\nEngineering 43, 2 (2020), 111–118.\\n[9] Lucas de Carvalho Scabora, Jaqueline Joice Brito, Ricardo Rodrigues Ciferri,\\nand Cristina Dutra de Aguiar Ciferri. 2016. Physical Data Warehouse Design\\non NoSQL Databases - OLAP Query Processing over HBase. In International\\nConference on Enterprise Information Systems . https://api.semanticscholar.org/\\nCorpusID:2636732\\n[10] Jeffrey Dean and Sanjay Ghemawat. 2004. MapReduce: Simplified Data Process-\\ning on Large Clusters.\\n[11] Khaled Dehdouh. 2016. Building OLAP Cubes from Columnar NoSQL Data\\nWarehouses. In Model and Data Engineering , Ladjel Bellatreche, Óscar Pastor,\\nJesús M. Almendros Jiménez, and Yamine Aït-Ameur (Eds.).\\n[12] Khaled Dehdouh, Omar Boussaid, and Fadila Bentayeb. 2014. Columnar NoSQL\\nStar Schema Benchmark. In Model and Data Engineering , Yamine Ait Ameur,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='[12] Khaled Dehdouh, Omar Boussaid, and Fadila Bentayeb. 2014. Columnar NoSQL\\nStar Schema Benchmark. In Model and Data Engineering , Yamine Ait Ameur,\\nLadjel Bellatreche, and George A. Papadopoulos (Eds.). Cham.\\n[13] David Dominguez-Sal, Norbert Martinez-Bazan, Victor Muntes-Mulero, Pere\\nBaleta, and Josep Lluis Larriba-Pey. 2010. A discussion on the design of graph\\ndatabase benchmarks. In Technology Conference on Performance Evaluation and\\nBenchmarking. Springer, 25–40.\\n[14] Mohammed El Malki, et al. 2022. Benchmarking Big Data OLAP NoSQL\\nDatabases. In Ubiquitous Networking, 2018, pp. 82–94 .\\n[15] Kevin P. Ryan Eliot Horowitz, Dwight Merriman. 2009. MongoDB. (2009).\\nhttps://www.mongodb.com/\\n[16] Abdelhak Khalil and Mustapha Belaissaoui. 2023. An Approach for Implementing\\nOnline Analytical Processing Systems under Column-Family Databases. IAENG\\nInternational Journal of Applied Mathematics 53, 1 (2023).\\n[17] Astrahan M., et. al. [n.d.]. System R: Relational Approach to Database Manage-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='International Journal of Applied Mathematics 53, 1 (2023).\\n[17] Astrahan M., et. al. [n.d.]. System R: Relational Approach to Database Manage-\\nment.\\n[18] Fatma Özcan, Yuanyuan Tian, and Pinar Tözün. 2017. Hybrid Transac-\\ntional/Analytical Processing: A Survey. In Proceedings of the 2017 ACM Interna-\\ntional Conference on Management of Data (SIGMOD ’17). Association for Com-\\nputing Machinery, 1771–1775. https://doi.org/10.1145/3035918.3054784\\n[19] Salvatore Sanfilippo. 2009. Redis. (2009). https://redis.io/\\n[20] Nadia Ben Seghier and Okba Kazar. 2021. Performance benchmarking and\\ncomparison of NoSQL databases: Redis vs mongodb vs Cassandra using YCSB\\ntool. In 2021 International Conference on Recent Advances in Mathematics and\\nInformatics (ICRAMI). IEEE, 1–6.\\n[21] Salman Ahmed Shaikh and Hiroyuki Kitagawa. 2020. StreamingCube: Seamless\\nIntegration of Stream Processing and OLAP Analysis. IEEE Access 8 (2020),\\n104632–104649. https://doi.org/10.1109/ACCESS.2020.2999572'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with acmart 2020/02/22 v1.70 Typesetting articles for the Association for Computing Machinery and hyperref 2023-04-22 v7.00x Hypertext links for LaTeX', 'creationdate': '2024-05-29T00:16:52+00:00', 'author': 'Rishi Kesav Mohan, Risheek Rakshit Sukumar Kanmani, Krishna Anandan Ganesan, and Nisha Ramasubramanian', 'keywords': '', 'moddate': '2024-05-29T00:16:52+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': 'Evaluating NoSQL Databases for OLAP Workloads: A Benchmarking Study of MongoDB, Redis, Kudu and ArangoDB', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Big data]_Evaluating NoSQL Databases for OLAP Workloads.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='Integration of Stream Processing and OLAP Analysis. IEEE Access 8 (2020),\\n104632–104649. https://doi.org/10.1109/ACCESS.2020.2999572\\n[22] Konstantin Shvachko, Hairong Kuang, Sanjay Radia, and Robert Chansler. 2010.\\nThe Hadoop Distributed File System. In 2010 IEEE 26th Symposium on Mass\\nStorage Systems and Technologies (MSST) . 1–10. https://doi.org/10.1109/MSST.\\n2010.5496972\\n[23] Michael Stonebreaker. 1996. PostgreSQL. (1996). https://www.postgresql.org/\\n[24] Claudius Weinberger and Frank Celle. 2015. ArangoDB. (2015). https://arangodb.\\ncom/\\n[25] Matei Zaharia, et al. 2016. Apache Spark. Communications of the ACM, vol. 59,\\nno. 11, 28 Oct. 2016, pp. 56–65 .'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 14 \\n \\nInfluence of CAP Theorem on Big Data Analysis  \\nDr Anand Kumar Pandey [2], Rashmi Pandey [2] \\n[1] Computer Science and Application, ITM University \\n[2] Computer Science, ITM Group of Institute - Gwalior \\n \\nABSTRACT \\nIn the current modern world different computing society has developed their innovative solutions to meet the difficult \\nchallenges to handle linear expansion in large data collection by different sources. In this paper, we took a research oriented \\nview to achieve more significant ideas in the field of big data world and data science engineering. The CAP Theorem is a \\ncommonly cited unfeasibility outcome in distributed computing systems, particularly with NoSQL distributed databases. Cap'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='commonly cited unfeasibility outcome in distributed computing systems, particularly with NoSQL distributed databases. Cap \\ntheorem is very much influenced by cloud providers in the reference of their usability, the latency limit and system requirement. \\nThe Cap theorem is also very much influenced by distributer database also. To discover a substitute for CAP with a latency-\\ncentric point of view we have to observe how operation latency is exaggerated by network latency at dissimilar levels of \\nconsistency. \\nKeywords:-  CAP, Big Data, Analysis, Distributed System, NoSQL. \\n \\nI.     INTRODUCTION \\n   The CAP theorem is also known as Brewer’s Theorem, \\nbecause it was introduced by MIT Professor Eric A. Brewer \\nduring 2000 with the concept of distributed computing. Our \\nmain purpose in this paper is to consider the influence of CAP \\nTheorem in the broader perspective of big data analysis and \\ndistributed computing theory. \\nData analysis is usually applicable on current distributed big'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='Theorem in the broader perspective of big data analysis and \\ndistributed computing theory. \\nData analysis is usually applicable on current distributed big \\ndata centres to accomplish high performance and accessibility. \\nMost of the data science services try to preserve their services \\nstability in all situations. In modern world big data analysis \\nand distributed database system is bound to have partitions in \\na real-world system due to network failure or some other \\nreason. To describe the practical implementation of CAP \\ntheorem we can choose any real big data computing \\nenvironment for data analysis such as MongoDB, Cassandra \\nand with NoSQL database [2] . CAP theorem describes tha t \\nbefore choosing any Database including distributed database, \\naccording to your requirement we have to choose only \\nappropriate properties out of three. CAP theorem allows us to \\nfind out how we want to operate our distributed database \\nsystems when some other database servers decline to'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='appropriate properties out of three. CAP theorem allows us to \\nfind out how we want to operate our distributed database \\nsystems when some other database servers decline to \\ncommunicate with each other due to some imperfection in the \\nsystem. During data analysis operations we try to retain the \\noriginality of actual data received from big data pool and \\nfollows all the suitable rules and regulations. Different types \\nof database segments, operators and users are activated during \\nthe task of appropriate data analysis. Therefore it is very \\nimportant to know about that which segment of dataset is \\nconsistent and suitable to apply some partition tolerance \\nrelated operations. Services availability and database partition \\ntolerance are inter-dependent to each other. It seems \\nmotivating to investigate which levels of regularity and \\nreliability are strong enough to be straight implicit by the CAP \\nconstraints. \\n \\nII.     THE CONCEPTUAL ASPECTS OF CAP \\nTHEOREM'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='motivating to investigate which levels of regularity and \\nreliability are strong enough to be straight implicit by the CAP \\nconstraints. \\n \\nII.     THE CONCEPTUAL ASPECTS OF CAP \\nTHEOREM \\nThe CAP theorem explained the thought that there is a \\nelementary transaction between availability, consistency, and \\npartition tolerance. This transaction, which has become \\nidentified as the CAP Theorem, has been commonly discussed \\never since.  CAP theorem supports non-relational database \\n(NoSQl) are best for distributed network applications and big \\ndata analysis [1]. \\n   With description of CAP theorem Professor Brewer \\nillustrates that it not possible that a distributed computer \\nsystem can support the 3 following properties at a time: \\n \\nFig. 1  CAP Theorem and Distributed Database \\nmanagement \\n    \\n     CAP Theorem is a considered that a distributed database \\norganization can only consist of 2 of the 3 properties : \\nConsistency, Availability and Partition Tolerance as shown in'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 0, 'page_label': '1'}, page_content='management \\n    \\n     CAP Theorem is a considered that a distributed database \\norganization can only consist of 2 of the 3 properties : \\nConsistency, Availability and Partition Tolerance as shown in \\nFigure 1. Some of the well known attention about the CAP \\nTheorem which derived from the fact is as follows: \\n \\n \\nRESEARCH ARTICLE                                                OPEN ACCESS'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 15 \\nA. Safety \\nCAP theorem has standard protection property, because \\nevery client sent correct request and receives correct response \\nfrom data centres. These data centre can be personalised or \\ngeneralized.   \\nB. Liveness \\nThe theorem is well defined but overall, it is quite lively. \\nAvailability is a classic liveness property because, every \\nrequest receives a response. \\nC. Unreliable \\n   There are so many different ways in which a system seems \\nunreliable. Since every request at last receives a response so \\nthat structure can be unreliable. There may be partitions, as is \\ndiscussed in the CAP Theorem. \\nIII. THE ROLE OF CAP THEOREM IN BIG \\nDATA ANALYSIS \\n     During the review of CAP theorem as a consider its role'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='discussed in the CAP Theorem. \\nIII. THE ROLE OF CAP THEOREM IN BIG \\nDATA ANALYSIS \\n     During the review of CAP theorem as a consider its role \\nand its influence for Big data analysis to achieve the \\ndistributed solutions. May be the first time solution is not \\nperfect, so it needs to repeat the process of data analysis and \\nagain identify the best appropriate solution. In this case we are \\ntrying to get solutions for such kind of big data distributed \\nsystem it is impossible to assurance all three features of Cap \\ntheorem like Consistency, availability and partition tolerance \\nall at the equivalent time. Here we try to analyses the \\napplicability and relationship of the CAP theorem with Big \\nData and distributed systems [1]. The CAP theorem is also \\nrelate with Hadoop, Big data Analytics, DBMS, network \\ncommunication system and with advanced data structure.  \\n    During data analysis at first we have to partition the data in'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='relate with Hadoop, Big data Analytics, DBMS, network \\ncommunication system and with advanced data structure.  \\n    During data analysis at first we have to partition the data in \\nto appropriate segments. At the time of partitioning the user \\ninteracts with operator and operator interacts with database. \\nLets discuss the relationship of CAP theorem with big data \\ncentres and their nodes associated with wireless area network. \\nAs shown in figure 2 we have two data centres with their \\nindividual single server nodes and interconnected with \\ndedicated network [3]. Here we try to propose a framework of \\ninterconnected big data centres for specifying a huge set of \\ndistributed data consistency model. The relationship of \\ndistributed big data described the correlated aspects of CAP \\ntheorem as per their requirements and their needs in this real \\nworld also. \\n \\n \\n \\n \\nFig. 2  Relationship of database \\n    In this framework the CAP theorem involves as a software'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='theorem as per their requirements and their needs in this real \\nworld also. \\n \\n \\n \\n \\nFig. 2  Relationship of database \\n    In this framework the CAP theorem involves as a software \\nservice consist in distributed system which makes wisdom to \\nbelieve those reliability models in this extent. In above \\nrelational database, we have achieved Availability of the both \\ndata centres with respect to their concerned nodes as well \\nas Partition Tolerance in both data centres even if they cannot \\ncorrespond [5]. A communicated network divider is a \\nparticular type of communication defect that divides the \\nnetwork into subsets of nodes such that nodes in one subset \\ncannot communicate with other nodes.\\n \\n \\nIV. FUTURE OF CAP THEOREM \\n   The concept of CAP theorem is best possible futuristic \\napproach to discuss basic trade off for available big data \\nanalytic solutions and distributed systems. In future it will \\neasily scrutinize the inbuilt trade off some insights into that'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='approach to discuss basic trade off for available big data \\nanalytic solutions and distributed systems. In future it will \\neasily scrutinize the inbuilt trade off some insights into that \\nhow system can be considered to gather an application’s needs, \\nin spite of unpredictable networks [6]. With the reference of \\nCAP theorem we must know all theoretical aspects to achieve \\nthese challenges, and some modern techniques for supporting \\nwith the problem in real big data world system.  \\nIn this artificial intelligent and Big data world, the \\nnetworked world has altered significantly in the last two \\ndecades, creating new challenges for system designers, and \\nnew areas in which these same inherent trade-offs can be \\nexplored. We need new theoretical insights to address these \\nchallenge, and new techniques for coping with the problem in \\nreal-world systems. \\n \\nA. Mobile Wireless Network \\n    The CAP Theorem primarily determined on modern'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='challenge, and new techniques for coping with the problem in \\nreal-world systems. \\n \\nA. Mobile Wireless Network \\n    The CAP Theorem primarily determined on modern \\nwireless network services. Now days, we illustrate that its \\nconsiderable growth proportion of network communication \\ngoing to initiated by advanced mobile devices. A big data \\ndistributed database system is dedicated to comprise partitions \\nin a real-world system due to network failure or some other \\nreason. \\n    Especially, wireless network communication is particularly \\nuntrustworthy. The main problem that the frequency is going \\nto change quickly, so it is not easy to motivated the CAP \\nTheorem for stable partitions. In every wireless networks, \\npartitions are less common. After re-evaluating the CAP \\nTheorem in the framework of wireless networks, we expect to \\nbetter recognize the best appropriate solutions that take place \\nin these types of scenarios [4]. By re-examining the CAP'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 1, 'page_label': '2'}, page_content='Theorem in the framework of wireless networks, we expect to \\nbetter recognize the best appropriate solutions that take place \\nin these types of scenarios [4]. By re-examining the CAP \\nTheorem in the situation of wireless networks, we might hope \\nto enhanced understand the unique trade-offs that occur in \\nthese types of scenarios. \\n \\nB. Scalability \\n    The CAP Theorem describes that in the current scenario of \\na network partition, the administrator has to decide between \\nconsistency and availability. Gradually, we involve that our \\nsystems be designed, scalable not just for today’ s consumers \\nbut also for future growth . Spontaneously, we believe that'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='International Journal of Information Technology (IJIT) – Volume 6 Issue 6, Nov - Dec 2020 \\nISSN: 2454-5414                                              www.ijitjournal.org                                                    Page 16 \\nstructure as scalable if it can mature resourcefully, using \\ninnovative resources capably to handle extra load. \\n \\nC. Tolerating Attacks \\nPartition tolerance describes those clusters that must \\ncontinue to work even though any number of communication \\nbreakdowns between nodes in the system. The CAP Theorem \\nfocuses on network partitions: occasionally, a number of \\nservers did not communicate consistently [5]. Progressively, \\nhowever, we felt that more rigorous attacks on networks. \\nTolerating these extra problematic forms of interruption \\nrequires a somewhat different understanding of the \\nfundamental consistency/ availability trade-offs. A rejection \\nof service attack, however, cannot basically be modeled as a \\nnetwork partition.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='requires a somewhat different understanding of the \\nfundamental consistency/ availability trade-offs. A rejection \\nof service attack, however, cannot basically be modeled as a \\nnetwork partition. \\n \\nV.    SEGMENTING TASK OF DATA   \\n  ANALYSIS \\nThose systems who are using aspects of CAP theorem some \\nof them many systems do not include a single uniform \\nrequirement. Some aspects of the system require strong \\nconsistency, and some require high availability. In this section \\nof the paper, we describe few of the dimensions along which a \\nsystem might be partitioned. It is not always clear the specific \\nguarantees that such segmentation provides, as it tends to be \\nspecific to the given application and the particular partitioning. \\n \\nA. Data Partitioning \\n     In this big data world different types of data analysis may \\nrequire different levels of consistency and availability. For \\nexample, an on-line shopping cart may be highly available,'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='In this big data world different types of data analysis may \\nrequire different levels of consistency and availability. For \\nexample, an on-line shopping cart may be highly available, \\nresponding rapidly to user requests; yet it may be occasionally \\ninconsistent, losing a recent update in anomalous \\ncircumstances. The on-line product information for an e-\\ncommerce site may be somewhat inconsistent: users will \\ntolerate somewhat out- of-date inventory information. The \\ncheck-out/billing/shipping records, however, have to be \\nstrongly consistent. \\n \\nB. Functional Partitioning \\n     Many services can be divided into different subservices \\nwhich have different requirements. For example, an \\napplication might use a service for coarse-grained locks and \\ndistributed coordination. Whatever service or function we \\nneed to be use can be partition as per condition and \\nrequirement. \\n \\nC. Operation Partitioning \\n     Different operations may require different levels of'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='need to be use can be partition as per condition and \\nrequirement. \\n \\nC. Operation Partitioning \\n     Different operations may require different levels of \\nconsistency and availability. Moreover, different types of \\nupdates might provide different levels of consistency. The \\nCAP theorem and its data analysis provide with differing \\ntrade-offs for different types of read and write operations. \\n \\n \\n \\nD. User Partitioning \\n     Network partitions, and unfortunate network performance \\nin general, normally correlate with real geographic distance: \\nusers that are far away are more likely to see poor \\nperformance. Usually, one could imagine that a social \\nnetworking site might try to partition its users, ensuring high \\navailability among groups of friends. \\nVI.    CONCLUSIONS \\n   In this paper we discussed several aspects of the CAP \\ntheorem: the definitions, the conceptual aspects of Cap \\ntheorem, the role of cap theorem in Big data analysis, future'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='VI.    CONCLUSIONS \\n   In this paper we discussed several aspects of the CAP \\ntheorem: the definitions, the conceptual aspects of Cap \\ntheorem, the role of cap theorem in Big data analysis, future \\nof CAP theorem in the literature are fairly paradoxical and \\ncounter- intuitive. The Cap theorem is also very much \\ninfluenced by distributer database also and it is not possible to \\nmake available reliable data on both the nodes and \\naccessibility of complete data. The CAP theorem describes \\nthat proposed distributed database system has to compose a \\ntransaction between Consistency and Availability when a \\nPartition occurs. \\n  \\nREFERENCES \\n[1] Brewer EA (2012) CAP twelve years later: How the \\n“rules” have changed. IEEE Computer 45(2):23–29, DOI \\n10.1109/MC.2012.37. \\n[2] Daniel J Abadi. Consistency tradeoffs in modern \\ndistributed database system design. IEEE Computer \\nMagazine, 45(2):37 – 42, February 2012. \\ndoi:10.1109/MC.2012.33. \\n[3] Dobre D, Viotti P, Vukolic M (2014) Hybris: Robust'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': '2021-01-03T09:01:06-08:00', 'moddate': '2021-01-03T09:01:06-08:00', 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[CAP theorem]_Influence of CAP Theorem on Big Data analytics.pdf', 'total_pages': 3, 'page': 2, 'page_label': '3'}, page_content='distributed database system design. IEEE Computer \\nMagazine, 45(2):37 – 42, February 2012. \\ndoi:10.1109/MC.2012.33. \\n[3] Dobre D, Viotti P, Vukolic M (2014) Hybris: Robust \\nhybrid cloud storage. In: ACM Symposium on Cloud ´ \\nComputing (SoCC), Seattle, WA, USA, pp 12:1 –12:14, \\nDOI 10.1145/2670979.2670991. \\n[4] Fekete A, Gupta D, Luchangco V, Lynch NA, \\nShvartsman AA (1996) Eventually-serializable data \\nservices. In: 15th ACM Symposium on Principles of \\nDistributed Computing (PODC), Philadelphia, PA, USA, \\npp 300–309. \\n[5] Francesc D. Munoz-Esco, Ruben de Juan-Martin, J. R. \\nGonzalez de, , Jose M. Bernabeu, CAP Theorm: \\nRevision of its Related Consistency Models, Technical \\nReport TR-IUMTI-SIDI-2017/002, Universitat \\nPolitecnica de Valencia, 46022 Valencia (Spain). \\n[6] Martin Kleppmann, A critique of the CAP Theorem, \\narticle published in researchgate on Sept 2015.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='Reﬂection on MongoDB Database Logical and\\nPhysical Modeling\\n1st Pieter Willem Jordaan\\nSchool for Electrical, Electronic and Computer Engineering\\nNorth-West University\\nPotchefstroom, South Africa\\npieterwjordaanpc@gmail.com\\n2nd Johann Erich Wolfgang Holm\\nSchool for Electrical, Electronic and Computer Engineering\\nNorth-West University\\nPotchefstroom, South Africa\\njohann.holm@nwu.ac.za\\nAbstract—Traditional relational database design uses concep-\\ntual, logical, and physical modeling based on Peter Chen’s\\nmethods. UML (Uniﬁed Modeling Language), though being a\\nset of software development tools, is often used to conceptually\\nmodel data and relationships. This paper presents a methodical\\napproach to logically and physically model data in MongoDB\\nby utilizing UML conceptual modeling aids. Application of data\\nmodels greatly impacts performance, scalability and ﬂexibility of\\ndata systems. Furthermore, application life-cycle and expansion\\nimpact long-term decisions made during modeling. This paper'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='models greatly impacts performance, scalability and ﬂexibility of\\ndata systems. Furthermore, application life-cycle and expansion\\nimpact long-term decisions made during modeling. This paper\\ntakes into consideration application requirements, access patterns\\nand life-cycle during logical and physical modeling in a cloud\\nenvironment. The ﬂexibility that a NoSQL modeling paradigm\\nembodies makes logical and physical modeling complex, with\\nno hard-and-fast rules. This paper presents logical and physical\\nmodeling concepts required during designing database applica-\\ntions with MongoDB.\\nIndex Terms—database, modeling, UML, MongoDB, cloud\\nI. I NTRODUCTION\\nEntity-relationship modeling methods and tools developed\\nby Peter Chen [1] have become the de facto standard for\\nmodeling and designing databases. Though designed for and\\nused by relational databases, it is being applied to NoSQL\\n(Not Only SQL) databases to similar effect [2].\\nNoSQL databases are, however, very different to traditional'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='used by relational databases, it is being applied to NoSQL\\n(Not Only SQL) databases to similar effect [2].\\nNoSQL databases are, however, very different to traditional\\ndatabases when it comes to modeling, since scalability, con-\\nsistency and performance greatly depend on the underlying\\ndesign, especially considering their ﬂexible schema options\\n[3].\\nCloud-based development deﬁnes application criteria re-\\nquirements for developers. Additional emphasis is placed on\\nthe following requirements speciﬁc to cloud systems [4]:\\n• Ease of use;\\n• Scalability;\\n• Availability;\\n• Security.\\nScalability and availability cannot always be predicted due to\\nchanging environments and requirements - these factors should\\nbe taken into account when designing a database to allow for\\nﬂexibility, which ﬁt well into NoSQL database systems traits\\n[5].\\nMongoDB is a NoSQL database capable of running in the\\ncloud. It provides high availability, horizontal scalability and'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='ﬂexibility, which ﬁt well into NoSQL database systems traits\\n[5].\\nMongoDB is a NoSQL database capable of running in the\\ncloud. It provides high availability, horizontal scalability and\\na ﬂexible data structure [6]. MongoDB has full relational\\ncapability, which suits traditional entity-relational modeling\\ntechniques [7]. Recently MongoDB gained ACID (Atomicity\\nConsistency Isolation Durability) support, which allows for\\ntransactions, much like with relational databases [8], [9].\\nFlexible schemas with no hard-and-fast rules such as the\\ntraditional 3NF (third normal form) [10] for designing con-\\nceptual and physical models, make NoSQL database modeling\\ncomplex.\\nThis paper provides a broad, but detailed, overview of\\ndatabase modeling concepts that have a bearing on design\\nchoices for NoSQL databases. We provide speciﬁc guidelines\\non how to employ MongoDB’s unique feature set. As a case\\nin point, we conceptually, logically and physically model a'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='choices for NoSQL databases. We provide speciﬁc guidelines\\non how to employ MongoDB’s unique feature set. As a case\\nin point, we conceptually, logically and physically model a\\nsimplistic use-case, based on Chen’s methods. A discussion\\non the complexity of NoSQL modeling is provided, followed\\nby a conclusion with further research suggestions.\\nII. R ELATED WORK AND CONTRIBUTIONS\\nRelated studies have contributed to NoSQL modeling re-\\nsearch. Shin et al. [2] has provided a framework for mod-\\neling NoSQL databases generically from a conceptual UML\\nmodel based on Chen’s framework. However, the study does\\nnot discuss the physical modeling phase, and has a generic\\ndocument-based logical model.\\nAn algorithmic approach was developed by Abdelhedi et\\nal. [11] to transform conceptual UML class models into\\nphysical models for various NoSQL databases. The logical\\nmodel is used as an intermediate abstraction layer for a subset\\nof features in three NoSQL database types: column-based,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}, page_content='physical models for various NoSQL databases. The logical\\nmodel is used as an intermediate abstraction layer for a subset\\nof features in three NoSQL database types: column-based,\\ndocument-based and graph-based.\\nNoAM (NoSQL Abstract Data Model) presented by Atzeni\\net al. [12] offers a database-independent data model for\\nNoSQL databases, also making use of a UML-based concep-\\ntual data model.\\nDe Lima et al. [3] has published an algorithmic approach to\\nNoSQL logical modeling. Workﬂow measurements are to be\\nspeciﬁed along with the conceptual model when developing\\nlogical models. Physical modeling and its effects are not\\ndemonstrated.\\nThe previously mentioned research publications does not\\nconsider application life-cycle with regards to growth, scala-\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='bility and ﬂexibility. Furthermore, our study does not attempt\\nto generically model multiple NoSQL databases, but rather\\nreﬂects on factors supporting and leveraging MongoDB and\\nits speciﬁc set of features.\\nIII. D ATABASE MODELING\\nA. Uniﬁed Modeling Language\\nUML, originally developed as a set of tools for software\\nengineering, can be used for data modeling. Class diagrams\\nare a superset of ER (Entity-Relationship) diagrams used in\\nChen’s methods [13].\\nTools that this paper will use for modeling include use-case\\ndiagrams and class diagrams. Use-case diagrams, as in Fig.1,\\nare used to model actors performing activities on systems [14].\\nThis diagram is used to identify the data elements used in\\nconceptual modeling.\\nClass diagrams represent data elements, attributes, methods,\\nrelationships, inheritance and cardinalities as shown in Fig.2.\\nUML stereotypes can be used to add additional information\\nduring logical and physical modeling, such as embedding,'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='relationships, inheritance and cardinalities as shown in Fig.2.\\nUML stereotypes can be used to add additional information\\nduring logical and physical modeling, such as embedding,\\nreferences and other MongoDB speciﬁc features [2].\\nComposition and aggregation annotations in class diagrams\\nfurther give an indication of the nature of relationships, leading\\nto either referencing (entities are standalone) or embedding\\n(entities are not independent) [14].\\nUML was found to provide better support and be more\\ncomprehensive than ER (entity-relationship) diagrams at data\\nmodeling by De Lucia et al. [15]. Since UML could be used\\nto model software classes, documentation could be shared\\nbetween application and data models.\\nB. Conceptual Modeling\\nThe ER design approach starts with a need or problem fol-\\nlowed by a requirements analysis [16]. From the requirements\\nanalysis actors, interactions or activities and some attributes\\nshould be known [10]. From this a use case diagram can be\\ndrafted [17].'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='lowed by a requirements analysis [16]. From the requirements\\nanalysis actors, interactions or activities and some attributes\\nshould be known [10]. From this a use case diagram can be\\ndrafted [17].\\nWhen requirements are satisfactorily determined, the con-\\nceptual model can be designed. This model still does not\\ncontain database speciﬁc annotations, and could be equivalent\\nto a relational database conceptual design. Conceptual designs\\ncontain high-level data entities, attributes, relationships and\\ncardinalities [10], [18]. Even during this phase the conceptual\\ndesign should have foresight and consideration of the full life-\\ncycle, and proper requirements elicitation will signiﬁcantly\\ncontribute to this cause [19].\\nThe following relationship cardinalities are possible [10]:\\n• One-to-One;\\n• One-to-Many;\\n• Many-to-Many.\\nCardinalities could also be speciﬁc, for example, a department\\nmay have between ﬁve and ﬁfteen employees [18].\\nC. MongoDB Logical Modeling'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='• One-to-One;\\n• One-to-Many;\\n• Many-to-Many.\\nCardinalities could also be speciﬁc, for example, a department\\nmay have between ﬁve and ﬁfteen employees [18].\\nC. MongoDB Logical Modeling\\nMongoDB is a document-oriented database and data objects\\nhave the familiar JSON (JavaScript Object Notation) format,\\nthough stored in a more efﬁcient binary format. Documents\\nare contained within collections which in turn belong to a\\ndatabase [20].\\nUnlike typical relational databases, MongoDB’s data struc-\\nture is not structured as rows and columns, but can have\\nhierarchy - arrays and sub-documents - and often closely\\nresembles data structures used in an application. There are\\ntherefore many ways to correctly model data and relationships,\\nbut performance characteristics could vary [20].\\nTraditionally, during logical modeling, the entity-relational\\nconceptual model is decomposed into 3NF as tables and rela-\\ntionships [1], [10]. Since MongoDB does not typically follow'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='Traditionally, during logical modeling, the entity-relational\\nconceptual model is decomposed into 3NF as tables and rela-\\ntionships [1], [10]. Since MongoDB does not typically follow\\na normalization design paradigm, choices about relationships\\nboil down to deciding on:\\n• Referencing (normalization);\\n• Embedding (denormalization).\\nEven within these two options many variations exist [21].\\nSome of these options are discussed in the following sections,\\nwith some advanced patterns to consider during physical\\nmodeling.\\n1) Referencing: Referencing is the MongoDB equivalent\\nto primary/foreign key relationships in relational databases [9].\\nA collection can be referenced by including a ﬁeld referencing\\na unique attribute of a document in another collection. A\\nspecial case of this is referencing another document in the\\nsame collection, called self-joining [20]. MongoDB does not\\nhowever automatically ensure referential integrity and relies\\non application level functionality to achieve that [22].'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='same collection, called self-joining [20]. MongoDB does not\\nhowever automatically ensure referential integrity and relies\\non application level functionality to achieve that [22].\\nOne factor that is always a certain scenario for referencing\\nis whether the relationship has a high arity. In other words,\\na distinction has to be made between many and few [22]. In\\nthe case of few, embedding could be considered, otherwise\\na single document would grow large (maximum of 16 MiB),\\nrequiring more RAM and cause slower data transfers [20].\\nHigh-arity many-to-many relationships should typically be\\nmodeled with a referenced join-collection, much like in a\\nrelational table. Again, if either side was few, embedding could\\nbe considered [20].\\nAdvantages of referencing include [20]:\\n• No redundancy;\\n• Immediate consistency;\\n• Supports high-arity relationships;\\n• Flexibility in queries and indexing (physical model);\\n• Expanded sharding options.\\nDisadvantages of referencing are [20]:'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2'}, page_content='• Immediate consistency;\\n• Supports high-arity relationships;\\n• Flexibility in queries and indexing (physical model);\\n• Expanded sharding options.\\nDisadvantages of referencing are [20]:\\n• Application level joins;\\n• Depending on query patterns, limited scalability;\\n2) Embedding: Embedding, here, refers to the process of\\nadding a sub-document or ﬁelds of a conceptual entity wholly\\nand directly into a related document or documents, instead of\\ncreating a separate collection and referencing only the primary\\nidentiﬁer of related documents [9].\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='Embedding has the following advantages [20]:\\n• Application side joins aren’t needed;\\n• Inherent atomic and isolated operations for one-to-few\\nrelationships exist;\\n• Data locality is utilized;\\n• Inheritance is efﬁciently modeled;\\n• Faster query performance is available.\\nDisadvantages of embedding data include [20]:\\n• Redundant copies are present;\\n• Eventual consistency is achieved;\\n• Application level cascading is required for consistency;\\n• Poor performance is offered for high-arity relationships;\\n• Less ﬂexibility is available as collections are pre-joined.\\n3) Summary: Table I summarizes UML class diagram\\nconcepts relating to MongoDB’s document model.\\nTABLE I\\nUML CLASSES TO MONGO DB ANALOGY\\nUML Concept MongoDB Concept\\nClass Collection\\nObject Document\\nAttribute Field\\nAssociation Embed or reference\\nAggregation Usually reference\\nComposition Usually embed\\nInheritance Usually embed\\nD. MongoDB Physical Modeling\\nDuring physical modeling, the database designer consid-'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3'}, page_content='Association Embed or reference\\nAggregation Usually reference\\nComposition Usually embed\\nInheritance Usually embed\\nD. MongoDB Physical Modeling\\nDuring physical modeling, the database designer consid-\\ners access-path-independent and access-path-dependent design\\nchoices. This includes application access patterns, indexing\\nschemes and sharding keys [1], [2], [11]. MongoDB physical\\nmodeling consists of many factors to consider [9]:\\n• _id choice;\\n• Schema validation;\\n• Indexing;\\n• Sharding;\\n• Replication;\\n• Consistency level choices;\\n• Application access patterns;\\n• MongoDB advanced features.\\nMany of these factors will also have a bearing on the underly-\\ning logical modeling choices, especially indexing and sharding\\nkey choices [9].\\nAll documents in MongoDB must have an _id ﬁeld that\\nis unique. In fact all collections include a unique index on\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4'}, page_content='Content Missing\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5'}, page_content='Content Missing\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='TABLE II\\nCONCEPTUAL DESIGN OPTIONS\\nConcept Referencing/Embedding Design Pros Cons\\nLibrary\\nReference (collection) Shardable Joins may be required\\nEmbed within Book Data locality Eventual consistency; difﬁcult to query\\nPartially embed within Book Shardable; data locality Eventual consistency\\nBook\\nReference (collection) Shardable Joins required\\nEmbed within Library Data locality; single read operations Boundless; complex queries;\\nPartially embed within Library Additional feature options Additional complexity\\nBook\\nCopies\\nReference (collection) Shardable Disjoint from Books\\nEmbed within Library Data locality Boundless\\nEmbed within Book Data locality; single view on checkout status Larger working set\\nCheckout\\nReference (collection) Shardable Transactions required\\nEmbed within Reader Single view; quick access Transactions required; possibly boundless\\nEmbed within Reader with bucket pattern Single view; quick access; not bounded Transactions required; outlier pattern'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='Embed within Reader with bucket pattern Single view; quick access; not bounded Transactions required; outlier pattern\\nEmbed within Book or Book Copies No transactions No history\\nPartially embed within Book Copies No transactions History; eventual consistency\\nReview\\nReference (collection) Shardable; single full-text index Joins may be required\\nEmbed within Checkout Single review per checkout Complex queries to group by book/reader\\nEmbed within Book Data locality Boundless\\nRatings\\nReference (collection) Shardable; single full-text index Joins may be required\\nEmbed within Checkout Single rating per checkout Complex queries to group by book/reader\\nEmbed within Book as counts Data locality; pre-calculation options Calculation required per read\\n<<collection>>\\nReader\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tname:\\tstring\\n+\\tcheckouts:\\t[<<embedded>>]\\n*\\n<<collection>>\\nBook\\n+\\tisbn:\\tstring\\t<<PK>>\\n+\\ttitle:\\tstring\\n+\\tauthor:\\t[string]\\n+\\tdescription:\\tstring\\n+\\tlibbooks:\\t[<<embedded>>]\\n+\\tratingtotal:\\tnumber'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='+\\tname:\\tstring\\n+\\tcheckouts:\\t[<<embedded>>]\\n*\\n<<collection>>\\nBook\\n+\\tisbn:\\tstring\\t<<PK>>\\n+\\ttitle:\\tstring\\n+\\tauthor:\\t[string]\\n+\\tdescription:\\tstring\\n+\\tlibbooks:\\t[<<embedded>>]\\n+\\tratingtotal:\\tnumber\\n+\\tratingcount:\\tnumber\\n<<collection>>\\nReview\\n+\\tid:\\tstring\\t<<PK>>\\n+\\treader:\\t<<FK>>\\n+\\tlibbook:\\t<<partial\\tembed>>\\n+\\treview:\\tstring\\n+\\treviewdate:\\tdate\\n<<embedded>>\\nCheckout\\n+\\tlibbook:\\t<<partial\\tembed>>\\n+\\tcheckout:\\tdate\\n+\\tcheckin:\\tdate\\t<<nullable>>\\n*\\n* 1\\n<<collection>>\\nLibrary\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tname:\\tstring\\n+\\taddress:\\tstring\\n<<embedded>>\\nLibraryBook\\n+\\tid:\\tstring\\t<<PK>>\\n+\\tbook:\\t<<FK>>\\n+\\tlibrary:\\t<<embedded>>\\n+\\tcheckout:\\tdate\\t<<nullable>>\\n1\\n*\\n1\\n1\\n1\\n1\\n*\\n1\\nFig. 4. Library logical model\\nﬁlter with the same constraints. The resulting list of documents\\nis then grouped and summed by reader. The result is sorted\\nand limited to 10 - the top ten readers of the month.\\nCounting the number of books rented requires an index on\\nthe embedded checkouts’ checkout date. Using the aggregation'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='and limited to 10 - the top ten readers of the month.\\nCounting the number of books rented requires an index on\\nthe embedded checkouts’ checkout date. Using the aggregation\\npipeline to match only readers that have checkouts in that\\nmonth. Unwinding and grouping by library, while summing\\neach checkout will deliver the number of rented books per\\nlibrary.\\nAverage ratings can be calculated on demand by taking\\nthe total summed ratings, divided by the count - the ﬁelds\\nwere already included in the logical model for Book. In order\\nto optimize this, an additional ﬁeld should be added to pre-\\ncalculate the monthly ratings. This can be scheduled daily for\\nexample.\\nConsidering the writes to the database, only checkouts are\\ncomplicated, since a library may have more than one copy of\\na book and there are more than one library. The LibraryBook\\nembedded array allows for a single book document to keep\\ntrack of each copy along with an optional checkout date ﬁeld.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='a book and there are more than one library. The LibraryBook\\nembedded array allows for a single book document to keep\\ntrack of each copy along with an optional checkout date ﬁeld.\\nIf this is set to null the book is available. During checkout\\nthis is set to the checkout date and when checked in it gets set\\nback to null. Similarly in order to keep track of each reader’s\\ncheckouts and check-ins, a transaction is done to modify the\\nLibraryBook element and modify the reader checkout array.\\nFor this project strong consistency can be employed for all\\ntransactions since the few embedded ﬁelds, such as library\\naddress and book ISBN rarely change.\\nIt is unlikely that sharding would be required for such a\\nproject, since reads and writes occur rarely and in person.\\nIf needed, though, Book and and Reader can be sharded on\\nprimary key.\\nIf it had been required that data per library be isolated a\\nmore normalized approach would have had to be followed'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6'}, page_content='If needed, though, Book and and Reader can be sharded on\\nprimary key.\\nIf it had been required that data per library be isolated a\\nmore normalized approach would have had to be followed\\nin order to make the shard key per library. This would\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='render checkouts, and library books as their own collection.\\nShard tags could then be employed to isolate speciﬁc library\\ninformation to speciﬁc database servers.\\nIn order to keep working sets small it could become\\nnecessary to move checkouts to a separate collection, which\\nwould require embedding reader information in the checkouts\\nand cascading updates to user details in order for aggregations\\nto group by reader.\\nReading from replicas could also improve read scalability,\\nwithout compromising on consistency for critical operations.\\nA. Discussion\\nAll design options given in Table II, though not inclusive\\nof every permutation, can validly be modeled in logical an\\nphysical form while still meeting requirements. However, each\\noption has an direct impact on the complexity and performance\\nof the application and database design. There is no golden\\nrule, such as the 3NF in relational databases, to aid NoSQL\\ndatabase designers.\\nSince many design choices rely on application logic, it can'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='of the application and database design. There is no golden\\nrule, such as the 3NF in relational databases, to aid NoSQL\\ndatabase designers.\\nSince many design choices rely on application logic, it can\\nbe said that the application and database have to be designed\\ntogether.\\nConsider, for example, that a reader may only rate a book\\nonce. A requirement such as that dramatically impacts design\\nchoices for the application and database. The current rating\\nsystem would not be able to support such a requirement.\\nThe complexity of the above use-case could be exponen-\\ntially increased with the addition of a new concept such as\\naccount management. The use-case is not intended to serve\\nas a complete and all-encompassing solution, but instead offer\\nan overview of the complexities involved with NoSQL data\\nmodeling due to its inherent ﬂexibility.\\nFurthermore, speciﬁc deployment requirements have not\\nbeen speciﬁed and also have a bearing on underlying design'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='modeling due to its inherent ﬂexibility.\\nFurthermore, speciﬁc deployment requirements have not\\nbeen speciﬁed and also have a bearing on underlying design\\nchoices, adding an additional layer of complexity.\\nV. C ONCLUSION\\nFrom the example use case it can be seen that MongoDB\\nmakes it possible to implement use cases in a variety of ways.\\nHow to design a database depends on use case requirements,\\nwhich must be properly elicited in order to avoid large changes\\nlater on. The processes and factors weighing in on the design\\nchoices have been discussed in the previous sections and\\nprovide a guide to concepts and tradeoffs required during the\\ndesign phase.\\nThe complexity of designs are affected by the complexity\\nof requirements. Complex designs will inevitably cost more to\\ndevelop and maintain, though complexity handled during the\\ndesign phases may result in simpler and more robust solutions\\nin the future.\\nForm-follows-function [24] - this common term has the'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='develop and maintain, though complexity handled during the\\ndesign phases may result in simpler and more robust solutions\\nin the future.\\nForm-follows-function [24] - this common term has the\\nimplication for database and application design, that function\\ndictates underlying form, structure and design choices. It is the\\nconjecture of the authors that the traditional entity-relational\\napproaches suffer from lack of foresight and understanding of\\nthe whole system and its functions and resource requirements.\\nIt is instead based on functions-follow-form where entities\\nwhich have data-value are modeled very directly as acted upon\\nby users. Rarely, application access patterns and underlying\\nresources are discussed and weighed in on design choices -\\nespecially when considering full life-cycle requirements.\\nMore speciﬁcally, future growth is often not accounted for\\nas it is usually addressed in the physical model as a means of\\noptimizing queries as dictated by the structure of the logical'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='More speciﬁcally, future growth is often not accounted for\\nas it is usually addressed in the physical model as a means of\\noptimizing queries as dictated by the structure of the logical\\ndesign. As shown in the use-case example the physical model\\ndesign is greatly impacted by and reliant on the prior designed\\nlogical model. Furthermore, changes required in the physical\\nmodel requires changes on the underlying logical model - they\\nare clearly interdependent.\\nThis paper therefore submits that physical and logical\\nmodeling phases in NoSQL, and more speciﬁcally MongoDB,\\nshould coincide as a single detail design phase. This is due to\\nthe fact that choices on either side are dependent on the other.\\nWhen considered as a unit, it may lead to shorter design phases\\nand less back-to-the-drawing-board situations.\\nIn large projects, the requirement for an interactive design\\neffort (physical and logical) implies good communication and\\ncoordination between design teams, should such teams be\\nused.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='In large projects, the requirement for an interactive design\\neffort (physical and logical) implies good communication and\\ncoordination between design teams, should such teams be\\nused.\\nFurthermore, requirements elicitation should provide insight\\ninto potential growth and outlier query patterns and require-\\nments, which often impact the general case. The requirements\\ndeﬁne the functions and constraints, which in turn dictates the\\nform that the application and database (the resources) should\\ntake on. It is an outside-in approach taking into consideration\\nthe application design and user access patterns chieﬂy.\\nThis is in contrast with applications of Chen’s methods,\\nwhich start off with data-yielding entities, leading to concepts,\\nlogical forms and ultimately a physical database. It is typically\\nonly after this where application access patterns are typically\\nﬁtted to pre-existing underlying data structures. This is an\\ninside-out data-oriented approach.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='only after this where application access patterns are typically\\nﬁtted to pre-existing underlying data structures. This is an\\ninside-out data-oriented approach.\\nEven though the example use case was imagined and\\nartiﬁcial, it has conveyed the type of design choices and\\nmethods that should be followed for MongoDB applications.\\nFurther research is needed to formally design a database\\nand application framework for modeling. It should address\\nthe following problems and considerations:\\n• User activity workﬂows;\\n• Impact of requirements;\\n• Maintenance workﬂow impacts;\\n• Design and maintenance costs;\\n• Deployment impacts.\\nIn general, NoSQL modeling techniques are under-\\nresearched and due to their ﬂexible nature, may bring forth\\nnew design approaches previously impossible or too complex\\nto attempt with only rows and columns.\\nFinally, with cloud-based applications usually being the\\ntarget for NoSQL development, the impacts of its constraints'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7'}, page_content='to attempt with only rows and columns.\\nFinally, with cloud-based applications usually being the\\ntarget for NoSQL development, the impacts of its constraints\\nand features on design choices should be further investigated.\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='VI. A UTHORS AND AFFILIATIONS\\nPieter Jordaan (MEng) is currently pursuing his PhD in\\nEngineering at the North-West University in South Africa.\\nHe is a student member of IEEE. His research ﬁeld includes\\ntopics on cloud applications, NoSQL database development\\nand scalability.\\nJohann Holm (PhD, PrEng) is an associate professor\\nat the School for Electrical, Electronic and Computer\\nEngineering of the North-West University in South Africa.\\nHe is a member of IEEE and a member of INCOSE, and\\nis actively involved in research and development, as well as\\nconsulting in operations and product development.\\nREFERENCES\\n[1] P. P.-S. Chen, “The entity-relationship model—toward a uniﬁed view of\\ndata,” ACM Transactions on Database Systems, vol. 1, no. 1, pp. 9–36,\\n3 1976.\\n[2] K. Shin, C. Hwang, and H. Jung, “NoSQL Database Design Using\\nUML Conceptual Data Model Based on Peter Chens Framework,”\\nInternational Journal of Applied Engineering Research, vol. 12, no. 5,\\npp. 632–636, 2017.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='UML Conceptual Data Model Based on Peter Chens Framework,”\\nInternational Journal of Applied Engineering Research, vol. 12, no. 5,\\npp. 632–636, 2017.\\n[3] C. de Lima and R. dos Santos Mello, “A workload-driven logical design\\napproach for NoSQL document databases,” in Proceedings of the 17th\\nInternational Conference on Information Integration and Web-based\\nApplications &Services - iiWAS ’15. New York, New York, USA:\\nACM Press, 2015, pp. 1–10.\\n[4] P. W. Jordaan and J. E. W. Holm, “Implementation and veriﬁcation of\\na cloud-based machine-to-machine data management system,” in 2013\\nAfricon. IEEE, 9 2013, pp. 1–5.\\n[5] K. Chodorow, Scaling MongoDB, 1st ed. Sebastopol: O’Reilly, 2011.\\n[6] A. Kanade, A. Gopal, and S. Kanade, “A study of normalization and\\nembedding in MongoDB,” Souvenir of the 2014 IEEE International\\nAdvance Computing Conference, IACC 2014, pp. 416–421, 2014.\\n[7] G. Zhao, W. Huang, S. Liang, and Y . Tang, “Modeling MongoDB'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='embedding in MongoDB,” Souvenir of the 2014 IEEE International\\nAdvance Computing Conference, IACC 2014, pp. 416–421, 2014.\\n[7] G. Zhao, W. Huang, S. Liang, and Y . Tang, “Modeling MongoDB\\nwith relational model,” Proceedings - 4th International Conference on\\nEmerging Intelligent Data and Web Technologies, EIDWT 2013, 2013.\\n[8] E. Horowitz, “MongoDB Drops ACID,” 2018. [Online]. Avail-\\nable: https://www.mongodb.com/blog/post/multi-document-transactions-\\nin-mongodb\\n[9] MongoDB, “The MongoDB 4.0 Manual,” 2019. [Online]. Available:\\nhttps://docs.mongodb.com/manual/\\n[10] C. Coronel, S. Morris, and P. Rob, Database Systems: Design, Im-\\nplementation, and Management, 11th ed. Stamford, CT: CENGAGE\\nLearning, 2009.\\n[11] F. Abdelhedi, A. A. Brahim, F. Atigui, and G. Zurﬂuh, “UMLtoNoSQL:\\nAutomatic transformation of conceptual schema to NoSQL databases,”\\nProceedings of IEEE/ACS International Conference on Computer Sys-\\ntems and Applications, AICCSA, vol. 2017-Octob, no. 1, pp. 272–279,\\n2018.'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='Proceedings of IEEE/ACS International Conference on Computer Sys-\\ntems and Applications, AICCSA, vol. 2017-Octob, no. 1, pp. 272–279,\\n2018.\\n[12] P. Atzeni, F. Bugiotti, L. Cabibbo, and R. Torlone, “Data modeling in\\nthe NoSQL world,” Computer Standards & Interfaces, no. October, pp.\\n0–1, 10 2016.\\n[13] M. Yusufu, H. J. Zhang, G. Yusufu, Z. D. Liu, P. Cheng, and D. Dilisati,\\n“Modeling and Analysis of Complex System with UML: A Case Study,”\\nApplied Mechanics and Materials, vol. 513-517, pp. 1346–1351, 2014.\\n[14] Object Management Group, “OMG Uniﬁed Modeling Language\\nTM (OMG UML),” p. 794, 2015. [Online]. Available:\\nhttp://www.omg.org/spec/UML/2.5/\\n[15] A. De Lucia, C. Gravino, R. Oliveto, and G. Tortora, “An experimental\\ncomparison of ER and UML class diagrams for data modelling,”\\nEmpirical Software Engineering, vol. 15, no. 5, pp. 455–492, 2010.\\n[16] C. Fahrner and G. V ossen, “A survey of database design transforma-\\ntions based on the Entity-Relationship model,” Data and Knowledge'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='[16] C. Fahrner and G. V ossen, “A survey of database design transforma-\\ntions based on the Entity-Relationship model,” Data and Knowledge\\nEngineering, vol. 15, no. 3, pp. 213–250, 1995.\\n[17] C. Churcher, Beginning database design. New York: Apress, 2012.\\n[18] P. Ponniah, Database design and development: an essential guide for\\nIT professionals. John Wiley & Sons, Inc., 2003.\\n[19] H. Sammaneh, “Requirements Elicitation with the Existence of Similar\\nApplications: A Conceptual Framework,” 2018 International Conference\\non Computer and Applications, ICCA 2018, pp. 444–449, 2018.\\n[20] R. Copeland, MongoDB Applied Design Patterns, 1st ed. Sebastopol:\\nO’Reilly, 2013.\\n[21] D. Coupal and K. W. Alger, “Building with Patterns: A Summary,” 2019.\\n[Online]. Available: https://www.mongodb.com/blog/post/building-with-\\npatterns-a-summary\\n[22] K. Chodorow and M. Dirolf, MongoDB: the deﬁnitive guide, 2nd ed.\\nSebastopol: O’Reilly, 2013.\\n[23] D. Coupal and K. W. Alger, “Building With Patterns: The Outlier'),\n",
       " Document(metadata={'producer': '3-Heights(TM) PDF Analysis & Repair Shell 4.12.26.3 (http://www.pdf-tools.com); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': 'TeX', 'creationdate': '2019-08-12T09:39:07+00:00', 'meeting starting date': '25 Sept. 2019', 'moddate': '2020-07-26T07:07:11-04:00', 'ieee article id': '9133972', 'trapped': 'False', 'ieee issue id': '9133731', 'subject': '2019 IEEE AFRICON;2019; ; ;', 'ieee publication id': '9123328', 'title': 'Reflection on MongoDB Database Logical and Physical Modeling', 'meeting ending date': '27 Sept. 2019', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016/Debian) kpathsea version 6.2.2', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling]_Reflection_on_MongoDB_Database_Logical_and_Physical_Modeling.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8'}, page_content='patterns-a-summary\\n[22] K. Chodorow and M. Dirolf, MongoDB: the deﬁnitive guide, 2nd ed.\\nSebastopol: O’Reilly, 2013.\\n[23] D. Coupal and K. W. Alger, “Building With Patterns: The Outlier\\nPattern,” 2019.\\n[24] M. H. Wen and V . O. Li, “Form Follows Function: Designing Smart Grid\\nCommunication Systems Using a Framework Approach,” IEEE Power\\nand Energy Magazine, vol. 12, no. 3, pp. 37–43, 2014.\\nAuthorized licensed use limited to: UNIVERSITY OF WESTERN ONTARIO. Downloaded on July 26,2020 at 11:07:11 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Abstract – Amazon DynamoDB is a next-gen NoSQL (Not \\nonly SQL), key-value and document database that delivers \\nsingle-digit millisecond performance at any scale. As more \\ncomplex web-based applications adopt DynamoDB, it is \\nimperative to develop sound design strategies for DynamoDB \\nschemas. Large-scale web-based applications often exhibit \\nconflicting requirements. Hence, the research problem is to \\ndesign the DynamoDB schemas of a given application with \\nconflicting requirements such that the database must satisfy \\ncertain predetermined performance goals. In the era of \\nrelational databases, normal forms were developed to guide \\nthe schema design process. Past knowledge and experiences, \\nwe believe, are also applicable to DynamoDB schema design. \\nSpecifically, based upon Nested Normal Form and XML \\ndatabases, this paper demonstrates the feasibility of a design \\nstrategy on DynamoDB schemas, particularly with the access \\npatterns of the data in mind. Simulation further substantiates'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='databases, this paper demonstrates the feasibility of a design \\nstrategy on DynamoDB schemas, particularly with the access \\npatterns of the data in mind. Simulation further substantiates \\nthe feasibility of our approach. \\n \\nKeywords – Amazon DynamoDB Schema Design, Nested \\nNormal Form, XML Databases \\n \\nI.  INTRODUCTION \\n \\n As large-scale web-based applications demand speed, \\nflexibility and scalability, NoSQL database systems, a \\nnext-gen database systems, are being developed. Many \\ncommercial NoSQL database systems are now available. \\nNotable examples include MongoDB [10], Cassandra [5], \\nRedis [12] and Amazon DynamoDB [1] and others. The \\nunderlying data models of NoSQL databases are quite \\ndifferent from the tabular data model of the traditional \\nrelational databases. Some prevalent data models for \\nNoSQL databases are the document model, the graph \\nmodel and the key-value model [11]. Amazon DynamoDB \\nadopts the key-value model and the document model, and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='NoSQL databases are the document model, the graph \\nmodel and the key-value model [11]. Amazon DynamoDB \\nadopts the key-value model and the document model, and \\nmany top-level entities of a database are modeled as nodes \\nand their relationships as edges as in graphs [1]. \\n The results of system analysis, an indispensable \\nactivity of any large-scale system development, include \\nmany different types of diagrams focusing on various \\naspects of the system under development. UML diagrams \\n[4], promoted by robust modeling tools like Visual \\nParadigm [13] and others, have become a software industry \\nstandard. The proposed DynamoDB schema design \\nstrategy also begins with a graphical diagram, called a \\nconceptual-model hypergraph, which is in a way similar to \\nbut simpler than UML domain model class diagrams.  The \\npurpose of such a hypergraph, like a UML domain model \\n`class diagram, is to capture the static, rather than the \\ndynamic, aspects of a system of interest.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='purpose of such a hypergraph, like a UML domain model \\n`class diagram, is to capture the static, rather than the \\ndynamic, aspects of a system of interest. \\n Nested Normal Form [9] was originally designed for \\nnested relational databases, which allow tables to be nested \\nwithin other tables, resulting in embedded tables and thus \\nhierarchical structures. Since XML databases [3] also \\nallow hierarchical structures, it is natural to extend Nested \\nNormal Form to XML databases. A critical contribution of \\nNested Normal Form is that XML databases that conform \\nto Nested Normal Form are guaranteed to be free of \\nunwanted redundant data. In [8], we devised algorithms \\nthat generate Nested Normal Form XML databases. We \\nshall shortly show that the algorithms in [8], with some \\nmodifications, are also applicable to DynamoDB as well. \\n We believe conceptual-model hypergraphs and the \\nXML database design algorithms form the conceptual \\nframework in which the solution for the research problem'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='We believe conceptual-model hypergraphs and the \\nXML database design algorithms form the conceptual \\nframework in which the solution for the research problem \\nstated in the abstract can be formulated. In contrast, in one \\nsingle page the documentation of Amazon DynamoDB \\nstates some NoSQL design principles for DynamoDB \\nschemas. Although the suggested design principles are \\nsound, they are so vague to formulate a formal DynamoDB \\ndesign strategy. \\n To convey the central idea of the proposed design \\nstrategy, the rigorous mathematics underlying [7,8,9] is \\navoided. Instead, this paper adopts a high-level, step-by-\\nstep presentation. To proceed, Section 2 concisely presents \\nthe methodology and shows that such an approach is \\nfeasible and desirable. Section 3 presents the results of our \\nexperiments, which were conducted on the AWS (Amazon \\nWeb Services) Educate platform  [2]. Section 4 discusses \\nmany-to-many relationships and we shall conclude and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='experiments, which were conducted on the AWS (Amazon \\nWeb Services) Educate platform  [2]. Section 4 discusses \\nmany-to-many relationships and we shall conclude and \\nstate the future research goals of this project in Section 5. \\n \\nII.  METHODOLOGY \\n \\n In a nutshell, the proposed design strategy is \\nsummarized as follows: \\n• Capture an application of interest with a \\nconceptual-model hypergraph [8], or some other \\nmeans that can be translated to a conceptual-\\nmodel hypergraph. \\n• Identify the access patterns of the most commonly \\nexecuted queries and operations in the \\nconceptual-model hypergraph.  \\n• Simplify the conceptual-model hypergraph by \\nidentifying object sets that are in one-to-one \\ncorrespondence with each other. \\n• Generate a set of hierarchical schemas from the \\naccess patterns based on Nested Normal Form [9]. \\nA Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal \\nForm Approach \\n \\nWai Yin Mok'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='access patterns based on Nested Normal Form [9]. \\nA Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal \\nForm Approach \\n \\nWai Yin Mok \\nDepartment of Information Systems, The University of Alabama in Huntsville, Huntsville, USA \\n (mokw@uah.edu) \\n903978-1-5386-7220-4/20/$31.00 ©2020 IEEE\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='• Load the data of the application to Amazon \\nDynamoDB in accordance to the generated \\nhierarchical schemas. \\n \\nCrnSecID\\nSection Pair\\nStudent SSN\\nSemester\\nGrade\\nCourse−\\n \\n \\nFig. 1: A sample conceptual-model hypergraph. \\n \\nA. Conceptual-Model Hypergraphs \\n  \\n A highly abstracted conceptual-model hypergraph for \\na simple reporting system of a small college is shown in \\nFig. 1. To show the essential features of the proposed \\ndesign strategy, many details of the hypergraph are \\nomitted. For example, the name, address, picture, and other \\npersonal items of a student are nowhere to be found in the \\nhypergraph. Here, we identify the salient features of the \\nhypergraph, although similar explanations can also be \\nfound in [8]. \\n The hypergraph of Fig. 1 has six sets of objects, which \\nare called Student, SSN (Social Security Number), \\nSemester, Course-Section Pair, CrnSecID, and Grade. The \\nsets Student, SSN, Semester, and Grade are all self-'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='are called Student, SSN (Social Security Number), \\nSemester, Course-Section Pair, CrnSecID, and Grade. The \\nsets Student, SSN, Semester, and Grade are all self-\\nexplanatory. The set Course-Section Pair contains the pairs \\nof every course and all the sections of that course. As an \\nexample, an element of that set might be IS301-01, which \\nrepresents Section 1 of the course IS301: Introduction of \\nInformation Systems. We use a six characters string in this \\npaper to represent a course-section pair and CrnSecID is \\nthe set of all such strings. \\n There are three relationship sets among the sets of \\nobjects in Fig. 1. The first relationship set, represented as a \\nline, is between the sets Student and SSN. It is called a \\nrelationship set because it is a set of relationships. As an \\nexample, relationships, or elements, of that set might be \\n(student\\n1, 111223333) and (student 2, 222334444), \\nrepresenting student1 is related to the SSN 111223333 and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='example, relationships, or elements, of that set might be \\n(student\\n1, 111223333) and (student 2, 222334444), \\nrepresenting student1 is related to the SSN 111223333 and \\nstudent2 to 222334444. The line between Student and SSN \\nhas arrow heads at both ends, which represents a one-to-\\none correspondence between the sets Student and SSN. In \\nour example, student\\n1 is thus paired with 111223333 and \\nso is student2 with 222334444. The second relationship set \\nis between the sets Course-Section Pair and CrnSecID, \\nwhich is also represented as a line. Because both ends of \\nthat line have arrow heads, each course-section pair is \\npaired with exactly one CrnSecID. The third relationship \\nset is among the sets Student, Semester, Grade, and \\nCourse-Section Pair, represented as a diamond. There is a \\nsingle arrow head of this relationship set, pointing at the \\nobject set Grade. It represents the functional dependency \\n(FD) Student, Course-Section Pair, Semester → Grade,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='single arrow head of this relationship set, pointing at the \\nobject set Grade. It represents the functional dependency \\n(FD) Student, Course-Section Pair, Semester → Grade, \\nwhich means that for each tuple of a Student element, a \\nCourse-Section Pair element and a Semester element, there \\ncan only be one Grade element. The circle close to the set \\nCourse-Section Pair means that a particular course-section \\npair might or might not relate to the elements of the other \\nthree sets. In other words, there are some existing course-\\nsection pairs are not used in the relationship set. \\n \\nB. Access Patterns of the most commonly executed Queries \\nand Operations \\n \\n Suppose that the current semester is Summer 2020 \\n(2020S). At the end of 2020S, every instructor will enter a \\ngrade for every student in his/her classes. The reporting \\nsystem thus needs to present a class list to each instructor \\nand the instructor will enter a grade for every SSN on the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='grade for every student in his/her classes. The reporting \\nsystem thus needs to present a class list to each instructor \\nand the instructor will enter a grade for every SSN on the \\nclass list. Hence, there are two access patterns, which are \\nshortened as AP1 and AP2, and their involved object sets \\nof Fig. 1 are outlined in Fig. 2. \\n \\nAccess Pattern 2\\nSection Pair\\nStudent SSN\\nSemester\\nGrade\\nCrnSecID\\nAccess Pattern 1\\nCourse−\\n \\n \\nFig. 2: Access patterns of two commonly executed query and update \\noperation at the end of 2020S. \\n \\nAP1: The query that generates a class list of 2020S will \\nfirst select the semester 2020S and a particular CrnSecID. \\nThen a list of SSNs is generated. Thus, this query involves \\nall of the object sets in Fig. 1 except Grade.  \\nAP2: The update operation is to record the grade earned by \\na student for any class of 2020S for which the student \\nregistered. Thus, this operation involves every object set of \\nthe relationship set in Fig. 1.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='a student for any class of 2020S for which the student \\nregistered. Thus, this operation involves every object set of \\nthe relationship set in Fig. 1. \\n \\nC. Simplification of Conceptual-model Hypergraphs  \\n \\n Because of the one-to-one correspondence between the \\nobject sets Student and SSN, each student can be replaced \\nby his/her SSN. The same is also true for the object sets \\nProceedings of the 2020 IEEE IEEM\\n904\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Course-Section Pair and CrnSecID. We thus obtain the \\nsimplified hypergraph in Fig. 3. \\n \\nAccess Pattern 2\\nSemester\\nGrade\\nAccess Pattern 1\\nSSN\\nCrnSecID\\n \\n \\nFig. 3: A simplified conceptual-model hypergraph. \\n \\nD. Generation of Hierarchical Schemas \\n \\n The algorithms in [8] can generate hierarchical \\nschemas from any conceptual-model hypergraph with any \\nnumber of relationship sets and each relationship set can \\ncontain any number of object sets. The hypergraph in Fig. \\n3 only has one single relationship set that contains four \\nobject sets. First, note that the single relationship set of Fig. \\n3 is in BCNF [6], meaning that it does not have data \\nredundancy with respect to the FD SSN, CrnSecID, \\nSemester → Grade. Based on Nested Normal Form, the \\nobject sets of the relationship set can be organized in any \\nsingle path hierarchical schema without any unwanted \\nredundant data. Two possible Nested Normal Form \\nhierarchical schemas are shown in Fig. 4.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='single path hierarchical schema without any unwanted \\nredundant data. Two possible Nested Normal Form \\nhierarchical schemas are shown in Fig. 4. \\n The left hierarchical schema is derived from Access \\nPattern 1, denoting the fact that for each pair of CrnSecID \\nand Semester, there is a set of SSNs. Note that the object \\nset Grade is left out because Grade is not needed in the \\nquery that generates the class list of any class of 2020S. \\nThe right hierarchical schema is derived from Access \\nPattern 2. It means that for each pair of SSN and Semester, \\nthere is a list of pairs of the form CrnSecID and Grade. \\n  \\nIII.  RESULTS \\n \\nA.  Simulation Background \\n \\nThe experiments were conducted on AWS Educate, a \\nfree platform for cloud learning [2]. A Cloud9 \\nenvironment, which is a cloud IDE (Integrated \\nDevelopment Environment) for writing, running, and \\ndebugging code, was first created. We selected the default \\nsettings for the environment whose EC2 instance type is'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Development Environment) for writing, running, and \\ndebugging code, was first created. We selected the default \\nsettings for the environment whose EC2 instance type is \\nt2.micro that has 1 virtual CPU, 1 GB of memory, and a \\nLinux OS. (Amazon Elastic Compute Cloud (EC2) is a web \\nservice that provides secure, resizable compute capacity in \\nthe cloud.) A real-world production environment will \\nclearly have a more substantial virtual computing \\nenvironment. \\n \\n \\n \\nFig. 4: Two hierarchical schemas generated from the access patterns in \\nFig. 3. \\n \\nAfter which, a Python program generated two \\nDynamoDB tables, called IEEM2020-CrnSec and \\nIEEM2020-Students where IEEM2020-CrnSec is to hold \\nthe data of the left hierarchical schema in Fig. 4 and \\nIEEM2020-Students for the right one. The read and write \\ncapacity of each table were set to 500 units, which costs \\nabout $290 monthly for each table. The partition key of \\nIEEM2020-CrnSec is set to Semester and the sort key to'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='capacity of each table were set to 500 units, which costs \\nabout $290 monthly for each table. The partition key of \\nIEEM2020-CrnSec is set to Semester and the sort key to \\nCrnSecID. This is appropriate because the records of \\nIEEM2020-CrnSec ought to be partitioned according to \\nSemester, and within each semester the records are further \\nsorted according to their Cr nSecIDs. Each Semester and \\nCrnSecID pair in IEEM2020-CrnSec has a list of SSNs \\nwhere a DynamoDB list is similar to a Python list. For the \\ntable IEEM2020-Students, the partition key is SSN and the \\nsort key is Semester. This is also appropriate because \\nstudents are the focus of IEEM2020-Students and thus SSN \\npartitions the table. The records of a student are further \\nsorted according to Semester. Each SSN and Semester pair \\nin IEEM2020-Students is associated with a list of maps \\nwhere a DynamoDB map is similar to a Python dictionary. \\nNote that the root nodes of the two hierarchical'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='in IEEM2020-Students is associated with a list of maps \\nwhere a DynamoDB map is similar to a Python dictionary. \\nNote that the root nodes of the two hierarchical \\nschemas each consists of two object sets. Although such \\nchoices are dictated in part by the semantics of the \\napplication, they are chosen also because primary keys of \\nDynamoDB tables can only contain two or less attributes. \\nNote that this is a severe limitation because the primary key \\nof the relationship set in Fig. 3 is in fact SSN, Semester, \\nCrnSecID and many real-world primary key may consist of \\nmore than two attributes. \\n \\nB.  Simulation Data \\n \\n We’ve randomly generated 1,000 through 10,000, in \\nthe increment of 1,000, active students to simulate the \\nworkload of the reporting system at the end of 2020S. Only \\nactive students have been simulated because only active \\nstudents have registered for classes in 2020S and only they \\nwill receive a grade for their classes of 2020S. Each active'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='active students have been simulated because only active \\nstudents have registered for classes in 2020S and only they \\nwill receive a grade for their classes of 2020S. Each active \\nstudent has enrolled in classes for a number x of semesters \\nwhere the integer x follows an exponential distribution with \\nthe scale set to 6. Hence, most active students have enrolled \\nin less than 5 semesters and at most 12 semesters. Each \\nactive student has registered a number x of courses in a \\nAccess Pattern 1\\nCrnSecID, Semester\\nSSN\\nSemesterSSN,\\nGradeCrnSecID,\\nAccess Pattern 2\\nProceedings of the 2020 IEEE IEEM\\n905\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='semester where the integer x follows a normal distribution \\nwith the mean and standard derivation respectively set to 4 \\nand 1. All of these data were written out to JSON \\n(JavaScript Object Notation) files, preparing to be loaded \\ninto the DynamoDB tables. \\n \\n[ \\n{ \\n\"SSN\": 100327004, \\n\"Semester\": \"2020S\", \\n\"CrnSecList\":{\"432754\": \" \",\"567017\": \" \",\"716933\": \" \",\"290347\": \" \"} \\n}, \\n{ \\n\"SSN\": 100327004, \\n\"Semester\": \"2020W\", \\n\"CrnSecList\":{\"317239\": \"D\",\"312013\": \"A\",\"772832\": \"A\",\"294905\": \"D\"} \\n}, \\n... \\n] \\nFig. 5: Some randomly generated active student records. \\nFig. 5 shows a part of the JSON file that contains some \\nrandomly generated student records about an active student \\nwhose SSN is 100327004, and the semesters for which \\nhe/she enrolled (2020S and 2020W), and the grade he/she \\nearned for each CrnSecID of which he/she registered. Note \\nthat the grades for 2020S are all blank. \\n \\n[ \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"116447\",'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='earned for each CrnSecID of which he/she registered. Note \\nthat the grades for 2020S are all blank. \\n \\n[ \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"116447\", \\n\"StdList\": [143267942, 167456489, 191522130, 203634078, 218949345, 228395488, \\n262223274, 299359064, 340291307, 383893398, 402528408, 429940466, 491781834, \\n503895266, 524376955, 555747721, 588971187, 622489021, 632925410, 683107756, \\n707819456, 736073179, 746724915, 753864030, 759208116, 763603644, 764238408, \\n775280136, 783255531, 808439142, 814791444, 827636210, 865237592, 921601262, \\n942888972, 969574148, 972276135] \\n}, \\n{ \\n\"Semester\": \"2020S\", \\n\"CrnSec\": \"143078\", \\n\"StdList\": [111183936, 143267942, 156225049, 157319776, 163586751, 224053635, \\n233214797, 234530365, 262458151, 295119560, 321794227, 329578153, 331832654, \\n393320569, 406218919, 459185184, 503871987, 509132188, 523055023, 547079471, \\n587846611, 589394822, 614437773, 655053546, 659577543, 665339504, 665548465,'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='393320569, 406218919, 459185184, 503871987, 509132188, 523055023, 547079471, \\n587846611, 589394822, 614437773, 655053546, 659577543, 665339504, 665548465, \\n738868279, 741704917, 775280136, 816330964, 823199550, 877078492, 897820459, \\n923783761, 944155639, 977837091] \\n}, \\n... \\n] \\nFig. 6: Some randomly generated class lists.   \\n Fig. 6 shows a part of the JSON file that contains the \\nclass lists of certain pairs of CrnSecID and 2020S. Each \\nclass list is essentially a list of SSNs \\n \\nC.  Simulation Results. \\n The query that retrieves every class list of 2020S and \\nthe update operation that assigns a grade to every class of \\neach active student of 2020S are written as parts of a \\nPython program, shown in Fig. 7. The code segment was \\nrepeated 5 times, and the elapsed time was printed out at \\nthe end of the loop.  \\nimport timeit \\n \\ncode_to_test = \"\"\" \\nimport random \\nimport boto3 \\nfrom boto3.dynamodb.conditions import Key \\n \\ndynamodb = boto3.resource(\\'dynamodb\\', region_name=\\'us-east-1\\')'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='the end of the loop.  \\nimport timeit \\n \\ncode_to_test = \"\"\" \\nimport random \\nimport boto3 \\nfrom boto3.dynamodb.conditions import Key \\n \\ndynamodb = boto3.resource(\\'dynamodb\\', region_name=\\'us-east-1\\') \\ntableC = dynamodb.Table(\\'IEEM2020-CrnSec\\') \\ntableS = dynamodb.Table(\\'IEEM2020-Students\\') \\n \\nrespC = tableC.query(KeyConditionExpression=Key(\\'Semester\\').eq(\\'2020S\\')) \\n \\nfor item in respC[\\'Items\\']: \\n    for sid in item[\\'StdList\\']: \\n        respS = tableS.update_item( \\n            Key = {\\'SSN\\': sid, \\'Semester\\': \\'2020S\\'}, \\n            ExpressionAttributeNames={ \\n                \"#crnsec\": item[\\'CrnSec\\'], \\n            }, \\n            ExpressionAttributeValues={ \\n                \":grade\": random.choice([\\'A\\',\\'B\\',\\'C\\',\\'D\\',\\'E\\']), \\n            }, \\n            UpdateExpression=\"set CrnSecList.#crnsec = :grade\", \\n            ) \\n     \\n\"\"\" \\nelapsed_time = timeit.timeit(code_to_test, number=5) \\nprint(elapsed_time) \\nFig. 7: The Python program that retrieves every class list of 2020S and'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=') \\n     \\n\"\"\" \\nelapsed_time = timeit.timeit(code_to_test, number=5) \\nprint(elapsed_time) \\nFig. 7: The Python program that retrieves every class list of 2020S and \\nupdates the grade of every class of every active student.  \\nSince the SSNs of every class list are randomly \\ngenerated, every student has an equal probability of being \\nupdated. Therefore, there are no localities of updates. Table \\n1 shows the average time the Python program took to \\nperform an update and Fig. 8 shows the information \\ngraphically. Note that DynamoDB took less than 0.0085 \\nsecond to perform an update for no more than 10,000 \\nstudents.\\n \\n  TABLE I \\nAVERAGE TIME OF AN UPDATE \\n \\nNum of \\nstudents \\nNum of \\ncourse \\nsection \\npairs \\nNum of \\niterations \\nNum of \\nupdates \\nElapsed \\ntime \\nAverage \\ntime of \\neach \\nupdate  \\n1000 100 5 3975 140.821 0.00709 \\n2000 200 5 7924 298.602 0.00754 \\n3000 300 5 12027 448.486 0.00746 \\n4000 400 5 16109 547.187 0.00679 \\n5000 500 5 20015 746.519 0.00746'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='time of \\neach \\nupdate  \\n1000 100 5 3975 140.821 0.00709 \\n2000 200 5 7924 298.602 0.00754 \\n3000 300 5 12027 448.486 0.00746 \\n4000 400 5 16109 547.187 0.00679 \\n5000 500 5 20015 746.519 0.00746 \\n6000 600 5 24031 885.607 0.00737 \\n7000 700 5 28020 1116.735 0.00797 \\n8000 800 5 31924 1293.276 0.00810 \\n9000 900 5 35851 1353.335 0.00755 \\n10000 1000 5 40207 1470.509 0.00731 \\n \\n \\n \\nFig. 8: Plotting average time of an update against number of updates.  \\n \\n To perform a stress test on the update operations on the \\ndatabase, we’ve randomly generated a long text string to \\nstore with each SSN and Semester pair in the table \\nIEEM2020-Students, where the length of every text string \\nfollows a normal distribution with the mean and standard \\nderivation set to 4,000 and 100 bytes respectively. We’ve \\nalso inserted a long text string for every pair of Semester \\nand CrnSecID in the table IEEM2020-CrnSec. The purpose \\nof these long text stings in both tables are to ensure that'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='also inserted a long text string for every pair of Semester \\nand CrnSecID in the table IEEM2020-CrnSec. The purpose \\nof these long text stings in both tables are to ensure that \\nthere are great physical distances among the updates. The \\nresults of the stress test are shown in Table 2. Note that \\neven though long text strings have been inserted in both \\ntables, the average time for each update in IEEM2020-\\nStudents is not much different than the case that has no long \\ninserted text strings. \\n \\n \\n \\nProceedings of the 2020 IEEE IEEM\\n906\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='TABLE II \\nAVERAGE TIME OF AN UPDATE WITH LONG TEXT STRINGS \\nINSERTED. \\n \\nNum of \\nstudents \\nNum of \\ncourse \\nsection \\npairs \\nNum of \\niterations \\nNum of \\nupdates \\nElapsed \\ntime \\nAverage \\ntime of \\neach \\nupdate  \\n1000 100 5 3992 140.662 0.00705 \\n2000 200 5 7976 300.682 0.00754 \\n3000 300 5 11997 470.264 0.00784 \\n4000 400 5 15956 567.652 0.00712 \\n5000 500 5 19935 732.344 0.00735 \\n \\nIV.  DISCUSSION \\n \\n Astute readers might discover that the relationship \\nbetween a SSN, a Semester and a CrnSecID is being stored \\nin two distinct places, once in the table IEEM2020-CrnSec \\nand once in IEEM2020-Students, although the same \\nrelationship is being stored with different focuses in the \\ntwo tables. The Developer Guide for Amazon DynamoDB \\nin fact recommends using adjacency list to model many-to-\\nmany relationships, where we can see that a many-to-many \\nrelationship is also being stored in two distinct places in the \\nadjacency list. A basic tenet of the theory of relational'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='many relationships, where we can see that a many-to-many \\nrelationship is also being stored in two distinct places in the \\nadjacency list. A basic tenet of the theory of relational \\ndatabases is that redundant data always lead to high update \\ncost. However, many practitioners of NoSQL databases \\noftentimes champion denormalization in order to improve \\nperformance. This, of course, deserves more study. \\n \\nV.  CONCLUSION \\n \\n In this paper, we have presented evidence for the \\nfeasibility of a DynamoDB schema design strategy, which \\nis based on Nested Normal Form and past experiences on \\nXML databases. Such a design strategy begins with a \\nconceptual-model hypergraph, which is in a way similar to \\nUML domain class diagrams. It then generates hierarchical \\nschemas in Nested Normal Form that are guaranteed to be \\nfree of unwanted redundant data. The generated \\nhierarchical schemas are then mapped to DynamoDB \\ntables for implementation. \\n We have performed experiments to further show the'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='free of unwanted redundant data. The generated \\nhierarchical schemas are then mapped to DynamoDB \\ntables for implementation. \\n We have performed experiments to further show the \\nusefulness of the proposed design strategy. We have \\nrandomly generated 1,000 through 10,000 active students, \\nin the increment of 1,000, and each active student has \\nenrolled in a number of semesters that follows an \\nexponential distribution. In addition, each active student \\nhas registered for a number of courses that follows a \\nnormal distribution in each semester. The results show that \\non average each update has taken less than 0.0085 second. \\nWe’ve also subjected the database to a stress test where a \\nlong text string was inserted into a student record and a \\ncourse-section record. The purpose of which is to ensure \\nthat updates are not done in close localities. It turns out that \\nthe average update time is about the same with or without \\nthe inserted long text strings.'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='that updates are not done in close localities. It turns out that \\nthe average update time is about the same with or without \\nthe inserted long text strings. \\n Much more work remains to be done. Currently we \\nconstruct a hierarchical schema for each assess pattern, \\nwhich may lead to redundancy. However, we shall study \\nthe update cost for making such a choice. As more results \\nare being developed, a future journal paper shall report the \\nfiner details of the proposed schema design strategy.  \\n \\nREFERENCES \\n \\n[1] “Amazon DynamoDB: Fast and flexible NoSQL database \\nservice for any scale,” 2020. Accessed on: May 30, 2020. \\n[Online].Available: https://aws.amazon.com/dynamodb/ \\n[2] “AWS Educate: Your Cloud Journey Starts Here,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://aws.amazon.com/education/awseducate/ \\n[3] “BaseX: The XML Framework,” 2020. Accessed on: May 30, \\n2020. [Online]. Available: http://basex.org/ \\n[4] G. Booch, J. Rumbaugh and, I. Jacobson, The Unified'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='[3] “BaseX: The XML Framework,” 2020. Accessed on: May 30, \\n2020. [Online]. Available: http://basex.org/ \\n[4] G. Booch, J. Rumbaugh and, I. Jacobson, The Unified \\nModeling Language User Guide . Addison Wesley object \\ntechnology series, Addison-Wesley, 2005 \\n[5] “Cassandra: Manage massive amounts of data, fast, without \\nlosing sleep,” 2020. Accessed on: May 30, 2020. [Online]. \\nAvailable: https://cassandra.apache.org/\\n \\n[6] D. Maier, The Theory of Relational Databases . Computer \\nScience Press, 1983. \\n[7] W.Y. Mok, and D.W. Embley, “Generating Compact \\nRedundancy-Free XML Documents from Conceptual-Model \\nHypergraphs,” IEEE Trans. Knowl. Data Eng. , vol. 18, no. \\n8, pp. 1082–1096, 2006. \\n[8] W.Y. Mok, J. Fong, and D.W. Embley, “Generating the \\nfewest redundancy-free scheme trees from acyclic \\nconceptual-model hypergraphs in polynomial time,” Inf. \\nSyst., vol. 41, pp. 20-44, 2014. \\n[9] W.Y. Mok, Y.K. Ng, and D.W. Embley, “A Normal Form \\nfor Precisely Characterizing Redundancy in Nested'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='conceptual-model hypergraphs in polynomial time,” Inf. \\nSyst., vol. 41, pp. 20-44, 2014. \\n[9] W.Y. Mok, Y.K. Ng, and D.W. Embley, “A Normal Form \\nfor Precisely Characterizing Redundancy in Nested \\nRelations,” ACM Trans. Database Syst. , vol. 21, no. 1, pp. \\n77-106, 1996. \\n[10] “MongoDB: The database for modern applications,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.mongodb.com/\\n \\n[11] “NoSQL Databases.” 2020. A ccessed on: May 30, 2020. \\n[Online]. Available: https://www.trustradius.com/nosql-\\ndatabases \\n[12] “Redis: Experience the fastest NoSQL database in the \\ncloud.” 2020. Accessed on: May 30, 2020. [Online]. \\nAvailable: https://redislabs.com/ \\n[13] “Visual Paradigm: Stay Competitive and Responsive to \\nChange Faster & Better in the Digital World,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.visual-paradigm.com/\\n \\nProceedings of the 2020 IEEE IEEM\\n907'),\n",
       " Document(metadata={'producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText® 5.5.6 ©2000-2015 iText Group NV (AGPL-version)', 'creator': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:06 PM'\", 'creationdate': '2020-10-23T18:42:02-07:00', 'meeting starting date': '14 Dec. 2020', 'moddate': '2021-06-18T07:54:55-04:00', 'ieee article id': '9309967', 'ieee issue id': '9309664', 'subject': '2020 IEEE International Conference on Industrial Engineering and Engineering Management (IEEM);2020; ; ;10.1109/IEEM45057.2020.9309967', 'application': \"'Certified by IEEE PDFeXpress at 10/23/2020 6:42:07 PM'\", 'ieee publication id': '9309717', 'title': 'A Feasible Schema Design Strategy for Amazon DynamoDB: A Nested Normal Form Approach', 'meeting ending date': '17 Dec. 2020', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[DataModeling_&_Performance_evaluation]_A_Feasible_Schema_Design_Strategy_for_Amazon_DynamoDB_A_Nested_Normal.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='Change Faster & Better in the Digital World,” 2020. \\nAccessed on: May 30, 2020. [Online]. Available: \\nhttps://www.visual-paradigm.com/\\n \\nProceedings of the 2020 IEEE IEEM\\n907\\nAuthorized licensed use limited to: San Francisco State Univ. Downloaded on June 18,2021 at 11:54:55 UTC from IEEE Xplore.  Restrictions apply.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n54 | P a g e  \\n \\nSQL vs. NoSQL: Choosing the Right Database for Your E-\\ncommerce Platform \\n1Lakshmi Nivas Nalla, 2Vijay Mallik Reddy \\n1Data Engineer Lead, Florida International University,11200 SW 8th St, Miami, FL 33199, \\nEmail:nallanivas@gmail.com \\n2Member of Technical Staff, University of North Carolina at Charlotte, Email: \\nvijaymr1012@gmail.com \\nAbstract: Selecting the appropriate database technology is a critical decision for e-commerce \\nplatforms aiming to scale effectively and accommodate the ever -increasing volume and \\ncomplexity of data. This paper provides an in -depth comparison of SQL (Structured Query \\nLanguage) and NoSQL (Not Only SQL) databases, elucidating their respecti ve strengths, \\nweaknesses, and suitability for e -commerce applications. Through a comprehensive analysis of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='Language) and NoSQL (Not Only SQL) databases, elucidating their respecti ve strengths, \\nweaknesses, and suitability for e -commerce applications. Through a comprehensive analysis of \\nfactors such as data structure, scalability, performance, consistency, and flexibility, we offer \\nguidance to help e-commerce businesses make informed decisions when choosing between SQL \\nand NoSQL databases. By understanding the distinctive features and trade -offs associated with \\neach database paradigm, businesses can optimize their database architecture to support seamless \\noperations, enhance user experiences, and drive sustainable growth in the competitive e-commerce \\nlandscape. \\nKeywords: SQL, NoSQL, Database Management Systems, E -commerce, Scalability, \\nPerformance, Data Modeling, Consistency, Flexibility, Decision-Making. \\nIntroduction \\nIn the contempora ry digital landscape, the proliferation of e -commerce platforms has'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}, page_content='Performance, Data Modeling, Consistency, Flexibility, Decision-Making. \\nIntroduction \\nIn the contempora ry digital landscape, the proliferation of e -commerce platforms has \\nrevolutionized the way consumers interact with businesses, ushering in an era of unprecedented \\nconvenience, choice, and accessibility. At the heart of these dynamic digital ecosystems lies  the \\ndatabase, serving as the foundational infrastructure that underpins the storage, retrieval, and \\nmanagement of vast volumes of transactional data, user profiles, and product catalogs. As e -\\ncommerce platforms strive to meet the evolving needs and expectations of modern consumers, the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n55 | P a g e  \\n \\nselection of an appropriate database technology emerges as a critical determinant of scalability, \\nperformance, and operational efficiency. \\nThe dichotomy between SQL (Structured Query Language) and NoSQL (Not Only SQL) \\ndatabases represents a fundamental choice faced by e -commerce businesses when architecting \\ntheir database systems. SQL databases, characterized by their relational data model and adherence \\nto ACID (Atomicity, Consistency, Isolation, Durability) properties, have long been the cornerstone \\nof traditional database management systems (DBMS). Conversely, NoSQL databases embrace a \\nmore flexible, schema -less approach, catering to the diverse data structures and distributed \\narchitectures prevalent in modern e-commerce applications. \\nThe decision -making process surrounding the selection of SQL versus NoSQL databases is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='architectures prevalent in modern e-commerce applications. \\nThe decision -making process surrounding the selection of SQL versus NoSQL databases is \\nmultifaceted, encompassing a myriad of technical, operational, and business considerations. From \\ndata modeling and scalability to performance optimization and  data consistency, each database \\nparadigm presents unique strengths and trade-offs that must be carefully evaluated in the context \\nof the specific requirements and constraints of the e-commerce platform. \\nThis paper embarks on a comprehensive exploration of the SQL versus NoSQL debate in the realm \\nof e-commerce database management, with a fervent commitment to scientific rigor, empirical \\nvalidation, and practical relevance. Through a synthesis of existing literature, empirical studies, \\nand real-world case examples, we endeavor to elucidate the distinctive features, advantages, and \\nlimitations associated with each database paradigm. By providing a nuanced understanding of the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='and real-world case examples, we endeavor to elucidate the distinctive features, advantages, and \\nlimitations associated with each database paradigm. By providing a nuanced understanding of the \\ntechnical nuances and strategic implications inherent in the SQL versus NoSQL decisio n, this \\npaper aims to empower e -commerce businesses to make informed choices that align with their \\noverarching goals and objectives. \\nMoreover, the conduction of data relevant to the topics at hand forms the cornerstone of this \\ninquiry. By drawing upon a di verse array of data sources, including benchmarking studies, \\nperformance evaluations, and case studies from industry practitioners, we seek to ground our \\nanalysis in empirical evidence and real -world insights. Through meticulous data collection, \\nvalidation, and analysis, we strive to offer actionable recommendations and best practices that \\nresonate with the evolving needs and challenges faced by e -commerce businesses in an'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='validation, and analysis, we strive to offer actionable recommendations and best practices that \\nresonate with the evolving needs and challenges faced by e -commerce businesses in an \\nincreasingly competitive marketplace. Thus, the scientific values upheld in this stud y underscore'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n56 | P a g e  \\n \\nour commitment to advancing knowledge and fostering informed decision -making within the e -\\ncommerce domain. \\nLiterature Review \\nThe discourse surrounding the SQL versus NoSQL debate in the context of e-commerce database \\nmanagement has been a focal point of scholarly inquiry and practical deliberation in recent years. \\nThis section offers a comprehensive review of the literature, synthesizing key findings, \\ncomparisons, and trends elucidated by researchers and industry experts. \\nHistorical Evolution: The historical evolution of database management systems (DBMS) sets the \\nstage for understanding the divergent trajectories of SQL and NoSQL databases. SQL databases, \\nrooted in the relational model pioneered by Edgar F. Codd in the 1970s, have long been \\nsynonymous with structured data storage, declarative querying, and transactional integrity. In'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='rooted in the relational model pioneered by Edgar F. Codd in the 1970s, have long been \\nsynonymous with structured data storage, declarative querying, and transactional integrity. In \\ncontrast, the emergence of NoSQL databases in the late 2000s marked a paradigm shift towards \\ndistributed, non -relational data stores optimized for scalability, flex ibility, and performance in \\nweb-scale applications (Brewer, 2012). \\nPerformance and Scalability:  One of the primary motivations driving the adoption of NoSQL \\ndatabases in e-commerce applications is their superior performance and scalability characteristics \\ncompared to traditional SQL databases. Research by Stonebraker et al. (2007) demonstrated the \\nlimitations of SQL databases in handling the high volume and velocity of data generated by e -\\ncommerce transactions, highlighting the need for alternative approaches to database management. \\nNoSQL databases, with their distributed architectures and horizontal scalability, offer compelling'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 2, 'page_label': '3'}, page_content='commerce transactions, highlighting the need for alternative approaches to database management. \\nNoSQL databases, with their distributed architectures and horizontal scalability, offer compelling \\nsolutions to the scalability challenges inherent in e -commerce platforms, enabling seamless \\nhandling of massive datasets and concurrent user interactions (Kaufman et al., 2010). \\nData Modeling and Flexibility:  The relational data model enforced by SQL databases imposes \\nrigid schema structures that can be cumbersome to adapt in the dynamic, rapidly evolving context \\nof e-commerce applications (Bruns, 2018). NoSQL databases, by contrast, embrace a schema-less \\nor schema -flexible approach, allowing for agile data modeling and iteration in response to \\nchanging business requirements (Fowler, 2013). This flexibility is particularly advantageous in e-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n57 | P a g e  \\n \\ncommerce environments characterized by diverse product catalogs, evolving user preferences, and \\ncomplex transactional workflows (Cattell, 2010). \\nConsistency and Transactional Integrity: A recurring critique leveled against NoSQL databases \\npertains to their relaxed consistency models and eventual consistency guarantees, which deviate \\nfrom the strict ACID properties upheld by traditional SQL databases (Bailis et al., 2013). While \\nthis eventual consistency model may suffice for certain e -commerce use ca ses, such as \\nrecommendation engines and content delivery systems, it may fall short in scenarios requiring \\nstrong transactional guarantees, such as order processing and inventory management (Coulouris et \\nal., 2012). \\nReal-World Case Studies:  Several real -world case studies provide empirical evidence of the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content=\"al., 2012). \\nReal-World Case Studies:  Several real -world case studies provide empirical evidence of the \\nefficacy and trade -offs associated with SQL and NoSQL databases in e -commerce applications. \\nFor instance, Amazon's transition from a monolithic SQL -based architecture to a distributed, \\nmicroservices-based architecture powered by NoSQL databases has been well -documented, \\nhighlighting the scalability and agility advantages afforded by NoSQL technologies in handling \\nthe company's massive scale and dynamic workload patterns (Vogels, 2009). \\nRecent Trends and Emerging Technologies: Recent years have witnessed the convergence of \\nSQL and NoSQL paradigms through the emergence of NewSQL databases, which aim to combine \\nthe scalability and flexibility of NoSQL with the transactional integrity of traditional SQL \\ndatabases (Stonebraker, 2010). These hybrid approaches, exemplified by technologies such as\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='the scalability and flexibility of NoSQL with the transactional integrity of traditional SQL \\ndatabases (Stonebraker, 2010). These hybrid approaches, exemplified by technologies such as \\nGoogle Spanner and CockroachDB, represent promising avenues for reconciling the divergent \\nrequirements of e -commerce applications while preserving the robustness and con sistency \\nguarantees of SQL databases (Corbett et al., 2012). \\nConclusion: In conclusion, the literature review underscores the multifaceted nature of the SQL \\nversus NoSQL debate in e -commerce database management, encompassing considerations of \\nperformance, scalability, flexibility, consistency, and transactional integrity. While NoSQL \\ndatabases offer compelling solutions to the scalability challenges inherent in e -commerce \\nplatforms, they come with trade -offs in terms of consistency and transactional guarant ees. \\nConversely, SQL databases provide strong consistency guarantees but may struggle to scale'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 3, 'page_label': '4'}, page_content='platforms, they come with trade -offs in terms of consistency and transactional guarant ees. \\nConversely, SQL databases provide strong consistency guarantees but may struggle to scale \\neffectively in high -volume, distributed environments. As e -commerce platforms continue to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n58 | P a g e  \\n \\nevolve and scale, the selection of an appropriate database technology r emains a pivotal decision, \\nnecessitating a nuanced understanding of the technical nuances and strategic implications \\nassociated with SQL and NoSQL paradigms. \\nLiterature Review \\nThe debate surrounding SQL versus NoSQL databases in e -commerce has been fueled by a \\nplethora of empirical studies and industry reports, each offering nuanced insights into the \\ncomparative advantages and limitations of these database paradigms. A seminal study by \\nStonebraker et al. (2007) compared the performance of SQL and NoSQL databases in handling e-\\ncommerce workloads, revealing that NoSQL databases outperformed their SQL counterparts in \\nterms of throughput and latency under high concurrency scenarios. These findings underscored the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='commerce workloads, revealing that NoSQL databases outperformed their SQL counterparts in \\nterms of throughput and latency under high concurrency scenarios. These findings underscored the \\nscalability advantages of NoSQL databases in acco mmodating the unpredictable traffic patterns \\nand surges characteristic of e-commerce platforms. \\nConversely, research by Das et al. (2011) highlighted the trade-offs inherent in NoSQL databases, \\nparticularly in terms of consistency and transactional integrity. Through a series of benchmarking \\nexperiments, Das et al. demonstrated that NoSQL databases, while excelling in scalability and \\navailability, often sacrificed strong consistency guarantees, leading to potential data inconsistency \\nand integrity issues in  e-commerce applications. These findings underscored the importance of \\ncarefully evaluating the consistency requirements and trade-offs associated with different database \\ntechnologies in the context of specific e-commerce use cases.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 4, 'page_label': '5'}, page_content='carefully evaluating the consistency requirements and trade-offs associated with different database \\ntechnologies in the context of specific e-commerce use cases. \\nMoreover, the advent of  cloud computing and distributed computing architectures has further \\ncomplicated the SQL versus NoSQL debate, introducing new considerations related to data \\nlocality, network latency, and cost -effectiveness. Research by Strauch et al. (2015) explored the \\nperformance implications of deploying SQL and NoSQL databases in cloud environments, \\nrevealing that while NoSQL databases offered inherent scalability benefits in distributed \\nenvironments, they incurred higher operational overhead and complexity compared to  SQL \\ndatabases. These findings underscored the need for e-commerce businesses to weigh the trade-offs \\nbetween scalability and operational simplicity when choosing between SQL and NoSQL databases \\nin cloud deployments.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n59 | P a g e  \\n \\nFurthermore, the proliferation of real-time analytics and personalized customer experiences in e -\\ncommerce has spurred interest in database technologies capable of supporting complex event \\nprocessing and real-time data ingestion. Studies by Chen et al. (2018) investigated the suitability \\nof SQL and NoSQL databases for real-time e-commerce analytics, highlighting the challenges and \\nopportunities associated with processing high -velocity, high -volume data streams in real -time. \\nWhile NoSQL databases offered inherent advantages in handling unstructured and semi-structured \\ndata, SQL databases excelled in complex query processing and ad-hoc analytics, underscoring the \\ncomplementary roles of both database paradigms in supporting diverse e-commerce use cases. \\nIn summary, the literature review provides a comprehensive overview of the SQL versus NoSQL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='complementary roles of both database paradigms in supporting diverse e-commerce use cases. \\nIn summary, the literature review provides a comprehensive overview of the SQL versus NoSQL \\ndebate in e -commerce database management, encompassing considerations of performance, \\nscalability, consistency, and suitability for real -time analytics. While NoSQL databases offer \\ninherent advantages in scalability and flexibility, they come with trade-offs in terms of consistency \\nand operational complexity. Conversely, SQL databases provide strong consistency guarantees \\nand robust query capabilities but may struggle to scale effectively in distributed, high-concurrency \\nenvironments. As e -commerce platforms continue to evolve and innovate, the selection of an \\nappropriate database technology remains a pivotal decision, necessitating a holistic understanding \\nof the technical nuances and strategic implications associated with SQL and NoSQL paradigms. \\nMethodology'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 5, 'page_label': '6'}, page_content='of the technical nuances and strategic implications associated with SQL and NoSQL paradigms. \\nMethodology \\nResearch Design: This study adopts a comparative research design to evaluate the suitability of \\nSQL and NoSQL databases for e -commerce applications. The research design encompasses a \\nsystematic analysis of relevant literature, benchmarking studies, and real-world case examples to \\nelucidate the distinctive features, advantages, and limitations associated with each database \\nparadigm. \\nLiterature Review:  A comprehensive literature review was conducted to synthesiz e existing \\nresearch and scholarly discourse on SQL and NoSQL databases in the context of e -commerce. \\nThis entailed a thorough examination of peer-reviewed journals, conference proceedings, industry \\nreports, and academic textbooks to identify key findings, comparisons, trends, and empirical \\nstudies relevant to the research objectives.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n60 | P a g e  \\n \\nData Collection: Data collection encompassed the gathering of scholarly articles, research papers, \\nand industry reports from reputable sources such as academic databases (e.g., IEEE Xplore, ACM \\nDigital Library) and scholarly search engines (e.g., Google Scholar). Additionally, real-world case \\nstudies and benchmarking studies published by industry practitioners and database vendors were \\ncurated to provide empirical insights and validation for the research findings. \\nData Analysis: The collected data were subjected to rigorou s analysis and synthesis to extract \\nmeaningful insights and trends pertaining to the comparative evaluation of SQL and NoSQL \\ndatabases in e -commerce applications. This involved thematic analysis, content analysis, and \\nqualitative synthesis techniques to identify common themes, patterns, and discrepancies across the \\nliterature corpus.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='qualitative synthesis techniques to identify common themes, patterns, and discrepancies across the \\nliterature corpus. \\nComparison Framework: A structured comparison framework was developed to systematically \\nevaluate the performance, scalability, consistency, flexibility, and suitability for real-time analytics \\nof SQL and NoSQL databases in e -commerce contexts. This framework served as a guiding \\nframework for organizing and synthesizing the research findings into a coherent narrative and \\nfacilitating objective comparisons between the two database paradigms. \\nCase Study Analysis:  Real-world case studies and benchmarking studies were analyzed to \\ncomplement the theoretical insights derived from the literature review with empirical evidence and \\npractical implications. This involved scrutinizing the met hodologies, findings, and \\nrecommendations presented in case studies from leading e -commerce companies and database'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 6, 'page_label': '7'}, page_content='practical implications. This involved scrutinizing the met hodologies, findings, and \\nrecommendations presented in case studies from leading e -commerce companies and database \\nvendors to glean actionable insights and best practices for database selection and architecture. \\nValidation and Reliability:  To enhance the v alidity and reliability of the research findings, \\nmultiple data sources were triangulated, and cross -referenced to ensure consistency and \\nrobustness. Moreover, peer -review feedback and expert validation were sought to verify the \\naccuracy and credibility of the research methodology, findings, and conclusions. \\nEthical Considerations: Throughout the research process, ethical considerations were upheld to \\nensure the responsible handling and use of copyrighted materials, proprietary data, and sensitive \\ninformation. Proper attribution and citation practices were adhered to, and permissions were \\nobtained where necessary to avoid plagiarism and copyright infringement.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n61 | P a g e  \\n \\nLimitations: It is important to acknowledge the limitations inherent in the research methodology, \\nincluding potential biases in the selection of literature and case studies, as well as the \\ngeneralizability of the findings to specific e-commerce contexts. Additionally, the dynamic nature \\nof database technology and e-commerce trends may introduce temporal limitations to the research \\nfindings, necessitating continuous monitoring and updating of the literature base. \\nConclusion: In conclusion, the research methodology adopted in this study provides a rigorous \\nand systematic approach to evaluating the suitabil ity of SQL and NoSQL databases for e -\\ncommerce applications. By integrating theoretical insights with empirical evidence and practical \\ncase examples, the methodology offers a comprehensive framework for informed decision-making'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='commerce applications. By integrating theoretical insights with empirical evidence and practical \\ncase examples, the methodology offers a comprehensive framework for informed decision-making \\nand strategic planning in database selection and architecture for e-commerce businesses. \\n \\nData Collection Methods:  The data collection process involved gathering scholarly articles, \\nresearch papers, and industry reports from reputable sources such as academic databases (e.g., \\nIEEE Xpl ore, ACM Digital Library) and scholarly search engines (e.g., Google Scholar). \\nAdditionally, real-world case studies and benchmarking studies published by industry practitioners \\nand database vendors were curated to provide empirical insights and validation  for the research \\nfindings. \\nFormulas Used in Analysis: \\n1. Performance Comparison: \\nPerformance metrics such as throughput (TP) and latency (LT) were calculated using the following \\nformulas:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 7, 'page_label': '8'}, page_content='findings. \\nFormulas Used in Analysis: \\n1. Performance Comparison: \\nPerformance metrics such as throughput (TP) and latency (LT) were calculated using the following \\nformulas: \\nThroughput (TP)=Total number of transactions processedTotal timeThroughput (TP)=Total time\\nTotal number of transactions processed \\nLatency (LT)=Total timeTotal number of transactions processedLatency (LT)=Total number of t\\nransactions processedTotal time \\n2. Scalability Assessment: \\nScalability was evaluated based on the scalability index (SI), calculated as:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n62 | P a g e  \\n \\nScalability Index (SI)=Throughput (TP2)Throughput (TP1)×100%Scalability Index (SI)=Throug\\nhput (TP1)Throughput (TP2)×100% \\nAnalysis Procedure: \\n1. Literature Review:  A comprehensive literature review was conducted to synth esize \\nexisting research and scholarly discourse on SQL and NoSQL databases in the context of \\ne-commerce. Key findings, comparisons, and trends were extracted from peer -reviewed \\njournals, conference proceedings, industry reports, and academic textbooks. \\n2. Data Synthesis: The collected data were subjected to thematic analysis, content analysis, \\nand qualitative synthesis techniques to identify common themes, patterns, and \\ndiscrepancies across the literature corpus. This involved organizing and categorizing the \\nresearch findings according to predefined comparison criteria and evaluation dimensions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='discrepancies across the literature corpus. This involved organizing and categorizing the \\nresearch findings according to predefined comparison criteria and evaluation dimensions. \\n3. Performance Evaluation:  Performance metrics such as throughput and latency were \\ncalculated based on benchmarking studies and real -world case examples. Comparative \\nanalysis was conducted to assess the relative performance of SQL and NoSQL databases \\nin handling e-commerce workloads under varying conditions. \\n4. Scalability Analysis:  Scalability was evaluated using scalability index calculations, \\ncomparing the throughput of SQL and NoSQL databases under increasing workload levels. \\nThe scalability index provided insights into the ability of each database paradigm to scale \\nlinearly with growing transaction volumes. \\n5. Validation and Reliability:  To enhance the validity and reliabilit y of the research \\nfindings, multiple data sources were triangulated, and cross -referenced to ensure'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 8, 'page_label': '9'}, page_content='5. Validation and Reliability:  To enhance the validity and reliabilit y of the research \\nfindings, multiple data sources were triangulated, and cross -referenced to ensure \\nconsistency and robustness. Peer -review feedback and expert validation were sought to \\nverify the accuracy and credibility of the analysis. \\nOriginal Work Published: \\nThe original work resulting from this analysis has been published in [Journal Name], [Year], by \\n[Author Name(s)]. The findings and insights presented in this study contribute to the body of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n63 | P a g e  \\n \\nknowledge on database management in e -commerce and offer p ractical guidance for database \\nselection and architecture in e-commerce businesses. \\nResults: \\nTo demonstrate the comparative performance of SQL and NoSQL databases in an e -commerce \\ncontext, we conducted a series of benchmarking experiments using synthetic workloads simulating \\ntypical e-commerce transaction scenarios. The results revealed notable differences in throughput \\nand latency between the two database paradigms under varying concurrency levels. \\nThroughput Analysis:  SQL databases exhibited robust throug hput performance in low to \\nmoderate concurrency scenarios, with transactional throughput averaging 1000 transactions per \\nsecond (TPS) under steady-state conditions. However, as concurrency levels increased beyond 100'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='moderate concurrency scenarios, with transactional throughput averaging 1000 transactions per \\nsecond (TPS) under steady-state conditions. However, as concurrency levels increased beyond 100 \\nconcurrent users, SQL databases experie nced diminishing throughput gains, plateauing at \\napproximately 1200 TPS due to resource contention and locking overhead. \\nIn contrast, NoSQL databases demonstrated superior scalability and throughput efficiency, \\nachieving linear scalability up to 10,000 TPS under high concurrency conditions. The distributed \\narchitecture and partitioning strategies inherent in NoSQL databases enabled seamless scaling to \\naccommodate the influx of concurrent transactions, resulting in sustained high throughput levels \\neven at peak loads. \\nLatency Analysis:  Latency analysis revealed compelling performance advantages for NoSQL \\ndatabases compared to SQL counterparts, particularly under high concurrency conditions. SQL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 9, 'page_label': '10'}, page_content='Latency Analysis:  Latency analysis revealed compelling performance advantages for NoSQL \\ndatabases compared to SQL counterparts, particularly under high concurrency conditions. SQL \\ndatabases exhibited increasing transaction latency as concurrency levels rose, peaking at 100 \\nmilliseconds (ms) for 100 concurrent users and escalating further to 200 ms for 500 concurrent \\nusers. \\nConversely, NoSQL databases maintained low and consistent latency levels across all concurrency \\nlevels, with transaction response times averaging below 50 ms even at peak loads. The distributed, \\nhorizontally scalable architecture of NoSQL databases facilitated efficient data partitioning and \\nparallel query processing, minimizing contention and latency overhead. \\nDiscussion:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n64 | P a g e  \\n \\nThe results of the benchmarking experiments underscore the significant performance disparities \\nbetween SQL and NoSQL databases in e -commerce applications, particularly in terms of \\nthroughput, scalability, and latency. While SQL databases offer robust transaction al capabilities \\nand strong consistency guarantees, they exhibit limitations in scalability and throughput efficiency \\nunder high concurrency scenarios. \\nConversely, NoSQL databases excel in scalability and throughput efficiency, leveraging \\ndistributed archit ectures and partitioning strategies to accommodate the dynamic workload \\npatterns and concurrency demands inherent in e -commerce platforms. The linear scalability and \\nlow-latency characteristics of NoSQL databases make them well -suited for handling the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='patterns and concurrency demands inherent in e -commerce platforms. The linear scalability and \\nlow-latency characteristics of NoSQL databases make them well -suited for handling the \\nunpredictability and variability of e -commerce transactions, enabling seamless scaling to meet \\ngrowing demands without sacrificing performance or user experience. \\nThe findings of this study have practical implications for e -commerce businesses seeking to \\noptimize their database architecture for performance and scalability. By leveraging NoSQL \\ndatabases, e-commerce platforms can enhance their ability to handle peak loads, support real-time \\ntransaction processing, and deliver responsive user experiences, ultimate ly driving customer \\nsatisfaction and business growth. \\nMoreover, the comparative analysis highlights the importance of aligning database technology \\nchoices with specific application requirements and performance objectives. While SQL databases'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 10, 'page_label': '11'}, page_content='Moreover, the comparative analysis highlights the importance of aligning database technology \\nchoices with specific application requirements and performance objectives. While SQL databases \\nmay suffice fo r transactional workloads with predictable concurrency patterns and stringent \\nconsistency requirements, NoSQL databases offer a compelling alternative for e -commerce \\nplatforms prioritizing scalability, flexibility, and low -latency performance in dynamic, h igh-\\nconcurrency environments. \\nIn conclusion, the results and discussion presented in this study underscore the critical role of \\ndatabase technology in shaping the performance and scalability of e -commerce platforms. By \\nunderstanding the strengths and limit ations of SQL and NoSQL databases and their implications \\nfor transaction processing and user experience, e -commerce businesses can make informed \\ndecisions to optimize their database architecture and drive competitive advantage in the digital \\nmarketplace.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n65 | P a g e  \\n \\nThroughput Analysis: \\nTo quantify the transactional throughput of SQL and NoSQL databases under varying concurrency \\nlevels, we conducted benchmarking experiments using synthetic workloads. The results are \\nsummarized in Table 1 below: \\nTable 1: Transactional Throughput (Transactions per Second, TPS) \\nConcurrency Level SQL Database Throughput (TPS) NoSQL Database Throughput (TPS) \\n10 800 1000 \\n50 1000 2500 \\n100 1200 5000 \\n500 1300 10000 \\n \\n \\nLatency Analysis: \\nTransaction latency was measured as the time taken for a transaction to be processed and \\ncompleted by the database system. The results are presented in Table 2 below: \\nTable 2: Transactional Latency (Milliseconds, ms) \\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n10 20 10 \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n1 2 3 4\\nTransactional Throughput (Transactions per Second, TPS)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 11, 'page_label': '12'}, page_content='Concurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n10 20 10 \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n1 2 3 4\\nTransactional Throughput (Transactions per Second, TPS)\\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n66 | P a g e  \\n \\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms) \\n50 50 15 \\n100 100 20 \\n500 200 30 \\n \\n \\nFormulas Used: \\n1. Throughput (TPS): \\nThroughput (TPS)=Total number of transactionsTotal timeThroughput (TPS)=Total timeTotal n\\number of transactions \\n2. Latency (ms): \\nLatency (ms)=Total timeTotal number of transactionsLatency (ms)=Total number of transactions\\nTotal time \\nExcel File for Charts: \\nThe data provided in Tables 1 and 2 can be easily exported to an Excel file for chart creation. By \\nutilizing the values from these tables, you can generate visual representations such as line charts \\nor bar charts to depict the throughput and latency trends of SQL and NoSQL databases under \\nvarying concurrency levels. This will facilitate a more intuitive understanding of the performance \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n1 2 3 4'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 12, 'page_label': '13'}, page_content='varying concurrency levels. This will facilitate a more intuitive understanding of the performance \\n0\\n100\\n200\\n300\\n400\\n500\\n600\\n700\\n800\\n1 2 3 4\\nTransactional Latency (Milliseconds, ms)\\nConcurrency Level SQL Database Latency (ms) NoSQL Database Latency (ms)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n67 | P a g e  \\n \\ndifferences between the two database paradigms and enable stakeholders to make informed \\ndecisions regarding database selection and optimization for e-commerce applications. \\nConclusion \\nIn this study, we conducted benchmarking experiments to compare the performance of SQL and \\nNoSQL databases in e -commerce applications, focusing on transactional throughput and latency \\nunder varying concurrency levels. The results of our analysis highlight notable differences between \\nthe two database paradigms, with implications for scalability, performance, and user experience in \\ne-commerce platforms.  From the throughput analysis, it is evident that NoSQL databases \\ndemonstrate superior scalability and throughput efficiency compared to SQL counterparts. Under \\nincreasing concurrency levels, NoSQL databases exhibit linear scalability, accommodating higher'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='demonstrate superior scalability and throughput efficiency compared to SQL counterparts. Under \\nincreasing concurrency levels, NoSQL databases exhibit linear scalability, accommodating higher \\ntransaction volumes with minimal degradation in throughput. This scalability advantage enables \\ne-commerce platforms to handle dynamic workload patterns and peak traffic lo ads without \\nsacrificing performance or user experience.  Similarly, the latency analysis reveals compelling \\nperformance advantages for NoSQL databases, particularly under high concurrency scenarios. \\nNoSQL databases consistently maintain low-latency response times, ensuring responsive and real-\\ntime transaction processing even at peak loads. In contrast, SQL databases experience escalating \\nlatency under high concurrency levels, potentially leading to user frustration and degraded \\nperformance. The findings of t his study have practical implications for e -commerce businesses'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='latency under high concurrency levels, potentially leading to user frustration and degraded \\nperformance. The findings of t his study have practical implications for e -commerce businesses \\nseeking to optimize their database architecture for scalability and performance. By leveraging \\nNoSQL databases, e-commerce platforms can enhance their ability to support growing transaction \\nvolumes, deliver responsive user experiences, and capitalize on peak demand periods without \\nexperiencing performance bottlenecks or downtime.  Moreover, the comparative analysis \\nunderscores the importance of aligning database technology choices with specific application \\nrequirements and performance objectives. While SQL databases may suffice for transactional \\nworkloads with predictable concurrency patterns and stringent consistency requirements, NoSQL \\ndatabases offer a compelling alternative for e -commerce pla tforms prioritizing scalability,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 13, 'page_label': '14'}, page_content='workloads with predictable concurrency patterns and stringent consistency requirements, NoSQL \\ndatabases offer a compelling alternative for e -commerce pla tforms prioritizing scalability, \\nflexibility, and low -latency performance in dynamic, high -concurrency environments.  In \\nconclusion, the results of this study shed light on the critical role of database technology in shaping \\nthe performance and scalability of e -commerce platforms. By understanding the strengths and \\nlimitations of SQL and NoSQL databases and their implications for transaction processing and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n68 | P a g e  \\n \\nuser experience, e-commerce businesses can make informed decisions to optimize their database \\narchitecture and drive competitive advantage in the digital marketplace. \\nReferences: \\n1. Gadde, S. S., & Kalli, V. D. R. (2020). Descriptive analysis of machine learning and its \\napplication in healthcare. Int J Comp Sci Trends Technol, 8(2), 189-196. \\n2. Bommu, R. (2022). Advancements in Medical Device Software: A Comprehensive Review \\nof Emerging Technologies and Future Trends.  Journal of Engineering and \\nTechnology, 4(2), 1-8. \\n3. Gadde, S. S., & Kalli, V. D. (2021). The Resemblance of Library and Information Science \\nwith Medic al Science.  International Journal for Research in Applied Science & \\nEngineering Technology, 11(9), 323-327. \\n4. Gadde, S. S., & Kalli, V. D. R. (2020). Technology Engineering for Medical Devices -A'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='Engineering Technology, 11(9), 323-327. \\n4. Gadde, S. S., & Kalli, V. D. R. (2020). Technology Engineering for Medical Devices -A \\nLean Manufacturing Plant Viewpoint. Technology, 9(4). \\n5. Bommu, R.  (2022). Advancements in Healthcare Information Technology: A \\nComprehensive Review. Innovative Computer Sciences Journal, 8(1), 1-7. \\n6. Gadde, S. S., & Kalli, V. D. R. (2020). Medical Device Qualification Use.  International \\nJournal of Advanced Research in Computer and Communication Engineering, 9(4), 50-55. \\n7. Bommu, R. (2022). Ethical Considerations in the Development and Deployment of AI -\\npowered Medical Device Software: Balancing Innovation with Patient Welfare. Journal of \\nInnovative Technologies, 5(1), 1-7. \\n8. Gadde, S. S., & Kalli, V. D. R. (2020). Artificial Intelligence To Detect Heart Rate \\nVariability. International Journal of Engineering Trends and Applications, 7(3), 6-10. \\n9. Brian, K., & Bommu, R. (2022). Revolutionizing Healthcare IT through AI and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 14, 'page_label': '15'}, page_content='Variability. International Journal of Engineering Trends and Applications, 7(3), 6-10. \\n9. Brian, K., & Bommu, R. (2022). Revolutionizing Healthcare IT through AI and \\nMicrofluidics: From Drug Screening to Precision Livestock Farming. Unique Endeavor in \\nBusiness & Social Sciences, 1(1), 84-99. \\n10. Gadde, S. S., & Kalli, V. D. R. (2020). Applications of Artificial Intelligence in Medical \\nDevices and Healthcare.  International Journal  of Computer Science Trends and \\nTechnology, 8, 182-188.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='International Journal of Advanced Engineering Technologies and Innovations  \\nVolume 01 Issue 02 (2022)  \\n \\n69 | P a g e  \\n \\n11. Brandon, L., & Bommu, R. (2022). Smart Agriculture Meets Healthcare: Exploring AI -\\nDriven Solutions for Plant Pathogen Detection and Livestock Wellness \\nMonitoring. Unique Endeavor in Business & Social Sciences, 1(1), 100-115. \\n12. Gadde, S. S., & Kalli, V. D. (2021). Artificial Intelligence at Healthcare \\nIndustry. International Journal for Research in Applied Science & Engineering \\nTechnology (IJRASET), 9(2), 313. \\n13. Thunki, P., Reddy, S. R. B., Raparthi, M., Ma ruthi, S., Dodda, S. B., & Ravichandran, P. \\n(2021). Explainable AI in Data Science -Enhancing Model Interpretability and \\nTransparency. African Journal of Artificial Intelligence and Sustainable \\nDevelopment, 1(1), 1-8. \\n14. Gadde, S. S., & Kalli, V. D. An Innovative Study on Artificial Intelligence and Robotics.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='Transparency. African Journal of Artificial Intelligence and Sustainable \\nDevelopment, 1(1), 1-8. \\n14. Gadde, S. S., & Kalli, V. D. An Innovative Study on Artificial Intelligence and Robotics. \\n15. Gadde, S. S., & Kalli, V. D. (2021). Artificial Intelligence and its Models.  International \\nJournal for Research in Applied Science & Engineering Technology, 9(11), 315-318. \\n16. Raparthi, M., Dodda, S. B., Redd y, S. R. B., Thunki, P., Maruthi, S., & Ravichandran, P. \\n(2021). Advancements in Natural Language Processing -A Comprehensive Review of AI \\nTechniques. Journal of Bioinformatics and Artificial Intelligence, 1(1), 1-10. \\n17. Gadde, S. S., & Kalli, V. D. R. A Qualitative Comparison of Techniques for Student \\nModelling in Intelligent Tutoring Systems. \\n18. Raparthi, M., Maruthi, S., Reddy, S. R. B., Thunki, P., Ravichandran, P., & Dodda, S. B. \\n(2022). Data Science in Healthcare Leveraging AI for Predictive Analytics a nd'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2019', 'creator': 'Microsoft® Word 2019', 'creationdate': '2024-05-27T16:27:19+05:00', 'author': 'Mujahid', 'moddate': '2024-05-27T16:27:19+05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[data_analysis]_SQL_vs_NoSQL_Choosing_the_Right_Database_for_Your_Ecommerce_Platform.pdf', 'total_pages': 16, 'page': 15, 'page_label': '16'}, page_content='18. Raparthi, M., Maruthi, S., Reddy, S. R. B., Thunki, P., Ravichandran, P., & Dodda, S. B. \\n(2022). Data Science in Healthcare Leveraging AI for Predictive Analytics a nd \\nPersonalized Patient Care. Journal of AI in Healthcare and Medicine, 2(2), 1-11. \\n19. Gadde, S. S., & Kalli, V. D. Artificial Intelligence, Smart Contract, and Islamic Finance. \\n20. Kalli, V. D. R. (2022). Human Factors Engineering in Medical Device Software Desi gn: \\nEnhancing Usability and Patient Safety. Innovative Engineering Sciences Journal, 8(1), 1-\\n7. \\n21. Kalli, V. D. R. (2022). Improving Healthcare Delivery through Innovative Information \\nTechnology Solutions. MZ Computing Journal, 3(1), 1-6.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='An Empirical Study on the Design and\\nEvolution of NoSQL Database Schemas\\nStefanie Scherzinger1 and Sebastian Sidortschuck2\\n1 OTH Regensburg, Regensburg, Germany\\nstefanie.scherzinger@oth-regensburg.de\\n2 OTH Regensburg, Regensburg, Germany\\n& SPARETECH.io, Stuttgart, Germany\\nsebastian.sidortschuck@sparetech.io\\nAbstract. We study how software engineers design and evolve their\\ndomain model when building applications against NoSQL data stores.\\nSpeciﬁcally, we target Java projects that use object-NoSQL mappers to\\ninterface with schema-free NoSQL data stores. Given the source code\\nof ten real-world database applications, we extract the implicit NoSQL\\ndatabase schema. We capture the sizes of the schemas, and investigate\\nwhether the schema is denormalized, as is recommended practice in data\\nmodeling for NoSQL data stores. Further, we analyze the entire project\\nhistory, and with it, the evolution history of the NoSQL database schema.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='modeling for NoSQL data stores. Further, we analyze the entire project\\nhistory, and with it, the evolution history of the NoSQL database schema.\\nIn doing so, we conduct the so far largest empirical study on NoSQL\\nschema design and evolution.\\nKeywords: Schema Evolution · NoSQL Databases · Empirical Study.\\n1 Introduction\\nSchema-ﬂexible NoSQL data stores have become popular backends for building\\ndatabase applications. Systems like MongoDB allow for ﬂexible changes to the\\ndomain model during application development. In particular, they have proven\\nthemselves in settings where applications are frequently deployed to their pro-\\nduction environment, e.g., when web applications are built in an agile approach.\\nWhile the data stores do not enforce a global schema, the application code\\ngenerally assumes that persisted entities adhere to a certain (if loose) domain\\nmodel. Given that schema-ﬂexibility is one of the major selling points of NoSQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 0, 'page_label': '1'}, page_content='generally assumes that persisted entities adhere to a certain (if loose) domain\\nmodel. Given that schema-ﬂexibility is one of the major selling points of NoSQL\\ndata stores, this raises the question how the domain model, and thereby the\\nimplied NoSQL database schema , actually evolves. We empirically study the\\ndynamics of NoSQL database schema evolution. Further, we investigate the\\nquestion whether the NoSQL database schema is denormalized, as commonly\\nrecommended in literature, e.g. [15].\\nUnfortunately, real-world data dumps of NoSQL data stores are hard to come\\nby. We therefore resort to analyzing the source code of applications hosted on\\nGitHub. We focus on the relevant software stack shown in Figure 1a, namely Java\\napplications that use an object-NoSQL mapper to store data in either Google\\narXiv:2003.00054v1  [cs.DB]  28 Feb 2020'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='2 Scherzinger and Sidortschuck\\nCloud Datastore3 or MongoDB4, both popular and mature data stores. Among\\nover 1.2K open source GitHub repositories with this stack, we have identiﬁed\\nthe ten projects with the largest NoSQL schemas (a notion introduced shortly).\\nPrevious studies on schema evolution have primarily focused on schema-full,\\nrelational databases [5,11,13,18,20,25]. About NoSQL schema evolution in real-\\nworld applications, little is known that is based on systematic, empirical studies\\n(versus anecdotal evidence): Earlier studies have a diﬀerent focus (such as the\\nusage of certain mapper features [14]), or analyze a single project (c.f. [12]).\\nIn this paper, we introduce our notion of the NoSQL database schema,\\nwhich is implicit in object mapper class declarations, even though the under-\\nlying NoSQL data stores are schema-free. In this setting, this paper makes the\\nfollowing contributions:\\n– We formulate three research questions, namely (RQ1) whether the NoSQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='lying NoSQL data stores are schema-free. In this setting, this paper makes the\\nfollowing contributions:\\n– We formulate three research questions, namely (RQ1) whether the NoSQL\\ndatabase schema is denormalized (as recommended in literature), (RQ2)\\nwhich growth in complexity we can observe in NoSQL database schemas\\nover the project development time, and (RQ3) how the NoSQL database\\nschema evolves, thereby identifying the common changes.\\n– We analyze the ten projects with the largest NoSQL database schemas\\namong over 1.2K candidate projects, based on static code analysis and the\\ncommit history. We are able to conﬁrm that denormalization is common in\\nNoSQL database schemas. We are further able to show evidence of evolu-\\ntionary changes to the NoSQL database schema in all analyzed projects.\\n– We discuss our ﬁndings w.r.t. related studies on relational schema evolution\\nand ﬁnd that the churn rate of NoSQL schemas is comparatively high.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='– We discuss our ﬁndings w.r.t. related studies on relational schema evolution\\nand ﬁnd that the churn rate of NoSQL schemas is comparatively high.\\nStructure. Next, we introduce preliminaries in Section 2. In Section 3, we describe\\nour methodology, and state our research questions. In Section 4, we present the\\nresults of our study, which we then discuss in Section 5. We point out threats to\\nthe validity of our results in Section 6, and give an overview over related work\\nin Section 7. We conclude with an outlook on future work.\\n2 Preliminaries\\nWe next introduce the software stack studied, as well as our terminology.\\nPhysical entities. We consider two popular NoSQL data stores: Google Cloud\\nDatastore (called Datastore hereafter) is commercial and hosted on the Google\\nCloud Platform, MongoDB is open source. Both data stores are schema-free\\n(however, MongoDB oﬀersoptional schema validation). Both manage document-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 1, 'page_label': '2'}, page_content='Cloud Platform, MongoDB is open source. Both data stores are schema-free\\n(however, MongoDB oﬀersoptional schema validation). Both manage document-\\nlike data, which we refer to as the (physical) entities. On an abstract level, an\\nentity is a collection of key-value pairs, or properties. Entities may be nested\\nand properties may be multi-valued. We sketch a Datastore entity representing\\na player and his or her missions in a role playing game in Figure 1a, in (simpliﬁed)\\nJSON notation, to abstract away from system-proprietary storage formats.\\n3 https://cloud.google.com/datastore/, available since 2009.\\n4 https://www.mongodb.com/, available since 2019'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='Design and Evolution of NoSQL Database Schemas 3\\n(a) The software stack.\\n (b) Code changes to class Player.\\nFig. 1. (a) The object-NoSQL mapper separates the domain model from the NoSQL\\ndata store (adapted from [6]). (b) Not all code changes are actually schema-relevant.\\nDomain models. In principle, each entity in a schema-free data store may have\\nits very own, unique structure. However, in database applications, it is safe\\nto assume that the software engineers have agreed on some domain model , as\\nsketched in Figure 1a. In our setting, the domain model is captured by Java class\\ndeclarations, yet in the Figure, we use the more compact UML notation. (For\\nnow, we ignore the @-labeled annotations.) Class Player declares attributes for\\nan identiﬁer, a name, an amount of credits, and a list of missions. Each mission\\nalso has an identiﬁer, a title, a level of diﬃculty, and tracks its completion.\\nObject-NoSQL Mappers. Object mappers are state-of-the-art in building data-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='also has an identiﬁer, a title, a level of diﬃculty, and tracks its completion.\\nObject-NoSQL Mappers. Object mappers are state-of-the-art in building data-\\nbase applications [6]. Like object-relational mappers, the object-NoSQL map-\\npers Objectify 5 and Morphia6 map Java objects to entities. Objectify is tied to\\nDatastore, and Morphia to MongoDB. With object-NoSQL mappers, develop-\\ners merely specify their domain model as Java classes that are annotated with\\nthe keyword @Entity. Each entity-class has a unique key (annotated with @Id).\\nThe object mapper provides methods for saving and loading: In Figure 1a, the\\nclass name and the identifying attribute are mapped to the designated proper-\\nties _kind and _id. Objectify maps the player’s list of missions to an array of\\nnested entities. Yet at application runtime, an entity-class declaration may not\\nmatch the structure of all persisted entities, as discussed next.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 2, 'page_label': '3'}, page_content='nested entities. Yet at application runtime, an entity-class declaration may not\\nmatch the structure of all persisted entities, as discussed next.\\nLazy data migration. The data store may also store legacy versions of entities.\\nFigure 1b shows a new version of entity-class Player, with changes due to new\\nrequirements in the software development project. Attribute coins has replaced\\ncredits. Merely changing the entity-class in the application code does not af-\\nfect any existing entities. Instead, persisted entities are only migrated lazily,\\n5 https://github.com/objectify/objectify\\n6 https://github.com/MorphiaOrg/morphia'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='4 Scherzinger and Sidortschuck\\nupon loading: The new version of entity-class Player in Figure 1b is backwards-\\ncompatible with Figure 1a. Once the legacy entity for Frodo has been loaded, the\\ncorresponding Java object will have an attributecoins, as annotation @AlsoLoad\\nlazily renames attributes.\\nThus, to obtain a summary of the structural variety of physical entities in\\nthe data store (based on code analysis alone, not having access to the data store\\ncontents itself), we need to consider the entire evolution history of entity-classes.\\nNoSQL database schema evolution. We base our notion of the NoSQL database\\nschema (or shorter, NoSQL schema) on the domain model. This idea of treating\\nentity-classes as schema declarations is re-current in literature, c.f. [4, 17]. Note\\nthat not all Java attributes are relevant for the NoSQL database schema: At-\\ntributes that are transient, e.g., carrying Objectify annotation @Ignore, are not'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='that not all Java attributes are relevant for the NoSQL database schema: At-\\ntributes that are transient, e.g., carrying Objectify annotation @Ignore, are not\\nschema-relevant: The value of hoursSinceLastLogin in Figure 1b is not per-\\nsisted (it may be derived from lastLogin). Also, class methods are not schema-\\nrelevant. Thus, code changes that only aﬀect transient attributes or class meth-\\nods are part of software evolution, but not of schema evolution. Therefore, they\\nare not considered schema changes by us.\\nDenormalized entity-classes. The recommendation in working with Datastore\\nand MongoDB is to intentionally denormalize the schema. 7 This can be done\\nby either nesting entities, or by using multi-valued properties, such as the array\\nof Missions in Figure 5. There are various motivations for denormalization, one\\nbeing that traditionally, the query languages do not provide a join operator\\n(such is still the case in Google Cloud Datastore, and this also used to be the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='being that traditionally, the query languages do not provide a join operator\\n(such is still the case in Google Cloud Datastore, and this also used to be the\\ncase for MongoDB), so joined data is materialized in the data store. Another\\nreason is that transactions between an arbitrary number of entities may not\\nbe supported 8. Consequently, transactionally safe updates are often realized by\\nupdates to a single, aggregate entity.\\nIn the following, we say an entity-class is denormalized if it does not declare\\nﬂat, relational-style tuples in ﬁrst normal form, i.e., with atomic attribute values\\nonly. So unless all schema-relevant attributes have Java primitive types (such as\\nInteger, String, Boolean, . . . ), we say the entity-class is denormalized. As we\\ndiscuss in Section 3.3, this is a practical yet conservative approach.\\nAs an example, the entity-class declarations for players, sketched in Figure 1,\\nis denormalized, due to the multi-valued attribute listOfMissions.\\n3 Methodology'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 3, 'page_label': '4'}, page_content='As an example, the entity-class declarations for players, sketched in Figure 1,\\nis denormalized, due to the multi-valued attribute listOfMissions.\\n3 Methodology\\nIn the following, we describe our methodology, such as the context of our analysis,\\nthe research questions, and the analysis process. While our outline has strong\\nanalogies to Qiu et al. [13] and their analysis of relational schema evolution, our\\n7 E.g. “6 Rules of Thumb for MongoDB Schema Design” at https://www.mongodb.\\ncom/blog/post/6-rules-of-thumb-for-mongodb-schema-design-part-2, June 2015.\\n8 We point to the concepts such of entity groups and cross-group transactions in the\\nclassic Google Cloud Datastore [16], which is in the process of being deprecated.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='Design and Evolution of NoSQL Database Schemas 5\\nT able 1. Characteristics of the studied database applications.\\nProject Life Cycle # Commits # Entity-\\nclasses LoC (K)\\nObjectify\\nCryptonomica/cryptonomica 04/16 ∼ 09/18 185 0 ∼ 29 0 ∼ 526\\nFraunhoferCESE/madcap 12/14 ∼ 03/18 853 0 ∼ 82 0 ∼ 17\\ngoogle/nomulus 03/16 ∼ 09/18 2,025 51 ∼ 55 138 ∼ 224\\nnareshPokhriyal86/testing 01/15 ∼ 02/15 25 0 ∼ 79 0 ∼ 449\\nNekorp/Tikal-Technology 04/15 ∼ 11/15 59 0 ∼ 43 0 ∼ 49\\nMorphia\\naltiplanogao/tallyframework 06/15 ∼ 06/16 167 0 ∼ 24 0 ∼ 5\\nbujilvxing/QinShihuang 10/16 ∼ 12/16 154 0 ∼ 36 0 ∼ 21\\ncatedrasaes-umu/NoSQLDataEngineering 11/16 ∼ 09/18 711 0 ∼ 28 0 ∼ 280\\nGBPeters/PubInt 10/16 ∼ 02/18 69 0 ∼ 27 0 ∼ 5\\nMKLab-ITI/simmo 07/14 ∼ 02/17 142 0 ∼ 51 0 ∼ 5\\nprocess is rather diﬀerent: we cannot analyze schemas declared in a declarative\\ndata deﬁnition language, such as SQL. Rather, we need to parse raw Java code.\\n3.1 Context\\nWe used BigQuery9 to identify relevant open source repositories on GitHub, as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='data deﬁnition language, such as SQL. Rather, we need to parse raw Java code.\\n3.1 Context\\nWe used BigQuery9 to identify relevant open source repositories on GitHub, as\\nof September 4th, 2018. We consider a repository (which we synonymously refer\\nto as a project) relevant if it contains Java import statements for Objectify or\\nMorphia. We cloned over 1.2K candidate repositories and excluded any reposi-\\ntories that (1) have fewer than 20 commits (to exclude tinker projects), (2) are\\nthe Morphia or Objectify source code (or forks thereof), (3) or are ﬂagged as\\nforks from repositories already covered, with no schema-relevant code changes\\nafter the fork. We analyze the project history using git log 10. This allows us\\nto re-trace the development history of all entity classes. We parse and aggregate\\nthe log output using Python scripts.\\nAmong all projects analyzed, we determined the maximum number of entity-\\nclasses throughout the project history, and settled on the top-5 projects for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='the log output using Python scripts.\\nAmong all projects analyzed, we determined the maximum number of entity-\\nclasses throughout the project history, and settled on the top-5 projects for\\nObjectify and Morphia respectively. Table 1 lists these projects with their life\\ncycles up to the latest commit at the time of our analysis. We also state the total\\nnumber of commits at the time. We state the minimum and maximum number\\nof entity-classes throughout the project history, as well as the total number of\\nlines of code between the ﬁrst and last analyzed commit (measured with cloc11\\nand reported in thousands).\\n9 Google BigQuery is a commercial cloud service. This data warehousing tool allows\\nfor querying the GitHub open data collection, mostly non-forked projects with an\\nopen source license: https://cloud.google.com/bigquery/.\\n10 We state the exact command pattern for reproducability: git log\\n--before=2018-09-04T00:00:00 --cherry-pick --date-order --pretty=format:\"%H;%aI;%cI;%P\" .'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 4, 'page_label': '5'}, page_content='10 We state the exact command pattern for reproducability: git log\\n--before=2018-09-04T00:00:00 --cherry-pick --date-order --pretty=format:\"%H;%aI;%cI;%P\" .\\n11 https://github.com/AlDanial/cloc'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='6 Scherzinger and Sidortschuck\\n3.2 Research Questions\\nRQ1: Are NoSQL schemas denormalized? We analyze the structure of entity-\\nclasses, whether they map to ﬂat tuples in ﬁrst normal form, or whether they\\nrepresent denormalized data.\\nRQ2: What is the growth in complexity of the NoSQL schema? We cap-\\nture schema complexity based on metrics recognized in literature.\\nRQ3: How does the NoSQL schema evolve? We automatically identify and\\nclassify evolutionary changes to the NoSQL schema.\\n3.3 Analysis Process\\nLocating entity-classes. We replay the commit histories and use the Java parser\\nQDox12 to parse class declarations. We identify entity-classes by the object map-\\nper annotation @Entity, which may also be inherited. 13\\nDenormalization. To determine whether an entity-class is denormalized, we parse\\nits Java declaration and strip away attributes that are not relevant to the NoSQL\\nschema. We then analyze the types of the remaining attributes. Unless all have'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='its Java declaration and strip away attributes that are not relevant to the NoSQL\\nschema. We then analyze the types of the remaining attributes. Unless all have\\nprimitive types (such as Integer, String, or Boolean), we assume that the\\nentity-class is denormalized.\\nIn most cases, we correctly recognize denormalization: (1) if the entity class\\ndeclaration contains container classes (e.g., a Java Collection), and therefore\\nan attribute is multi-valued. (2) Equally, the entity-class may contain nested\\nentity classes, giving it a hierarchical structure.\\nHowever, there are also cases where this approach is a conservative sim-\\npliﬁcation, and we might falsely categorize an entity-class as denormalized: an\\nattribute type may be declared in a third-party library, which is inaccessible\\nto us (see also our discussion in Section 6). Also, an attribute type may be a\\ncustom type that the developers declared. To realize that a custom type is just a'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='to us (see also our discussion in Section 6). Also, an attribute type may be a\\ncustom type that the developers declared. To realize that a custom type is just a\\nwrapper for a basic Java type, we would have to run more involved code analysis.\\nYet typically, polymorphic types are involved, and we are confronted with the\\ninherent limitations of static code analysis.\\nIdentifying schema changes. We identify commits with schema-relevant changes\\nby comparing succeeding versions of the application source code: We register\\nwhen (1) a new entity-class is added or an entity-class is removed, (2) a schema-\\nrelevant attribute is added or removed in an entity-class declaration, and (3) fur-\\nther, changes to schema-relevant attributes, such as to their types, default ini-\\ntializations, or even object mapper annotations. We only focus on changes which\\nwe can recognize programmatically. Recognizing renaming or splitting an entity-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 5, 'page_label': '6'}, page_content='tializations, or even object mapper annotations. We only focus on changes which\\nwe can recognize programmatically. Recognizing renaming or splitting an entity-\\nclass, or renaming an attribute, are instances of the challenge of schema matching\\nand mapping [2], and cannot be fully automated.\\n12 https://github.com/paul-hammant/qdox\\n13 In earlier versions of the mapper libraries, this annotation was only optional, so it\\ncannot be relied upon. We therefore also search for the mandatory annotation @Id,\\nand thus reliably detect polymorphic entity-classes. (c.f. Section 6).'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='Design and Evolution of NoSQL Database Schemas 7\\nCryptonomica/\\ncryptonomica\\nFraunhoferCESE/\\nmadcap\\ngoogle/\\nnomulus\\nnareshPokhriyal86/\\ntesting\\nNekorp/\\nTikal-Technology\\n(a) Objectify-based projects.\\naltiplanogao/\\ntallyframework\\nbujilvxing/\\nQinShihuang\\ncatedrasaes-umu/\\nNoSQLDataEngineering\\nGBPeters/\\nPubInt\\nMKLab-ITI/\\nsimmo\\nDenormalized entity-class\\nOther\\n(b) Morphia-based projects.\\nFig. 2. Visualization of denormalized NoSQL database schemas.\\n4 Results of the Study\\n4.1 RQ1: Are NoSQL schemas denormalized?\\nWe analyze the entity-class declarations in their most current version w.r.t. de-\\nnormalization. The results are visualized in Figure 2. For each analyzed project,\\nwe show a dot matrix chart. The number of dots represents the number of entity-\\nclasses. The brighter (orange) dots represent the entity-class declarations which\\nwe must assume to be denormalized, due to the limits of static code analysis.\\nThe darker (blue) dots represent the other entity-classes.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='we must assume to be denormalized, due to the limits of static code analysis.\\nThe darker (blue) dots represent the other entity-classes.\\nNotably, each project contains at least one denormalized entity-class, so all\\nschemas are denormalized. With the exception of two Objectify-based projects,\\ndenormalized entity-classes dominate the NoSQL database schemas. There are\\neven two Morphia-based projects where all entity-classes are denormalized.\\nResults. We ﬁnd that each project analyzed has denormalized entity-classes in its\\nNoSQL schema. This shows that developers make active use of denormalization.\\nHowever, without qualitative studies based on developer surveys, we do not\\nknow whether (1) the developers consciously chose a database which allows for\\na denormalized database schema, as this better suits their conceptual model.\\nHowever, it could also be that (2) they are actually forced denormalize their\\ndata model, due to the technological limitations of NoSQL data stores (brieﬂy'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 6, 'page_label': '7'}, page_content='However, it could also be that (2) they are actually forced denormalize their\\ndata model, due to the technological limitations of NoSQL data stores (brieﬂy\\ndiscussed in Section 2).\\n4.2 RQ2: What is the growth in complexity of the NoSQL schema?\\nIn empirical studies on relational schema evolution, the number of tables is\\nconsidered a simple approximation for schema complexity [7]. Accordingly, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='8 Scherzinger and Sidortschuck\\n0% 20% 40% 60% 80% 100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\nCryptonomica/cryptonomica\\n0% 20% 40% 60% 80% 100%\\nFraunhoferCESE/madcap\\n0% 20% 40% 60% 80% 100%\\ngoogle/nomulus\\n0% 20% 40% 60% 80% 100%\\nnareshPokhriyal86/testing\\n0% 20% 40% 60% 80% 100%\\nNekorp/Tikal-Technology\\n# Entity-classes Schema-LoC\\n(a) Objectify-based projects.\\n0% 20% 40% 60% 80% 100%\\n0%\\n20%\\n40%\\n60%\\n80%\\n100%\\naltiplanogao/tallyframework\\n0% 20% 40% 60% 80% 100%\\nbujilvxing/QinShihuang\\n0% 20% 40% 60% 80% 100%\\ncatedrasaes-umu/NoSQLD...\\n0% 20% 40% 60% 80% 100%\\nGBPeters/PubInt\\n0% 20% 40% 60% 80% 100%\\nMKLab-ITI/simmo\\n(b) Morphia-based projects.\\nFig. 3. Evolution trend of entity classes. The horizontal axes show the project progress,\\nin percentage of commits analyzed. The vertical axes show the complexity of the schema\\nw.r.t. its maximum, for two alternative metrics. (Visualization modeled after [13].)\\ntrack the number of entity-classes over time in Figure 3 (based on a visualization'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='w.r.t. its maximum, for two alternative metrics. (Visualization modeled after [13].)\\ntrack the number of entity-classes over time in Figure 3 (based on a visualization\\nidea from [13]). For each project, one chart is shown. On the horizontal axis, we\\ntrack the progress of the project, measured as the percentage of git commits\\nanalyzed. For the madcap project, this is based on 853 commits (c.f. Table 1).\\nOn the vertical axis, we track the size of the NoSQL database schema using two\\nmetrics. One is the number of entity classes (blue solid line). This metric is also\\nnormalized w.r.t. its maximum throughout the project history. So for madcap,\\nthe 100% peak corresponds to 82 entity classes, some of which were removed in\\nthe later phase of the project.\\nThe second line denotes a “proxy metric” [7] for approximating the size of the\\nNoSQL schema, where we count the lines of code of the entity-classes (including\\nsuperclasses, excluding comments and empty lines), and thereby compute the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='NoSQL schema, where we count the lines of code of the entity-classes (including\\nsuperclasses, excluding comments and empty lines), and thereby compute the\\nSchema-LoC.14 There is shrinkage, yet overall, schema complexity increases.\\nResults. 1) As in the study by Qiu et al. on relational software evolution [13], we\\ncan conﬁrm that while the projects diﬀer in their life-spans and commit activity,\\nin nearly all projects, the NoSQL schema grows over time. However, there may be\\nphases of refactoring, leading to dips in the curves. 2) Apparently, Schema-LoC\\nlends itself nicely as a proxy-metric, and we obtain high correlation coeﬃcients\\n14 We ﬁnd this proxy-metric preferable over counting (schema-relevant) attributes, as\\nis common in studies on relational schema evolution: (1) Entity-classes with more\\nschema-relevant attributes have more lines of code accordingly. (2) In static code\\nanalysis, we cannot reliably count nested attributes: Abstract container classes and'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 7, 'page_label': '8'}, page_content='schema-relevant attributes have more lines of code accordingly. (2) In static code\\nanalysis, we cannot reliably count nested attributes: Abstract container classes and\\nthe use of polymorphism in general, make it impossible to know the number and na-\\nture of nested attributes at compile time. With Schema-LoC, we are able to abstract\\nfrom this issue.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='Design and Evolution of NoSQL Database Schemas 9\\nFig. 4. Visualizing relative schema sizes and churn. Each rectangle represents an entity-\\nclass, its area proportional to its size in lines of code (speciﬁcally Schema-LoC). The\\nhue represents the relative frequency of schema changes within the same project.\\nwhen comparing to the number of entity classes. As Schema-LoC depends on\\nthe number of attributes in an entity class, we can retrace an eﬀect reported\\nin [13], namely that entity-classes and their attributes (corresponding to tables\\nand columns) have largely analogous dynamics. 4) In general, the schema grows\\nmore than it shrinks. This is in line with studies on relational schema evolution.\\n5) One observation in [13] was that the schema stabilizes early: There, for 7 out\\nof 10 projects, 60% of the maximum number of tables is reached in the ﬁrst 20%\\nof the commits. Interestingly, in our study, the number of entity-classes reaches'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='of 10 projects, 60% of the maximum number of tables is reached in the ﬁrst 20%\\nof the commits. Interestingly, in our study, the number of entity-classes reaches\\nthe 60% in only 4 projects. 6) In [13], less than 2% of all commits contain valid\\nschema changes (across all ten projects analyzed there). In our study, the share\\nof commits with schema-relevant changes is between 2.8% and over 30%, with 4\\nprojects reaching over 20%. Clearly, we observe higher churn rates.\\n4.3 RQ3: How does the NoSQL schema evolve?\\nWe ﬁrst investigate how often entity-classes undergo schema changes when com-\\npared to others inside the same project, and how large they are in terms of our\\nproxy metric Schema-LoC. Figure 4 visualizes the entity classes making up the\\nten NoSQL schemas as a tree map. This ﬁgure is best viewed in color. Each\\ncolored area represents one project. Inside, each rectangle represents one entity-\\nclass, the area proportional to its Schema-LoC. Darker hue indicates that an'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 8, 'page_label': '9'}, page_content='colored area represents one project. Inside, each rectangle represents one entity-\\nclass, the area proportional to its Schema-LoC. Darker hue indicates that an\\nentity-class has undergone more schema changes than the other entity-classes\\nin the same project. For instance, for nomulus, the darkest area represents 12\\nschema changes against the same entity-class. Thus, some entity-classes change\\nquite more often than others. However, there are also projects where schema\\nchanges aﬀect entity-classes quite uniformly.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='10 Scherzinger and Sidortschuck\\n0% 20% 40% 60% 80% 100%\\nNekorp/Tikal-Technology\\nnareshPokhriyal86/testing\\ngoogle/nomulus\\nFraunhoferCESE/madcap\\nCryptonomica/cryptonomica\\nObjectify\\nAdd entity-class\\nRemove entity-class\\nAdd schema-relevant attribute\\nRemove schema-relevant attribute\\nChange schema-relevant attribute\\n0% 20% 40% 60% 80% 100%\\nMKLab-ITI/simmo\\nGBPeters/PubInt\\ncatedrasaes-umu/NoSQLD...\\nbujilvxing/QinShihuang\\naltiplanogao/tallyframework\\nMorphia\\n(a) By project.\\nEntity-class Schema-relevant attribute\\nadd remove add remove change\\nObjectify 56.3% 8.4% 24.7% 4.0% 6.5%\\nMorphia 34.5% 16.2% 21.6% 10.4% 17.2%\\nOverall 34.8% 15.1% 27.3% 7.4% 15.4 %\\n(b) Objectify-based vs. Morphia-based projects.\\nProject Type Initialization Annotations\\nObjectify\\nCryptonomica/cryptonomica 2 0 3\\nFraunhoferCESE/madcap 9 0 6\\ngoogle/nomulus 11 2 58\\nnareshPokhriyal86/testing 0 0 0\\nNekorp/Tikal-Technology 0 0 3\\nMorphia\\naltiplanogao/tallyframework 7 3 54\\nbujilvxing/QinShihuang 32 33 15'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='FraunhoferCESE/madcap 9 0 6\\ngoogle/nomulus 11 2 58\\nnareshPokhriyal86/testing 0 0 0\\nNekorp/Tikal-Technology 0 0 3\\nMorphia\\naltiplanogao/tallyframework 7 3 54\\nbujilvxing/QinShihuang 32 33 15\\ncatedrasaes-umu/NoSQLDataEngineering 43 0 13\\nGBPeters/PubInt 0 2 2\\nMKLab-ITI/simmo 7 5 18\\n(c) Drill-down into the remaining changes to schema-relevant attributes.\\nFig. 5. Distinguishing diﬀerent kinds of schema changes: (a) and (b): Relative shares of\\nschema changes (by project and by mapper library). (c) Zooming in on the remaining\\nchanges in schema-relevant attributes mentioned in (a), showing absolute values.\\nIn Figure 5, we capture the distribution of schema changes according to\\nthe kind of change. In Subﬁgure 5a (after Qiu et al. in [13]), we break down\\nthe distribution of changes by project. Note that when a new entity-class is\\nadded, we do not count this as adding attributes at the same time. Notably, the\\ndistributions are project-speciﬁc. We now discuss two projects that stand out.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 9, 'page_label': '10'}, page_content='added, we do not count this as adding attributes at the same time. Notably, the\\ndistributions are project-speciﬁc. We now discuss two projects that stand out.\\nIn the fourth Objectify-based project, adding an entity-class makes up for\\nnearly all changes. Considering the project characteristics in Table 1 reveals that'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='Design and Evolution of NoSQL Database Schemas 11\\nthis project is an outlier in several regards: With only 25 commits, it has barely\\nmade the bar for being considered in our analysis (see Section 3.1). At the same\\ntime, this project holds the second largest number of entity-classes in any project\\nconsidered in this analysis. Since the life cycle considered is only two months,\\nthis project is in a very early stage of development at the time of this analysis.\\nThus, it seems plausible that at this early phase, the developers kick start their\\ndata model by declaring the entity classes in bulk.\\nIn contrast, the second Morphia-based project stands out as the project with\\nthe least share of entity class additions. Since the git commit messages are in\\nChinese (which the authors of this paper do not master), we ﬁnd it diﬃcult to\\nretrace the developers’ motivation. What is noticeable in Subﬁgure 3b is that\\nwhile the number of entity classes increases in less than 10 distinct steps, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='retrace the developers’ motivation. What is noticeable in Subﬁgure 3b is that\\nwhile the number of entity classes increases in less than 10 distinct steps, the\\nproxy metric Schema-LoC changes in more ﬁne-granular steps. Thus, the entity-\\nclasses undergo more frequent changes. This matches the distribution plotted,\\nas then the share of entity-class creations is smaller by comparison. Subﬁgure 5b\\nsummarizes Subﬁgure 5a, and aggregates the changes by the mapper library.\\nWhile we see project-speciﬁc ﬂuctuations, when we group by mapper library, we\\nalso observe diﬀerences in the distribution. Overall, additions (whether of entity\\nclasses or of schema-relevant attributes) dominate.\\nIn Table 5c, we break down the schema-relevant attribute changes listed in\\nSubﬁgures 5a and 5b: (a) For some projects, types change. (b) For others, the\\ninitialization changes. A drill-down reveals that (as may be expected) adding\\nan initial value is the most frequent change, followed by changing the initializa-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='initialization changes. A drill-down reveals that (as may be expected) adding\\nan initial value is the most frequent change, followed by changing the initializa-\\ntion value. (c) In other cases, mapper annotations that aﬀect the schema are\\nadded or removed. The most frequent annotations added are @PersistField\\nand @Reference. The ﬁrst is from a third-party framework. Since it is schema-\\nrelevant, we report it. The second supports referential constraints. Sporadically,\\nthird-party annotations are added to declare additional constraints, such as@Min.\\nResults. 1) We can conﬁrm the observations from related work on relational\\nschema evolution that schema changes are generally not distributed uniformly [13,\\n24]. 2) As already observed for RQ2, the trend is that entity-classes are added\\nmore frequently than they are removed. We see a similar pattern for schema-\\nrelevant attributes, in line with studies on relational schema evolution. Over-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='more frequently than they are removed. We see a similar pattern for schema-\\nrelevant attributes, in line with studies on relational schema evolution. Over-\\nall, in 9 out of 10 projects, additions collectively account for more than 50%\\nof the changes. In 5 projects, they even account for over 70% of the changes.\\n3) While additions are generally more frequent, there are also projects where\\nremovals of entity classes occur to a non-signiﬁcant degree. Related work on\\nrelational schema evolution has shown that there are what the authors call sur-\\nvivor tables [22], whereas there are that are more short-lived. The observation\\nthat entity-class removals are very project speciﬁc has also been made in [13].\\n4) Among all annotation changes, only 15 concern referential constraints (an-\\nnotation @Reference). The authors of two relate studies on relational schema\\nevolution, both [13] and [21], have observed that changes concerning referential'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 10, 'page_label': '11'}, page_content='notation @Reference). The authors of two relate studies on relational schema\\nevolution, both [13] and [21], have observed that changes concerning referential\\nintegrity constraints are also rare in relational schema evolution. With NoSQL\\ndata stores, this is to be expected, as referential integrity is not supported to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='12 Scherzinger and Sidortschuck\\nthe same extent. 4) While Qiu et al. [13] found changes in attribute types to\\nbe the number one change for half of the projects analyzed (even outnumbering\\nadditions of either tables or columns), we do not see evidence of this eﬀect here.\\n5 Discussion\\nWe can reproduce the main results from related work on relational schema evo-\\nlution: There is strong evidence of NoSQL schema evolution, and additions are\\ndominant schema changes. However, we do not see the schema stabilizing in\\nthe early phases of all projects, which may partly be due to shorter project life\\nspans: The ten projects studied in [13] are PHP applications backed by rela-\\ntional databases, and have longer life cycles (two with ten years), more commits\\n(starting at nearly 5K), and more lines of code. This is to be expected with a\\nmuch older and thus more widely adopted stack.\\nStill, we do suspect that NoSQL developers evolve their schema more continu-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='much older and thus more widely adopted stack.\\nStill, we do suspect that NoSQL developers evolve their schema more continu-\\nously. One indicator supporting this hypothesis is that we see higherchurn rates,\\nso a larger share of the commits contains code changes that aﬀect the schema.\\nThis calls for further study. Due to this churn, making sure that entity-class dec-\\nlarations are “backwards” compatible with legacy entities, persisted by earlier\\nversions of the application code, may become an overwhelming task. There are\\nﬁrst proposals for assisting tools, e.g., by type-checking versions of entity-class\\ndeclarations [3]. Clearly, more research is needed on systematic tool support.\\nThe fact that denormalization is common shows that solutions for managing\\nrelational schema evolution, managing ﬂat tuples, will not transfer immediately.\\nRather, when devising frameworks, we may want to turn to related work on\\nframeworks for handling schema evolution in XML (e.g. [9]) or object-oriented'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='Rather, when devising frameworks, we may want to turn to related work on\\nframeworks for handling schema evolution in XML (e.g. [9]) or object-oriented\\ndatabases (e.g. [26]) for inspiration on what has shown to be feasible.\\n6 Threats to Validity\\nConstruct validity. (1) With applications using older versions of Objectify and\\nMorphia, we cannot rely on the @Entity-annotation to identify entity-classes, so\\nwe also consider the @Id annotation. To be conﬁdent that this does not lead to\\nfalse positives, we performed manual checks. (With Objectify, we cross-checked\\nwhich entity-classes were registered with ObjectifyService, a mandatory pro-\\ngramming step.) (2) In static analysis, we encounter a limitation with attribute\\ntypes from third-party libraries. Tracking down these libraries is out of scope\\n(and not even possible in all cases). Thus, there are attributes that are not fully\\ncaptured by Schema-LoC. Yet as this is a proxy-metric to start with, we con-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 11, 'page_label': '12'}, page_content='(and not even possible in all cases). Thus, there are attributes that are not fully\\ncaptured by Schema-LoC. Yet as this is a proxy-metric to start with, we con-\\nsider this threat acceptable. Third-party libraries also aﬀect the recognition of\\nentity-classes as denormalized. Having sampled and inspected the entity-class\\ndeclarations, we are conﬁdent that – given the limitations of static code analysis\\n– the risk of false positives is acceptable. (3) We treat each single commit as\\ncontributing to a new version of the schema. There are software development\\nteams that operate by continuous deployment, so tested code is immediately'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='Design and Evolution of NoSQL Database Schemas 13\\nand autonomously deployed to the production environment. There, in theory,\\neach commit containing a schema change comprises a new schema version. Yet\\nrather often, a release to production comprises more than one commit. Unfor-\\ntunately, we are not able to tell in static code analysis which commits where\\nreleased when. There are development teams that tag release commits, but this\\nis project-internal culture, and not consistently the practice across all ten stud-\\nied projects. Therefore, we must go by the simplifying assumption that each\\ncommit declares a new NoSQL database schema.\\nExternal validity. We next discuss threats in generalizing our results to other\\nsoftware stacks. (1) It would be desirable to search additional code repositories,\\nand extend to further NoSQL data stores, object mapper libraries, and program-\\nming languages. (2) Extending our analysis to projects that do not use object-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='and extend to further NoSQL data stores, object mapper libraries, and program-\\nming languages. (2) Extending our analysis to projects that do not use object-\\nmappers requires a diﬀerent kind of static code analysis, and was implemented\\nin a related study that involved a single MongoDB project [12]. At the same\\ntime, object mappers are state-of-the art in modern application development,\\nand by now, Objectify and Morphia are actually part of oﬃcial Datastore and\\nMongoDB tutorials (even though they started as independent projects). Thus,\\nwe do analyze a highly relevant stack. (3) There is the fundamental question\\nwhether studies on open source projects generalize to commercial projects.\\n7 Related Work\\nDatabase schema evolution is a timeless research area, with various proposals\\nhow to systematically manage schema changes. Providing tool support, however,\\nis not the scope of this paper. In the following discussion of related work, we'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='how to systematically manage schema changes. Providing tool support, however,\\nis not the scope of this paper. In the following discussion of related work, we\\ntherefore focus on empirical studies on schema evolution in open source projects.\\nIt is only natural that the availability of public code repositories has en-\\nabled empirical studies on relational schema evolution [5,11,13,18–20,22,23,25].\\nAmong their key ﬁndings, these studies show that the schema evolves. They\\nconﬁrm that adding tables or columns are frequent changes. In these settings,\\nthe schema is speciﬁed declaratively (usually in SQL). Accordingly, the term\\nschema modiﬁcation operations (SMOs) [5] does not transfer well to our stack.\\nRather than declarative DDL statements, we need to parse raw Java code: While\\nthe authors of [25] also parse application code, they do so to extract declarative\\nstatements embedded in code.\\nSo far, there are only few empirical studies on schema evolution in NoSQL'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 12, 'page_label': '13'}, page_content='the authors of [25] also parse application code, they do so to extract declarative\\nstatements embedded in code.\\nSo far, there are only few empirical studies on schema evolution in NoSQL\\ndata stores. Our work builds on an earlier analysis [14] on the adoption of map-\\nper annotations for lazy schema evolution, which is a diﬀerent focus. The au-\\nthors in [12] present an approach for identifying a schema evolution history in\\nMongoDB-based Java applications. Diﬀerent from us, the authors do not assume\\nthat an object-NoSQL mapper is used to access the data store. Rather, they an-\\nalyze direct calls to the MongoDB API. The schema derived is similar to our\\nnotion of the NoSQL schema, since it captures the perspective of the application\\ncode. The authors evaluated their approach for a single open source project,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='14 Scherzinger and Sidortschuck\\nwhereas our study has a broader basis, considering ten projects. Moreover, their\\ncontribution is to derive a visualization of the schema evolution history.\\nMeanwhile, there is a growing body of work on extracting schema descrip-\\ntions [1, 4, 8] from large collections of JSON data. While this a bottom-up ap-\\nproach, starting from the data, we proceed top-down, analyzing application code.\\nIn capturing schema complexity based on Java class declarations, we could\\nhave resorted to software metrics [10]. However, it is not clear how metrics\\nindicating an overly complex object-oriented design (e.g. classes with many at-\\ntributes) transfer. The practice of building aggregate models in NoSQL schema\\ndesign may actually be orthogonal.\\nWe refer to [7] for a high-level discussion on schema variety versus code\\nvariety, as well as metrics for programmatic schema analysis.\\n8 Conclusion and Outlook'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='design may actually be orthogonal.\\nWe refer to [7] for a high-level discussion on schema variety versus code\\nvariety, as well as metrics for programmatic schema analysis.\\n8 Conclusion and Outlook\\nIn this paper, we present the study on NoSQL schema evolution with the largest\\ndata basis so far, analyzing ten real-world, open source projects. We track the\\nschema growth as well as the nature of changes to the NoSQL schema. We are\\nable to reproduce most of the insights of related studies on relational schema\\nevolution, but we have also identiﬁed subtle diﬀerences.\\nSince this is a ﬁrst systematic study, many interesting questions remain unan-\\nswered. We remark on two. (1) Originally, we set out to compile detailed statis-\\ntics on the structure of denormalized entity-classes, such as their nesting depth.\\nHowever, we found that Java code written by experienced developers (e.g., as is\\nthe case with Google’s nomulus project) is highly polymorphic. This makes it'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='However, we found that Java code written by experienced developers (e.g., as is\\nthe case with Google’s nomulus project) is highly polymorphic. This makes it\\nimpossible to compute reliable statistics based on the static analysis of entity-\\nclass declarations. However, more holistic analysis techniques, such as data ﬂow\\nanalysis of the entire application code, might reveal further insights. (2) We see\\nevidence that the schema evolves, but we do not know the factors that inﬂuence\\nNoSQL schema evolution. This calls for follow-up work, where we take the git\\ncommit messages into account, which often comment the reason for a schema\\nchange. What is also needed are qualitative studies, surveying developers who\\nroutinely deal with NoSQL schema evolution.\\nAcknowledgements This project was funded by the Deutsche Forschungsgemein-\\nschaft (DFG, German Research Foundation), grant number #385808805.\\nReferences\\n1. Baazizi, M., Colazzo, D., Ghelli, G., Sartiani, C.: Parametric schema inference for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 13, 'page_label': '14'}, page_content='schaft (DFG, German Research Foundation), grant number #385808805.\\nReferences\\n1. Baazizi, M., Colazzo, D., Ghelli, G., Sartiani, C.: Parametric schema inference for\\nmassive JSON datasets. VLDB J. 28(4), 497–521 (2019)\\n2. Bellahsene, Z., Bonifati, A., Rahm, E.: Schema Matching and Mapping. Springer\\nPublishing Company, Incorporated, 1st edn. (2011)\\n3. Cerqueus, T., Cunha de Almeida, E., Scherzinger, S.: Safely Managing Data Vari-\\nety in Big Data Software Development. In: Proc. BIGDSE’15 (2015)\\n4. Chilln, A.H., Ruiz, D.S., Molina, J.G., Morales, S.F.: A Model-Driven Approach\\nto Generate Schemas for Object-Document Mappers. IEEE Access 7, 59126–59142\\n(2019)\\n5. Curino, C.A., Tanca, L., Moon, H.J., Zaniolo, C.: Schema evolution in Wikipedia:\\nToward a Web Information System Benchmark. In: Proc. ICEIS’08 (2008)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Design and Evolution of NoSQL Database Schemas 15\\n6. Fowler, M.: Patterns of Enterprise Application Architecture. Addison-Wesley Long-\\nman Publishing Co., Inc., Boston, MA, USA (2002)\\n7. Jain, S., Moritz, D., Howe, B.: High variety cloud databases. In: Proc. ICDE Work-\\nshops 2016 (2016)\\n8. Klettke, M., St¨ orl, U., Scherzinger, S.: Schema Extraction and Structural Outlier\\nDetection for JSON-based NoSQL Data Stores. In: Proc. BTW’15 (2015)\\n9. Kl´ ımek, J., Mal´ y, J., Necask´ y, M., Holubov´ a, I.: eXolutio: Methodology for Design\\nand Evolution of XML Schemas Using Conceptual Modeling. Informatica, Lith.\\nAcad. Sci. 26(3), 453–472 (2015)\\n10. Lanza, M., Marinescu, R.: Object-Oriented Metrics in Practice: Using Software\\nMetrics to Characterize, Evaluate, and Improve the Design of Object-Oriented\\nSystems. Springer Publishing Company, Incorporated, 1st edn. (2010)\\n11. Lin, D.Y., Neamtiu, I.: Collateral Evolution of Applications and Databases. In:\\nProc. IWPSE-Evol’09 (2009)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='Systems. Springer Publishing Company, Incorporated, 1st edn. (2010)\\n11. Lin, D.Y., Neamtiu, I.: Collateral Evolution of Applications and Databases. In:\\nProc. IWPSE-Evol’09 (2009)\\n12. Meurice, L., Cleve, A.: Supporting schema evolution in schema-less NoSQL data\\nstores. In: Proc. SANER’17 (2017)\\n13. Qiu, D., Li, B., Su, Z.: An Empirical Analysis of the Co-evolution of Schema and\\nCode in Database Applications. In: Proc. ESEC/FSE’13 (2013)\\n14. Ringlstetter, A., Scherzinger, S., Bissyand´ e, T.F.: Data Model Evolution Using\\nobject-NoSQL Mappers: Folklore or State-of-the-art? In: Proc. BIGDSE’16 (2016)\\n15. Sadalage, P.J., Fowler, M.: NoSQL Distilled: A Brief Guide to the Emerging World\\nof Polyglot Persistence. Addison-Wesley Professional, 1st edn. (2012)\\n16. Sanderson, D.: Programming Google App Engine with Java: Build & Run Scalable\\nJava Applications on Google’s Infrastructure. O’Reilly Media, Inc., 1st edn. (2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='16. Sanderson, D.: Programming Google App Engine with Java: Build & Run Scalable\\nJava Applications on Google’s Infrastructure. O’Reilly Media, Inc., 1st edn. (2015)\\n17. Scherzinger, S., Cerqueus, T., Cunha de Almeida, E.: ControVol: A framework for\\ncontrolled schema evolution in NoSQL application development. In: ICDE 2015.\\npp. 1464–1467 (2015)\\n18. Sjøberg, D.: Quantifying schema evolution. Information & Software Technology\\n35(1), 35–44 (1993)\\n19. Skoulis, I., Vassiliadis, P., Zarras, A.V.: Open-Source Databases: Within, Outside,\\nor Beyond Lehman’s Laws of Software Evolution? In: Proc. CAiSE 2014. pp. 379–\\n393 (2014)\\n20. Skoulis, I., Vassiliadis, P., Zarras, A.V.: Growing Up with Stability. Inf. Syst.\\n53(C), 363–385 (Oct 2015)\\n21. Vassiliadis, P., Kolozoﬀ, M., Zerva, M., Zarras, A.V.: Schema evolution and foreign\\nkeys: a study on usage, heartbeat of change and relationship of foreign keys to table\\nactivity. Computing 101(10), 1431–1456 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.17', 'creator': 'LaTeX with hyperref package', 'creationdate': '2020-03-03T02:12:53+00:00', 'author': '', 'keywords': '', 'moddate': '2020-03-03T02:12:53+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.17 (TeX Live 2016) kpathsea version 6.2.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_An_Empirical_Study_on_the_Design_and_Evolution_of_NoSQL_Database_Schemas.pdf', 'total_pages': 15, 'page': 14, 'page_label': '15'}, page_content='keys: a study on usage, heartbeat of change and relationship of foreign keys to table\\nactivity. Computing 101(10), 1431–1456 (2019)\\n22. Vassiliadis, P., Zarras, A.V.: Survival in Schema Evolution: Putting the Lives of\\nSurvivor and Dead Tables in Counterpoint. In: Proc. CAiSE 2017. pp. 333–347\\n(2017)\\n23. Vassiliadis, P., Zarras, A.V., Skoulis, I.: How is Life for a Table in an Evolving\\nRelational Schema? Birth, Death and Everything in Between. In: Proc. ER 2015.\\npp. 453–466 (2015)\\n24. Vassiliadis, P., Zarras, A.V., Skoulis, I.: Gravitating to rigidity: Patterns of schema\\nevolution - and its absence - in the lives of tables. Inf. Syst. 63, 24–46 (2017)\\n25. Wu, S., Neamtiu, I.: Schema Evolution Analysis for Embedded Databases. In: Proc.\\nICDEW’11 (2011)\\n26. Xue Li: A survey of schema evolution in object-oriented databases. In: Proceedings\\nTechnology of Object-Oriented Languages and Systems. pp. 362–371 (1999)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 0, 'page_label': '1'}, page_content='Author’s Accepted Manuscript\\nData Modeling in the NoSQL World\\nPaolo Atzeni, Francesca Bugiotti, Luca Cabibbo,\\nRiccardo Torlone\\nPII:\\nS0920-5489(16)30118-0\\nDOI:\\nhttp://dx.doi.org/10.1016/j.csi.2016.10.003\\nReference:\\nCSI3149\\nTo appear in:\\nComputer Standards & Interfaces\\nReceived date:\\n25 March 2016\\nRevised date:\\n30 September 2016\\nAccepted date:\\n6 October 2016\\nCite this article as: Paolo Atzeni, Francesca Bugiotti, Luca Cabibbo and Riccardo\\nTorlone, Data Modeling in the NoSQL World, \\nComputer Standards &\\nInterfaces, \\nhttp://dx.doi.org/10.1016/j.csi.2016.10.003\\nThis is a PDF file of an unedited manuscript that has been accepted for\\npublication. As a service to our customers we are providing this early version of\\nthe manuscript. The manuscript will undergo copyediting, typesetting, and\\nreview of the resulting galley proof before it is published in its final citable form.\\nPlease note that during the production process errors may be discovered which'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 0, 'page_label': '1'}, page_content='review of the resulting galley proof before it is published in its final citable form.\\nPlease note that during the production process errors may be discovered which\\ncould affect the content, and all legal disclaimers that apply to the journal pertain.\\nwww.elsevier.com'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 1, 'page_label': '2'}, page_content='Data Modeling in the NoSQL World ✩\\nPaolo Atzenia, Francesca Bugiottib,L u c aC a b i b b oa, Riccardo Torlonea\\naUniversit`aR o m aT r e\\nbCentraleSup´elec\\nAbstract\\nNoSQL systems have gained their popularity for many reasons, including the\\nﬂexibility they provide in organizing data, as they relax the rigidity provided by\\nthe relational model and by the other structured models. This ﬂexibility and\\nthe heterogeneity that has emerged in the area have led to a little use of tradi-\\ntional modeling techniques, as opposed to what has happened with databases\\nfor decades.\\nIn this paper, we argue how traditional notions related to data modeling\\ncan be useful in this context as well. Speciﬁcally, we propose NoAM (NoSQL\\nAbstract Model), a novel abstract data model for NoSQL databases, which ex-\\nploits the commonalities of various NoSQLsystems. We also proposea database\\ndesign methodology for NoSQL systems based on NoAM, with initial activities'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 1, 'page_label': '2'}, page_content='ploits the commonalities of various NoSQLsystems. We also proposea database\\ndesign methodology for NoSQL systems based on NoAM, with initial activities\\nthat are independent of the speciﬁc target system. NoAM is used to specify a\\nsystem-independent representation of the application data and, then, this inter-\\nmediate representation can be implemented in target NoSQL databases, taking\\ninto account their speciﬁc features. Overall, the methodology aims at support-\\ning scalability, performance, and consistency, as needed by next-generation web\\napplications.\\nKeywords: Data models, database design, NoSQL systems\\n✩This paper extends a short article appeared in the Proceedings of the 33rd International\\nConference on Conceptual Modeling (ER 2014) with the title Database Design for NoSQL\\nSystems [1].\\nPreprint submitted to Journal of L ATEX Templates October 15, 2016'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}, page_content='1. Introduction\\nNoSQL database systems are today an eﬀective solution to manage large\\ndata sets distributed over many servers. A primary driver of interest in No-\\nSQL systems is their support for next-generation Web applications, for which\\nrelational DBMSs are not well suited. These are simple OLTP applications for5\\nwhich (i) data have a structure that does not ﬁt well in the rigid structure of\\nrelational tables, (ii) access to data is based on simple read-write operations,\\n(iii) relevant quality requirements include scalability and performance, as well\\nas a certain level of consistency [2, 3].\\nNoSQL technology is characterized bya high heterogeneity; indeed, more\\n10\\nthan ﬁfty NoSQL systems exist [4], each with diﬀerent characteristics. They can\\nbe classiﬁed into a few main categories [2], including key-value stores, document\\nstores, and extensible record stores. In any case, this heterogeneity is highly\\nproblematic to application developers [4], even within each category.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}, page_content='stores, and extensible record stores. In any case, this heterogeneity is highly\\nproblematic to application developers [4], even within each category.\\nBeside the diﬀerences between the various systems, NoSQL datastores ex-\\n15\\nhibit an additional phenomenon: they usually support signiﬁcant ﬂexibility in\\ndata, with limited (if any) use of the notion of schema as it is common in\\ndatabases. So, the organization of data, and their regularity, is mainly hard-\\ncoded within individual applications and is not exposed, probably because there\\nis little need for sharing data between applications. Indeed, the notion of20\\nschema, and the need for a separation between data and programs, were moti-\\nvated in databases by the need for sharing data between applications. If this\\nrequirement does not hold any longer, many developers are led to believe that\\nthe importance of schemas gets reduced or even disappears.\\nAs the idea of data model is usually tightly related to that of schema, this\\n25'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 2, 'page_label': '3'}, page_content='the importance of schemas gets reduced or even disappears.\\nAs the idea of data model is usually tightly related to that of schema, this\\n25\\n“schemaless” point view may lead to claim that the very notion of model and of\\nmodeling activities becomes irrelevant with respect to NoSQL databases. The\\ngoal of this paper is to argue that models and modeling do have an interesting\\nrole in this area. Indeed, modeling is an abstraction process, and this helps in\\ngeneral and probably even more in a world of diversity, as the analyst/designer30\\n2'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4'}, page_content='can reason at a high level, before delving into the details of the speciﬁc systems.\\nInstead, given the variety of systems, it is currently the case that the design\\nprocess for NoSQL applications is mainly based on best practices and guide-\\nlines [5], which are speciﬁcally related to the selected system [6, 7, 8], with no\\nsystematic methodology. Several authors have observed that the development35\\nof high-level methodologies and tools supporting NoSQL database design are\\nneeded [9, 10, 11], and models here are deﬁnitely needed, in order to achieve\\nsome level of generality.\\nLet us recall the various reasons for which modeling is considered important\\nin database design and development [12]. First of all, beside being crucial\\n40\\nin the conceptual and logical design phases, it oﬀers support throughout the\\nlifecycle, from requirement analysis, where it helps in giving a structure to the\\nprocess,tocodingandmaintenance, wher eit givesvaluabledocumentation. The'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4'}, page_content='lifecycle, from requirement analysis, where it helps in giving a structure to the\\nprocess,tocodingandmaintenance, wher eit givesvaluabledocumentation. The\\nmain point to be mentioned is that modeling allows the specialist to describe\\nthe domain of interest and the application from various perspectives and at45\\nvarious levels of abstraction. Moreover, it provides support to communication\\n(and to individual comprehension). Finally, it provides support to performance\\nmanagement, as physical database design is also based on data structures, and\\nquery processing eﬃciency is often basedon reference to the regularity of data.\\nConceptual and logical modeling, as they are currently known, were devel-\\n50\\noped in the database world, with speciﬁc attention to relational systems, but\\nfound applications also in other contexts. Indeed, while the importance of rela-\\ntional databases was clear since the Eighties, it was soon understood that there'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 3, 'page_label': '4'}, page_content='found applications also in other contexts. Indeed, while the importance of rela-\\ntional databases was clear since the Eighties, it was soon understood that there\\nwere many “non-business” application domains for which other modeling fea-\\ntures were needed: the advocates of object-oriented databases observed, more\\n55\\nor less at the same time, that some requirements were not satisﬁed, such as\\nthose in CAD, CASE, and multimedia and text management [13]. This led\\nto the development of models with nested structures, more complex than the\\nrelational one, and less regular, and so more diﬃcult to manage.\\nFlexibility in structures was also required in another area, which emerged a60\\ndecade later, and has since been very important: the area of Web applications,\\n3'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5'}, page_content='where there were at least two kinds of developments concerned with models. On\\nthe one hand, work on complex object models for representing hypertexts [14,\\n15, 16], and on the other hand signiﬁcant development in semistructured data,\\nespecially with reference to XML [17].65\\nAnother recurring claim in the database world in the last ten or ﬁfteen\\nyears has been the fact that, while relational databases are ade factostandard,\\nit is not the case that there is one solution that works well for all kinds of\\napplications. As Stonebraker and C¸etintemel [18] argued, it is not the case that\\n“one size ﬁts all,” and diﬀerent engines and technologies are needed in diﬀerent\\n70\\ncontexts, for example OLAP and OLTP have diﬀerent requirements, but the\\nsame holds for other kinds of applications, such as stream processing, sensor\\nnetworks, or scientiﬁc databases.\\nThe NoSQL movement emerged for a number of motivations, including most'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5'}, page_content='same holds for other kinds of applications, such as stream processing, sensor\\nnetworks, or scientiﬁc databases.\\nThe NoSQL movement emerged for a number of motivations, including most\\nof the above, with the goal of supporting highly scalable systems, with speciﬁc\\n75\\nrequirements, usually with very simple operations over many nodes, on sets of\\ndata that have ﬂexible structure. Given that there are many diﬀerent appli-\\ncations and the speciﬁc requirementsvary, many systems have emerged, each\\noﬀeringadiﬀerentwayoforganizingdata andadiﬀerentprogramminginterface.\\nHeterogeneity can become a problem if migration or integration are needed, as\\n80\\nthis is often the case, in a world with changing requirements and new tech-\\nnological developments. Also, the availability of many diﬀerent systems, with\\ndiﬀerent implementations, has led to diﬀerent design techniques, usually related\\njust to individual systems or small families thereof.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 4, 'page_label': '5'}, page_content='diﬀerent implementations, has led to diﬀerent design techniques, usually related\\njust to individual systems or small families thereof.\\nIn this paper we argue that a model-based approach can be useful to tackle85\\nthe diﬃculties related to heterogeneity, and provide support in the form of\\nabstraction. In fact, modeling can be at the basis of a design process, at various\\nlevel; at a higher one to represent the features of interest for the application,\\nand at a lower one to describe some implementation features in a concrete but\\nsystem-independent way.\\n90\\nIndeed, we will present a high-level data model for NoSQL databases, called\\nNoAM (NoSQL Abstract Model) and show how it can be used as an interme-\\n4'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6'}, page_content='diate data representation in the context of a general design methodology for\\nNoSQL applications having initial steps that are independent of the individual\\ntarget system. We propose a design process that includes a conceptual phase,95\\nas common in traditional application, followed (and this is unconventional and\\noriginal) by a system-independent logical design phase, where the intermediate\\nrepresentation is used, as the basis for both modeling and performance aspects,\\nwith only a ﬁnal phase that takes into account the speciﬁc features of individual\\nsystems.\\n100\\nThe rest of the paper is organized as follows. In Section 2, we illustrate\\nthe features of the main categories of NoSQL systems arguing that, for each\\nof them, there exists a sort of data model. In Section 3 we present NoAM,\\nour system-independent data model for NoSQL databases, and in Section 4 we\\ndiscuss our design methodology for NoSQL databases. In Section 5 we brieﬂy\\n105'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6'}, page_content='our system-independent data model for NoSQL databases, and in Section 4 we\\ndiscuss our design methodology for NoSQL databases. In Section 5 we brieﬂy\\n105\\nreview some related literature. Finally, in Section 6 we draw some conclusions.\\n2. NoSQL data models\\nIn this section we brieﬂy present and compare a number of representative\\nNoSQL systems, to make apparent the heterogeneity (as well as the similarities)\\nin the way they organize data and in their programming interfaces. We ﬁrst110\\nintroduce a sample application dataset, and then we show how to represent\\nthese data in the representative systems we consider.\\n2.1. Running example\\nLet us consider, as a running example, an application for an on-line social\\ngame. This is indeed a typical scenario in which the use of a NoSQL database\\n115\\nis suitable, that is, a simple next-generation Web application (as discussed in\\nthe Introduction).\\nThe application should manage various types of objects, including players,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 5, 'page_label': '6'}, page_content='115\\nis suitable, that is, a simple next-generation Web application (as discussed in\\nthe Introduction).\\nThe application should manage various types of objects, including players,\\ngames, and rounds. A few representative objects are shown in Figure 1. The\\nﬁgure is a UML object diagram. Boxes and arrows denote objects and relation-120\\nships between them, respectively.\\n5'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 6, 'page_label': '7'}, page_content='mary : Player\\nusername = \"mary\"\\nfirstName = \"Mary\"\\nlastName = \"Wilson \"\\nrick : Player\\nusername = \"rick\"\\nfirstName = \"Ricky\"\\nlastName = \"Doe\"\\nscore = 42\\n2345 : Game\\nid = 2345\\nfirstPlayer secondPlayer\\n: GameInfo\\ngames [0]\\ngameopponent\\n: GameInfo\\ngames [0]\\ngame opponent\\n: Round : Round\\nrounds [0] rounds [1]\\n: Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 1: Sample application objects\\nmary : Player\\nusername = \"mary\"\\nfirstName = \"Mary\"\\nlastName = \"Wilson\"\\nrick : Player\\nusername = \"rick\"\\nfirstName = \"Ricky\"\\nlastName = \"Doe\"\\nscore = 42\\n2345 : Game\\nid = 2345\\nfirstPlayer secondPlayer\\n: GameInfo\\ngames [0]\\ngameopponent\\n: GameInfo\\ngames [0]\\ngame opponent\\n: Round : Round\\nrounds[0] rounds [1]\\n: Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 2: Sample aggregates (as groups of objects)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 6, 'page_label': '7'}, page_content=': Move : Move\\nmoves[0] moves[1]\\n: Move\\nmoves[0]\\n: GameInfo\\ngames [2]\\n: GameInfo\\ngames [1]\\n: GameInfo\\ngames [1]...\\n...\\n...\\n...\\n...\\n...\\nFigure 2: Sample aggregates (as groups of objects)\\nTo represent a dataset in a NoSQL database, it is often useful to arrange\\ndata in aggregates [19, 20]. Each aggregate is a group of related application\\nobjects, representing a unit of data access and atomic manipulation. In our\\nexample, relevant aggregates are players and games, as shown by closed curves125\\nin Figure 2. Note that the rounds of a game are grouped within the game itself.\\nIn general, aggregatescan be considered as complex-value objects [21], as shown\\nin Figure 3.\\nThedataaccessoperationsneededbyouro n-linesocialgamearesimpleread-\\nwrite operations on individual aggregates; for example, create a new player and130\\nretrieve a certain game. Other operations involve just a portion of an aggregate;\\n6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 7, 'page_label': '8'}, page_content='Player:mary : ⟨\\nusername : ”mary”,\\nﬁrstName : ”Mary”,\\nlastName : ”Wilson”,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:rick ⟩,\\n⟨ game : Game:2611, opponent : Player:ann ⟩\\n}\\n⟩\\nPlayer:rick : ⟨\\nusername : ”rick”,\\nﬁrstName : ”Ricky”,\\nlastName : ”Doe”,\\nscore : 42,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:mary ⟩,\\n⟨ game : Game:7425, opponent : Player:ann ⟩,\\n⟨ game : Game:1241, opponent : Player:johnny ⟩\\n}\\n⟩\\nGame:2345 : ⟨\\nid : ”2345”,\\nﬁrstPlayer : Player:mary,\\ns\\necondPlayer : Player:rick,\\nrounds : {\\n⟨ moves : ... , comments : ... ⟩,\\n⟨ moves : ... , actions : ..., spell : ... ⟩\\n}\\n⟩\\nFigure 3: Sample aggregates (as complex values)\\nfor example, add a round to an existing game. In general, it is indeed the\\ncase that most real applications require only operations that access individual\\naggregates [2, 22].\\n2.2. NoSQL database models135\\nNoSQL database systems organize their data according to quite diﬀerent\\ndata models. They usually provide simple read-write data-access operations,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 7, 'page_label': '8'}, page_content='aggregates [2, 22].\\n2.2. NoSQL database models135\\nNoSQL database systems organize their data according to quite diﬀerent\\ndata models. They usually provide simple read-write data-access operations,\\nwhich also diﬀer from system to system. Despite this heterogeneity, a few main\\ncategories can be identiﬁed according to the modeling features of these sys-\\n7'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9'}, page_content='tems [2, 3]: key-value stores, document stores, extensible record stores, plus140\\nothers (e.g., graph databases) that are beyond the scope of this paper.\\n2.3. Key-value stores\\nIn general, in akey-value store, a database is a schemaless collection of key-\\nvalue pairs, with data access operations on either individual key-value pairs or\\ngroups of related pairs.145\\nAs a representative key-value store we consider hereOracle NoSQL[23]. In\\nthis system,keys are structured; they are composed of amajor keyand aminor\\nkey. The major key is a non-empty sequence of strings. The minor key is a\\nsequence of strings. Eachelement of a key is called acomponent of the key. On\\nthe other hand, eachvalue is an uninterpreted binary string.150\\nA sample key-value is the pair composed of key/Player/mary/-/username\\nand value ”mary”.I n t h e k e y , s y m b o l ‘/’ separates key components, while\\nsymbol ‘-’ separates the major key from the minor key. The distinction between'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9'}, page_content='and value ”mary”.I n t h e k e y , s y m b o l ‘/’ separates key components, while\\nsymbol ‘-’ separates the major key from the minor key. The distinction between\\nmajor key and minor is especially relevant to control data distribution and\\nsharding.155\\nIn a pair, the value can be either a simple value (such as the string”mary”)\\nor a complex value. In the former case, it is common to use some data inter-\\nchange format (such as XML, JSON, and Protocol Buﬀers [24]) to represent\\nsuch complex values.\\nOracle NoSQL oﬀers simple atomic access operations, to access and modify\\n160\\nindividual key-value pairs: put(key,value) to add or modify a key value pair\\nand get(key) to retrieve a value, given the key. Oracle NoSQL also provides\\nan atomic multiGet(majorKey) operation to access a group of related key-value\\npairs, and speciﬁcally the pairs having the same major key. Moreover, it oﬀers\\nan execute operation for executing multiple put operations in an atomic and\\n165'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 8, 'page_label': '9'}, page_content='pairs, and speciﬁcally the pairs having the same major key. Moreover, it oﬀers\\nan execute operation for executing multiple put operations in an atomic and\\n165\\neﬃcient way (provided that the keys speciﬁed in these operations all share a\\nsame major key).\\nThe data representation for a dataset in a key-value store can be based on\\naggregates. These are two common representations for aggregates:\\n8'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10'}, page_content='•Represent an aggregate using a single key-value pair. The key (major key)170\\nis the aggregateidentiﬁer. The value is the complex value ofthe aggregate.\\nSee Figure 4(a).\\n•Represent an aggregate using multiple key-value pairs. Speciﬁcally, the\\naggregate is split in parts that need to be accessed or modiﬁed separately,\\nand each part is represented by a distinct but related key-value pair. The175\\naggregateidentiﬁer is usedasmajorkeyfor allthese parts, while the minor\\nkey identiﬁes the part within the aggregate. See Figure 4(b).\\nThe data access operations provided by key-value stores usually enable an ef-\\nﬁcient and atomic data access to aggregates with respect to both data repre-\\nsentations. Indeed, all systems support the access to individual key-value pairs\\n180\\n(useful in the former case) and most of them (such as Oracle NoSQL) provide\\nalso the access to groups of related key-value pairs (required inthe latter case).\\n2.4. Document stores'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10'}, page_content='180\\n(useful in the former case) and most of them (such as Oracle NoSQL) provide\\nalso the access to groups of related key-value pairs (required inthe latter case).\\n2.4. Document stores\\nIn adocument store, a database is a set of documents, each having a complex\\nstructure and value.\\n185\\nIn this category, a widely used system isMongoDB [25]. It is an open-source,\\ndocument-oriented data store that oﬀers a full-index support on any attribute,\\na rich document-based query API and Map-Reduce support.\\nIn MongoDB, adatabase comprises one or more collections. Eachcollection\\nis a named group of documents. Eachdocument is a structured document, that\\n190\\nis, a complex value, a set of attribute-value pairs, which can comprise simple\\nvalues, lists, and even nested documents. Thus, documents are neither freeform\\ntext documents nor Oﬃce documents. Documents are schemaless, that is, each\\ndocument can have its own attributes, deﬁned at runtime.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 9, 'page_label': '10'}, page_content='text documents nor Oﬃce documents. Documents are schemaless, that is, each\\ndocument can have its own attributes, deﬁned at runtime.\\nSpeciﬁcally, MongoDB documents are based on BSON (Binary JSON), a\\n195\\nvariant of the popular JSON format. Values constituting documents can be of\\nthe following types: (i) basic types, such strings numbers, dates, and boolean\\nvalues; (ii) arrays, i.e., ordered sequences of values; and (iii) documents (or\\n9'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11'}, page_content='key (/major/key/-) value\\n/Player/mary/- { username: ”mary”, ﬁrstName: ”Mary”, ... }\\n/Player/rick/- { username: ”rick”, ﬁrstName: ”Ricky”, ... }\\n/Game/2345/- { id: ”2345”, ﬁrstPlayer: ”Player:mary”, ... }\\n(a) Single key-value pair per aggregate\\nkey (/major/key/-/minor/key) value\\nPlayer/mary/-/username ”mary”\\nPlayer/mary/-/ﬁrstName ”Mary”\\nPlayer/mary/-/lastName ”Wilson”\\nPlayer/mary/-/games[0] {game: ”Game:2345”, opponent: ”Player:rick”}\\nPlayer/mary/-/games[1] {game: ”Game:2611”, opponent: ”Player:ann”}\\nPlayer/rick/-/username ”rick”\\nPlayer/rick/-/ﬁrstName ”Ricky”\\nPlayer/rick/-/lastName ”Doe”\\nPlayer/rick/-/score 42\\nPlayer/rick/-/games[0] {game: ”Game:2345”, opponent: ”Player:mary”}\\nPlayer/rick/-/games[1] {game: ”Game:7425”, opponent: ”Player:ann”}\\nPlayer/rick/-/games[2] {game: ”Game:1241”, opponent: ”Player:johnny”}\\nGame/2345/-/id 2345\\nGame/2345/-/ﬁrstPlayer ”Player:mary”\\nGame/2345/-/secondPlayer ”Player:rick”\\nGame/2345/-/rounds[0] {moves : ..., comments: ...}'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11'}, page_content='Game/2345/-/id 2345\\nGame/2345/-/ﬁrstPlayer ”Player:mary”\\nGame/2345/-/secondPlayer ”Player:rick”\\nGame/2345/-/rounds[0] {moves : ..., comments: ...}\\nGame/2345/-/rounds[1] {moves : ..., actions: ..., spell: ...}\\n(b) Multiple key-value pairs per aggregate\\nFigure 4: Representing aggregates in Oracle NoSQL\\nobjects): a document is a collection of zero or more key-value pairs, where each\\nkey is a plain string, while each value is of any of these types. Figure 5 shows a200\\nJSON representation of the complex value of a samplePlayer aggregate object\\nof Figures 2 and 3.\\nA main document is a top-level document with a unique identiﬁer, repre-\\nsented by a special attribute\\nid, associated to a value of a special typeObjectId.\\nData access operations are usually over individual documents, which are205\\nunits of data distribution and atomic data manipulation. The basic operations\\noﬀered by MongoDB are as follows:insert(coll,doc) adds a main documentdoc'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 10, 'page_label': '11'}, page_content='units of data distribution and atomic data manipulation. The basic operations\\noﬀered by MongoDB are as follows:insert(coll,doc) adds a main documentdoc\\ninto collectioncoll;a n dﬁnd(coll,selector) retrieves from collectioncoll all main\\ndocuments matching document selector. The simplest selector is the empty\\ndocument {}, which matches with every document; it allows to retrieve all210\\n10'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 11, 'page_label': '12'}, page_content='[\\n\"username\" : \"mary\",\\n\"firstName\" : \"Mary\",\\n\"lastName\" : \"Wilson\",\\n\"games\" : {\\n[ \"id\" : \"Game:2345\", \"opponent\" : \"Player:rick\" ],\\n[ \"id\" : \"Game:2611\", \"opponent\" : \"Player:ann\"]\\n}\\n]\\nFigure 5: The JSON representation of the complex value of a sample Player object\\ncol lection\\n document id\\n document\\nPlayer\\n mary\\n {\"\\n id\":\"mary\", \"username\":\"mary\", \"firstName\":\"Mary\", ... }\\nPlayer\\n rick\\n {\"\\n id\":\"rick\", \"username\":\"rick\", \"firstName\":\"Rock\", ... }\\nGame\\n 2345\\n {\"\\n id\":\"2345\", \"firstPlayer\":\"Player:mary\", ... }\\nFigure 6: Representing aggregates in MongoDB\\ndocuments in a collection. Another useful selector is document{\\n id:ID},w h i c h\\nmatches with the document having identiﬁerID. There is also an operation to\\nupdate a document. Moreover, it is also possible to access or update just a\\nspeciﬁc portion of a document.\\nIn a document store, each aggregate is usually represented by a single main215\\ndocument. The document collection corresponds to the aggregate class (or'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 11, 'page_label': '12'}, page_content='speciﬁc portion of a document.\\nIn a document store, each aggregate is usually represented by a single main215\\ndocument. The document collection corresponds to the aggregate class (or\\ntype). The document identiﬁer ID is the aggregate identiﬁer. The content of\\nthe document is the complex-value of the aggregate, in JSON/BSON, including\\nalso an additional key-value pair{\\n id:ID} for the identiﬁer. See Figure 6.\\nAlso in this case, the data access operations oﬀered by document stores220\\n(such as MongoDB) provide an atomic and eﬃcient data access to aggregates.\\nSpeciﬁcally, they generally support both operations on individual aggregates, or\\nto speciﬁc portions of them, thereof.\\n2.5. Extensible record stores\\nIn an extensible record store, a database is a set of tables, each table is a225\\nset of rows, and each row contains a set of attributes (or columns), each with a\\nname and a value. Rows in a table are not required to have the same attributes.\\n11'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13'}, page_content='Data access operations are usually overindividual rows, which are units of data\\ndistribution and atomic data manipulation.\\nA representative extensible record store isAmazon DynamoDB [26], a No-230\\nSQL database service provided on the cloud by Amazon Web Services (AWS).\\nIn DynamoDB a database is organized in tables. Atable is a set of items. Each\\nitem contains one or moreattributes,e a c hw i t haname and a value (or a set\\nof values). Each table designates an attribute asprimary key. Items in a same\\ntable are not required to have the same set of attributes — apart from the pri-235\\nmary key, which is the only mandatory attribute of a table. Thus, DynamoDB\\ndatabases are mostly schemaless.\\nSpeciﬁcally, the primary key is composed of apartition keyand an optional\\nsort key. If the primary key of a table includes a sort key, then DynamoDB\\nstores together all the items having the same partition key, in such a way that240\\nthey can be accessed in an eﬃcient way.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13'}, page_content='sort key. If the primary key of a table includes a sort key, then DynamoDB\\nstores together all the items having the same partition key, in such a way that240\\nthey can be accessed in an eﬃcient way.\\nDistribution is operated at the item level and, for each table, is controlled\\nby the partition key only.\\nSome operations oﬀered by DynamoDB are as follows:putItem(table,key,av)\\nadds (or modiﬁes) a new item in tabletable with primary keykey,u s i n gt h e245\\nset of attribute-value pairsav;a n dgetItem(table,key) retrieves the item of table\\ntable having primary keykey. It is also possible to access or update just a subset\\nof the attributes of an item. All these operations can be executed in an eﬃcient\\nway.\\nIn an extensible record store (such as DynamoDB), each aggregate can be250\\nrepresented by a record/row/item. The table corresponds to the aggregate class\\n(or type). The primary key (partition key) is the aggregate identiﬁer. Then,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 12, 'page_label': '13'}, page_content='represented by a record/row/item. The table corresponds to the aggregate class\\n(or type). The primary key (partition key) is the aggregate identiﬁer. Then,\\nthe item can have a distinct attribute-value pair for each top-level attribute of\\nthe complex value of the aggregate(or for each major part of the aggregatethat\\nneeds to be accessed separately). See Figure 7.\\n255\\nAgain, the data access operations provided by the systems in this category\\nsupport an eﬃcient data access to aggregates or to speciﬁc portions of them.\\n12'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14'}, page_content='table Player\\nusername ﬁrstName lastName score games[0] games[1] games[2]\\n”mary” ”Mary” ”Wilson” { game: ..., opponent: ... }{ ... }\\n”rick” ”Ricky” ”Doe” 42 { game: ..., opponent: ... }{ ... }{ ... }\\ntable Game\\nid ﬁrstPlayer secondPlayer rounds[0] rounds[1] rounds[2]\\n2345 Player:mary Player:rick { moves : ..., comments: ... }{ ... }\\nFigure 7: Representing aggregates in DynamoDB (abridged)\\n2.6. Comparison\\nTo summarize, it is possible to say that each NoSQL system provides a num-\\nber of “modeling elements” to organize data, which can be considered the “data260\\nmodel” of the system. Moreover, the various systems can be eﬀectively classiﬁed\\nin a few main categories, where each category is based on “data models” that,\\neven though not identical, do share some similarities. In the next section we\\nshow that it is possible to pursue these similarities, thus deﬁning an “abstract\\ndata model” for NoSQL databases.265\\n3. The NoAM data model'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14'}, page_content='show that it is possible to pursue these similarities, thus deﬁning an “abstract\\ndata model” for NoSQL databases.265\\n3. The NoAM data model\\nIn this section we presentNoAM (NoSQL Abstract Data Model), a system-\\nindependent data model for NoSQL databases. In the following section we will\\nalso discuss how this data model can be used to support the design of NoSQL\\ndatabases.270\\nIntuitively, the NoAM data model exploits the commonalities of the data\\nmodeling elements available in the various NoSQL systems and introduces ab-\\nstractions to balance their diﬀerences and variations.\\nA ﬁrst observation is that all NoSQL systems have a data modeling element\\nthat is a data access and distribution unit. By “data access unit” we mean\\n275\\nthat the system oﬀers operations to access and manipulate an individual unit\\nat a time, in an atomic, eﬃcient, and scalable way. By “distribution unit” we\\nmean that each unit is entirely stored in aserver of the cluster, whereas diﬀer-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 13, 'page_label': '14'}, page_content='at a time, in an atomic, eﬃcient, and scalable way. By “distribution unit” we\\nmean that each unit is entirely stored in aserver of the cluster, whereas diﬀer-\\nent units are distributed among the various servers. With reference to major\\n13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15'}, page_content='NoSQL categories, this element is: (i) a group of related key-value pairs, in key-280\\nvalue stores; (ii) a document, in document stores; or (iii) a record/row/item, in\\nextensible record stores.\\nIn NoAM, a data access and distribution unit is modeled by ablock. Specif-\\nically, a block represents amaximal data unit for which atomic, eﬃcient, and\\nscalable access operations are provided. Indeed, while the access to an individ-285\\nual block can be performed in an eﬃcient way in the various systems, the access\\nto multiple blocks can be quite ineﬃcient. In particular, NoSQL systems do\\nnot usually provide an eﬃcient “join” operation. Moreover, most NoSQL sys-\\ntems provide atomic operations only over single blocks and do not support the\\natomic manipulation of a group of blocks. For example, MongoDB [25] provides\\n290\\nonly atomic operations over individual documents, whereas Bigtable does not\\nsupport transactions across rows [22].'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15'}, page_content='atomic manipulation of a group of blocks. For example, MongoDB [25] provides\\n290\\nonly atomic operations over individual documents, whereas Bigtable does not\\nsupport transactions across rows [22].\\nA second common feature of NoSQL systems is the ability to access and\\nmanipulate just a component of a data access unit (i.e., of a block). This\\ncomponent is: (i) an individual key-value pair, in key-value stores; (ii) a ﬁeld,\\n295\\nin document stores; or (iii) a column, in extensible record stores. In NoAM,\\nsuch a smaller data access unit is called anentry.\\nFinally, most NoSQL databases providea notion of collection of data access\\nunits. For example, a table in extensiblerecord stores or a document collection\\nin document stores. In NoAM, a coll ection of data access units is called a300\\ncollection.\\nAccording to the above observations, the NoAM data model is deﬁned as\\nfollows.\\n•AN o A Mdatabase is a set ofcollections. Each collection has a distinct\\nname.305'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 14, 'page_label': '15'}, page_content='collection.\\nAccording to the above observations, the NoAM data model is deﬁned as\\nfollows.\\n•AN o A Mdatabase is a set ofcollections. Each collection has a distinct\\nname.305\\n•A collection is a set ofblocks. Each block in a collection is identiﬁed by a\\nblock key, which is unique within that collection.\\n•A block is a non-empty set ofentries. Each entry is a pair⟨ek,ev⟩,w h e r e\\nek is theentry key (which is unique within its block) andev is its value\\n14'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 15, 'page_label': '16'}, page_content='Player\\nmary\\nusername\\n ”mary”\\nﬁrstName\\n ”Mary”\\nlastName\\n ”Wilson”\\ngames[0]\\n ⟨ game : Game:2345, opponent : Player:rick ⟩\\ngames[1]\\n ⟨ game : Game:2611, opponent : Player:ann ⟩\\nrick\\nusername\\n ”rick”\\nﬁrstName\\n ”Ricky”\\nlastName\\n ”Doe”\\nscore\\n 42\\ngames[0]\\n ⟨ game : Game:2345, opponent : Player:mary ⟩\\ngames[1]\\n ⟨ game : Game:7425, opponent : Player:ann ⟩\\ngames[2]\\n ⟨ game : Game:1241, opponent : Player:johnny ⟩\\nGame\\n2345\\nid\\n 2345\\nﬁrstPlayer\\n Player:mary\\nsecondPlayer\\n Player:rick\\nrounds[0]\\n ⟨ moves : ..., comments : ... ⟩\\nrounds[1]\\n ⟨ moves : ..., actions : ..., spell : ... ⟩\\nFigure 8: A sample database in NoAM\\n(either complex or scalar), called theentry value.310\\nFor example, Figure 8 shows a possible representation of the aggregates\\nof Figures 2 and 3 in terms of the NoAM data model. There, outer boxes\\ndenote blocks representing aggregates, while inner boxes show entries. Note\\nthat entry values can be complex, being this another commonality of various\\nNoSQL systems.315'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 15, 'page_label': '16'}, page_content='denote blocks representing aggregates, while inner boxes show entries. Note\\nthat entry values can be complex, being this another commonality of various\\nNoSQL systems.315\\nPlease note that the same data can be usually represented in diﬀerent ways.\\nCompare, for example, Figure 8 with Figure 9. We will discuss this possibility\\nin the next section.\\nIn summary, NoAM describes in a uniform way the features of many NoSQL\\nsystems, and so can be eﬀectively used,as we show in the next section, for an\\n320\\n15'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 16, 'page_label': '17'}, page_content='Player\\nmary\\n ϵ\\n⟨username:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:rick ⟩,\\n⟨ game : Game:2611, opponent : Player:ann ⟩\\n}⟩\\nrick\\n ϵ\\n⟨username:”rick”,\\nﬁrstName:”Ricky”,\\nlastName:”Doe”,\\nscore:42,\\ngames : {\\n⟨ game : Game:2345, opponent : Player:mary ⟩,\\n⟨ game : Game:7425, opponent : Player:ann ⟩,\\n⟨ game : Game:1241, opponent : Player:johnny ⟩\\n}⟩\\nGame\\n2345\\n ϵ\\n⟨id : ”2345”,\\nﬁrstPlayer : Player:mary,\\nsecondPlayer : Player:rick,\\nrounds : {\\n⟨ moves :..., comments : ... ⟩,\\n⟨ moves :..., actions : ..., spell : ... ⟩\\n}⟩\\nFigure 9: Another NoAM sample database\\nintermediate representation in a NoSQL database design methodology.\\n4. System-independent design of NoSQL databases with NoAM\\nThe main goal of NoAM is to support a design methodology for NoSQL\\ndatabases that has initial activities that are independent of the speciﬁc tar-\\nget system. In particular, NoAM is used to specify an intermediate, system-325'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 16, 'page_label': '17'}, page_content='databases that has initial activities that are independent of the speciﬁc tar-\\nget system. In particular, NoAM is used to specify an intermediate, system-325\\nindependent representation of the application data. The implementation in a\\ntarget NoSQL system is then a ﬁnal step, with a translation that takes into\\naccount its peculiarities.\\n16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18'}, page_content='The motivations to consider database design for NoSQL systems are as fol-\\nlows. It is important to notice that despite the fact that NoSQL databases330\\nare claimed to be “schemaless,” the data of interest for applications do show\\nsome structure, which should be mapped to the modeling elements (collections,\\ntables, documents, key-value pairs) available in the target system. Moreover,\\ndiﬀerent alternatives in the organization of data in a NoSQL database are usu-\\nally possible, but they are not equivalent in supporting qualities such as perfor-335\\nmance, scalability, and consistency (which are typically required when a NoSQL\\ndatabase is adopted). For example, a “wrong” database representation can lead\\nto performance that are worse by an order of magnitude as well as to the in-\\nability to guarantee atomicity of important operations.\\nSpeciﬁcally, our design methodology has the goal of designing a “good” rep-\\n340\\nresentation of the application data in a target NoSQL database, and is intended'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18'}, page_content='Speciﬁcally, our design methodology has the goal of designing a “good” rep-\\n340\\nresentation of the application data in a target NoSQL database, and is intended\\nto support major qualities such as performance, scalability, and consistency, as\\nneeded by next-generation Web applications.\\nThe NoAM approach is based on the following main activities:\\n•conceptual data modeling and aggregate design, to identify the various345\\nentities and relationships thereof needed in an application, and to group\\nrelated entities into aggregates;\\n•aggregate partitioning and high-level NoSQL database design, where ag-\\ngregates are partitioned into smaller data elements and then mapped to\\nthe NoAM intermediate data model;350\\n•implementation, to map the intermediate data representation to the spe-\\nciﬁc modeling elements of a target datastore.\\nIn this approach, only the implementation depends on the target datastore.\\nWe will discuss the various steps of this approach in the rest of this section.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 17, 'page_label': '18'}, page_content='In this approach, only the implementation depends on the target datastore.\\nWe will discuss the various steps of this approach in the rest of this section.\\n4.1. Conceptual modeling and aggregate design355\\nThe methodology starts, as it is usual in database design, by building a con-\\nceptual representation of the data of interest, in terms of entities, relationships,\\n17'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19'}, page_content='and attributes. (This activity is discussed in most database textbooks, e.g.,\\n[12].) Following Domain-Driven Design (DDD [19]), which is a widely followed\\nobject-oriented methodology, we assume that the outcome of this activity is a360\\nconceptual UML class diagram deﬁning the entities, value objects, and relation-\\nships of the application. Anentity is a persistent object that has independent\\nexistence and is distinguished by a unique identiﬁer (e.g., a player or a game,\\nin our running example). Avalue objectis a persistent object which is mainly\\ncharacterized by its value, without an own identiﬁer (e.g., a round or a move).\\n365\\nThen, the methodology proceeds by identifying aggregates.\\nThe design of aggregates has the goal of identifying the classes of aggregates\\nfor an application, and various approaches are possible. After the preliminary\\nconceptual design phase, entities and value objects are grouped into aggregates.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19'}, page_content='for an application, and various approaches are possible. After the preliminary\\nconceptual design phase, entities and value objects are grouped into aggregates.\\nEach aggregate has an entity as its root, and it can also contain many value370\\nobjects. Intuitively, an entity and a group of value objects are used to deﬁne an\\naggregate having a complex structure and value.\\nThe relevant decisions in aggregate design involve the choice of aggregates\\nand of their boundaries. This activity can be driven by the data access pat-\\nterns of the application operations, as well as by scalability and consistency\\n375\\nneeds [19]. Speciﬁcally, aggregates should be designed as the units on which\\natomicity must be guaranteed [20] (with eventual consistency for update op-\\nerations spanning multiple aggregates [27]). In general, it is indeed the case\\nthat most real applications require only operations that access individual aggre-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 18, 'page_label': '19'}, page_content='erations spanning multiple aggregates [27]). In general, it is indeed the case\\nthat most real applications require only operations that access individual aggre-\\ngates [2, 22]. Eachaggregateshould be large enough so as to include all the data380\\nrequired by a relevant data access operation. (Please note that NoSQL systems\\ndo not provide a “join” operation, and this is a main motivation for clustering\\neach group of related application objects into an aggregate.) Furthermore, to\\nsupport strong consistency (that is, atomicity) of update operations, each ag-\\ngregate should include all the data involved by some integrity constraints or\\n385\\nother forms of business rules [28]. On the other hand, aggregates should be as\\nsmall as possible; smallaggregates reduce concurrency collisions and support\\nperformance and scalability requirements [28].\\n18'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20'}, page_content='Thus, aggregate design is mainly driven by data access operations. In our\\nrunning example, the online game application needs to manage various collec-390\\ntions of objects, including players, games, and rounds. Figure 2 shows a few\\nrepresentativeapplication objects. (There, boxes and arrowsdenote objects and\\nlinks between them, respectively. An object having a colored top compartment\\nis an entity, otherwise it is a value object.) When a player connects to the\\napplication, all data on the player should be retrieved, including an overview395\\nof the games she is currently playing.Then, the player can select to continue\\na game, and data on the selected game should be retrieved. When a player\\ncompletes a round in a game she is playing, then the game should be updated.\\nThese operations suggest that the candidate aggregate classes are players and\\ngames. Figure 2 also shows how application objects can be grouped in aggre-400\\ngates. (There, a closed curve denotes the boundary of an aggregate.)'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20'}, page_content='games. Figure 2 also shows how application objects can be grouped in aggre-400\\ngates. (There, a closed curve denotes the boundary of an aggregate.)\\nAs we mentioned above,aggregatedesign is alsodriven by consistency needs.\\nAssume that the application should enforce a rule specifying that a round can\\nbe added to a game only if some condition that involves the other rounds of the\\ngame is satisﬁed. An individual round cannot check, alone, the above condition;\\n405\\ntherefore, it cannot be an aggregate by itself. On the other hand, the above\\nbusiness rule can be supported by a game (comprising, as an aggregate, its\\nrounds).\\nIn conclusion, the aggregate classes for our sample application arePlayer\\nand Game, as shown in Figures 2 and 3.\\n410\\n4.2. Data representation in NoAM and aggregate partitioning\\nIn our approach, we use the NoAM data model (Section 3) as an intermedi-\\nate model between application aggregates (Section 4.1) and NoSQL databases'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 19, 'page_label': '20'}, page_content='In our approach, we use the NoAM data model (Section 3) as an intermedi-\\nate model between application aggregates (Section 4.1) and NoSQL databases\\n(Section 2). We represent each class of aggregates by means of a distinct col-\\nlection, and each individual aggregate by means of a block. We use the class415\\nname to name the collection, and the identiﬁer of the aggregate as block key.\\nThe complex value of each aggregate is represented by a set of entries in the\\ncorresponding block. For example, the aggregates of Figures 2 and 3 can be\\n19'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21'}, page_content='represented by the NoAM database shown in Figure 8. The representation of\\naggregates as blocks is motivated by the fact that both concepts represent a420\\nunit of data access and distribution, but at diﬀerent abstraction levels. Indeed,\\nNoSQL systems provide eﬃcient, scalable, and consistent (i.e., atomic) opera-\\ntions on blocks and, in turn, this choice propagates such qualities to operations\\non aggregates.\\nIn general, an application dataset of aggregates can be represented in NoAM425\\ndatabase in several diﬀerent ways. Eachdata representationfor a datasetδis a\\nNoAMdatabase Dδrepresentingδ. Speciﬁcally, thevariousdatarepresentations\\nfor a dataset diﬀer only in the choice of the entries used to represent the complex\\nvalue of each aggregate. We ﬁrst discuss basic data representation strategies,\\nwhich we illustrate with respect to the example described in Figure 3. We then\\n430\\nintroduce additional and more ﬂexible data representations.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21'}, page_content='which we illustrate with respect to the example described in Figure 3. We then\\n430\\nintroduce additional and more ﬂexible data representations.\\nA simple data representation strategy, called Entry per Aggregate Object\\n(EAO), represents each individual aggregate using a single entry. The entry\\nkey is empty. The entry value is the whole complex value of the aggregate. The\\ndata representation of the aggregates of Figure 3 according to the EAO strategy435\\nis shown in Figure 9.\\nAnotherdatarepresentationstrategy,called Entry per Top-level Field(ETF),\\nrepresents each aggregate by means of multiple entries, using a distinct entry\\nfor each top-level ﬁeld of the complex value of the aggregate. For each top-level\\nﬁeld f of an aggregateo, it employs an entry having as value the value of ﬁeldf440\\nin the complex value ofo (with values that can be complex themselves), and as\\nkey the ﬁeld namef. Figure 10 shows the data representation of the aggregates\\nof Figure 3 according to the ETF strategy.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 20, 'page_label': '21'}, page_content='in the complex value ofo (with values that can be complex themselves), and as\\nkey the ﬁeld namef. Figure 10 shows the data representation of the aggregates\\nof Figure 3 according to the ETF strategy.\\nAs a comparison, we can observe that the EAO data representation uses a\\nblock with a single entry to represent thePlayerobject having usernamemary,445\\nwhile the ETF representation needs a block with four entries, corresponding to\\nﬁelds username, ﬁrstName, lastName,a n dgames. Moreover, blocks in EAO\\ndo not depend on the structure of aggregates, while blocks in ETF depend on\\nthe top-level structure of aggregates (which can be “almost ﬁxed” within each\\n20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 21, 'page_label': '22'}, page_content='Player\\nmary\\nusername\\n ”mary”\\nﬁrstName\\n ”Mary”\\nlastName\\n ”Wilson”\\ngames\\n{⟨ game: Game:2345, opponent: Player:rick ⟩,\\n⟨ game: Game:2611, opponent: Player:ann ⟩}\\nrick\\nusername\\n ”rick”\\nﬁrstName\\n ”Ricky”\\nlastName\\n ”Doe”\\nscore\\n 42\\ngames\\n{⟨ game: Game:2345, opponent: Player:mary ⟩,\\n⟨ game: Game:7425, opponent: Player:ann ⟩,\\n⟨ game: Game:1241, opponent: Player:johnny ⟩}\\nGame\\n2345\\nid\\n 2345\\nﬁrstPlayer\\n Player:mary\\nsecondPlayer\\n Player:rick\\nrounds\\n{⟨ moves: ..., comments: ..., ⟩\\n⟨ moves: ..., actions: ..., spell: ... ⟩}\\nFigure 10: The ETF data representation\\nclass).450\\nThe general data representation strategies we just described can be suited in\\nsome cases, but they are often too rigid and limiting. For example, none of the\\nabove strategies leads to the data representation shown in Figure 8. The main\\nlimitation of such generaldata representations is that they refer only to the\\nstructure of aggregates, and do not take into account the data access patterns\\n455'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 21, 'page_label': '22'}, page_content='limitation of such generaldata representations is that they refer only to the\\nstructure of aggregates, and do not take into account the data access patterns\\n455\\nof the application operations. Therefore, these strategies are not usually able to\\nsupport the performance of these operations. This motivates the introduction\\nof aggregate partitioning.\\nWe ﬁrst need to introduce a preliminary notion ofaccess path, to specify a\\n“location”in the structure ofa complexvalue. Intuitively, ifv is acomplex value460\\nand w is a value (possibly complex as well) occurring inv, then the access path\\n21'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23'}, page_content='ap for w in v represents the sequence of “steps” that should be taken to reach\\nthe component valuew in v. More precisely, an access pathap is a (possibly\\nempty) sequence ofaccess steps, ap = p1 p2 ...p n, where each steppi identiﬁes\\nac o m p o n e n tv a l u ei nas t r u c tured value. Furthermore, ifv is a complex value465\\nand ap is an access path, thenap(v) denotes the component value identiﬁed by\\nap in v.\\nFor example, consider the complex valuevmary of the Player aggregate\\nhaving username mary shown in Figure 3. Examples of access paths for this\\ncomplex value areﬁrstNameand games[0].opponent. If we apply these access470\\npaths tovmary, we access valuesMary and Player:rick, respectively.\\nA complex valuev can be represented using a set of entries, whose keys are\\naccess paths forv. Each entry is intended to represent a distinct portion of the\\ncomplex valuev, characterized by a location in its structure (the access path,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23'}, page_content='access paths forv. Each entry is intended to represent a distinct portion of the\\ncomplex valuev, characterized by a location in its structure (the access path,\\nused as entry key) and a value (the entry value). Speciﬁcally, in NoAM we475\\nrepresent each aggregate by means of apartition of its complex valuev,t h a ti s ,\\nas e tE of entries that fully coverv, without redundancy. Consider again the\\ncomplex valuevmary shown in Figure 3; a possible entry forvmary is the pair\\n⟨games[0].opponent, Player:rick⟩. We have already applied the above intuition\\nearlier in this section. For example, the ETF data representation (shown in480\\nFigure 10) uses ﬁeld names as entry keys (which are indeed a case of access\\np a t h s )a n dﬁ e l dv a l u e sa se n t r yv a l u e s .\\nAggregate partitioning can be based on the following guidelines (which are a\\nvariant of guidelines proposed in [12] in the context of logical database design):'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 22, 'page_label': '23'}, page_content='Aggregate partitioning can be based on the following guidelines (which are a\\nvariant of guidelines proposed in [12] in the context of logical database design):\\n•If an aggregate is small in size, or all or most of its data are accessed or485\\nmodiﬁed together, then it should be represented by a single entry.\\n•Conversely, an aggregate should be partitioned in multiple entries if it is\\nlarge in size and there are operationsthat frequently access or modify only\\nspeciﬁc portions of the aggregate.\\n•T w oo rm o r ed a t ae l e m e n t ss h o u l db e l o n gt ot h es a m ee n t r yi ft h e ya r e490\\nfrequently accessed or modiﬁed together.\\n22'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24'}, page_content='Game\\n2345\\nϵ\\n⟨ id:2345,\\nﬁrstPlayer:Player:mary,\\nsecondPlayer:Player:rick ⟩\\nrounds[0]\\n ⟨ moves :. . . , comments :. . . ⟩\\nrounds[1]\\n ⟨ moves :. . . , actions :. . . , spell :. . . ⟩\\nFigure 11: An alternative data representation for games ( Rounds)\\n•Two or more data elements should belong to distinct entries if they are\\nusually accessed or modiﬁed separately.\\nThe applicationof the above guidelines suggests a partitioning of aggregates,\\nwhich we will use to guide the representation in the target database.495\\nFor example, in our sample application, consider the operations involving\\ngames and rounds. When a player selects to continue a game, data on the\\nselected game should be retrieved. When a player completes a round in a game\\nshe is playing, then the aggregate for the game should be updated. To support\\nperformance, it is desirable that this update is implemented in the database\\n500\\njust as an addition of a round to a game, rather than a complete rewrite of the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24'}, page_content='performance, it is desirable that this update is implemented in the database\\n500\\njust as an addition of a round to a game, rather than a complete rewrite of the\\nwhole game. Thus, data for each individual round is always read or written\\ntogether. Moreover, data for the various rounds of a game are read together,\\nbut each round is written separately. Therefore, each roundis a candidate to\\nbe represented by an autonomous entry. These observations lead to a data\\n505\\nrepresentation for games shown in Figure 8. However, apart from rounds, the\\nremaining data for each game comprises just a few ﬁelds, which can be therefore\\nrepresented together in a single entry. This further observation leads to an\\nalternative data representation for games, shown in Figure 11.\\n4.3. Implementation\\n510\\nWe now discuss how a NoAM data representation can be implemented in\\na target NoSQL database. Given that NoAM generalizes the features of the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 23, 'page_label': '24'}, page_content='4.3. Implementation\\n510\\nWe now discuss how a NoAM data representation can be implemented in\\na target NoSQL database. Given that NoAM generalizes the features of the\\nvarious NoSQL systems, while keeping their major aspects, it is rather straight-\\nforward to perform this activity. We have implementations for various NoSQL\\n23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25'}, page_content='systems, including Cassandra, Couchbase, Amazon DynamoDB, HBase, Mon-515\\ngoDB, Oracle NoSQL, and Redis. For the sake of space, we discuss the im-\\nplementation only with respect to a single representative system for each main\\nNoSQL category. Moreover, with reference to the same aggregate objects of\\nFigures 2 and 3 we will sometimes show only the data for one aggregate. Sim-\\nilar representations can be obtained for the other aggregates of the running\\n520\\nexample.\\n4.3.1. Key-value store: Oracle NoSQL\\nIn the key-value store Oracle NoSQL [23] (Section 2.3), a data representa-\\ntion D for an application dataset can be implemented as follows. We use a\\nkey-value pair for each entry⟨ek,ev⟩ in D. The major key is composed of the525\\ncollection name C and the block keyid, while the minor key is a proper cod-\\ning of the entry keyek (recall that ek is an access path, which we represent\\nusing a distinct key component for each of its steps). An example of key is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25'}, page_content='ing of the entry keyek (recall that ek is an access path, which we represent\\nusing a distinct key component for each of its steps). An example of key is\\n/Player/mary/-/ﬁrstName,w h e r es y m b o l/ separates components, and symbol\\n- separates the major key from the minor key. The value associated with this530\\nkey is a representation of the entry valueev; for example,Mary.T h ev a l u ec a n\\nbe either simple or a serialization of a complex value, e.g., in JSON.\\nThe retrieval of a block can be implemented, in an eﬃcient and atomic way,\\nusing a singlemultiGet operation — this is possible because all the entries of a\\nblock share the same major key. The storage of a block can be implemented535\\nusing variousput operations. These multipleput operations can be executed in\\nan atomic way — since, again, all the entries of a block share the same major\\nkey.\\nFor example, Figure 4(b) shows the implementation in Oracle NoSQL of the'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 24, 'page_label': '25'}, page_content='an atomic way — since, again, all the entries of a block share the same major\\nkey.\\nFor example, Figure 4(b) shows the implementation in Oracle NoSQL of the\\ndata representation of Figure 8. Moreover, Figure 4(a) shows the implementa-\\n540\\ntion in Oracle NoSQL of the EAO data representation of Figure 9.\\nAnimplementationcanbeconsidered eﬀectiveifaggregatesareindeedturned\\ninto units of data access and distribution.The eﬀectiveness of our implementa-\\ntion is based on the use we make of Oracle NoSQL keys, where the major key\\n24'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26'}, page_content='controls distribution (sharding is based on it) and consistency (an operation in-545\\nvolving multiple key-value pairs can be executed atomically only if the various\\npairs are over a same major key).\\nMore precisely, a technical precaution is needed toguarantee atomic con-\\nsistency when the selected data representation uses more than one entry per\\nblock. Consider two separate operations that need to update just a subset of550\\nthe entries of the block for an aggregateobject. Since aggregatesshould be units\\nof atomicity and consistency, if these operations are requested concurrently on\\nthe same aggregate object, then the application would require that the NoSQL\\nsystem identiﬁes a concurrency collision, commits only one of the operations,\\nand aborts the other. However, if the operations update twodisjoint subsets\\n555\\nof entries, then Oracle NoSQL is unable to identify the collision, since it has\\nno notion of block. We support this requirement, thus providing atomicity and'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26'}, page_content='555\\nof entries, then Oracle NoSQL is unable to identify the collision, since it has\\nno notion of block. We support this requirement, thus providing atomicity and\\nconsistency over aggregates, by always including in each update operation the\\naccess to the entry that includes the identiﬁer of the aggregate (or some other\\ndistinguished entry of the block).\\n560\\n4.3.2. Extensible record store: DynamoDB\\nIn the extensible record store Amazon DynamoDB ([26], Section 2.5), the\\nimplementation of a NoAM database can be based on a distinct table for each\\ncollection, and a single item for each block. The item contains a number of\\nattributes, which can be deﬁned from the entries of the block for the item.565\\nA NoAM datarepresentationD can be representedin DynamoDB as follows.\\nConsider a blockb in a collectionC having block keyid. According toD,o n e\\nor multiple entries are used within each block. We use all the entries of a block'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 25, 'page_label': '26'}, page_content='Consider a blockb in a collectionC having block keyid. According toD,o n e\\nor multiple entries are used within each block. We use all the entries of a block\\nb to create a new item in a table forb. Speciﬁcally, we proceed as follows: (i)\\nthe collection nameC is used as a DynamoBD table name; (ii) the block key\\n570\\nid is used as a DynamoBD primary key in that table; (iii) the set of entries\\n(key-value pairs) of a blockb is used as the set of attribute name-value pairs\\nin the item forb (a serialization of the values is used, if needed). For example,\\nFigure 7 shows the implementation of the NoAM database of Figure 8.\\n25'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 26, 'page_label': '27'}, page_content='col lection Player\\nid\\n document\\nmary\\n{\\nid:”mary”,\\nusername:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames:\\n[ { game:”Game:2345”, opponent: ”Player:rick”},\\n{ game:”Game:2611”, opponent: ”Player:ann”} ]\\n}\\nFigure 12: Implementation in MongoDB\\nThe retrieval ofa block, given its collectionC and block keyid,c a nb ei m p l e -575\\nmented by performing a singlegetItem operation, which retrieves the item that\\ncontains all the entries of the block. The storage of a block can be implemented\\nusing a putItem operation, to save all the entries of the block, in an atomic way.\\nIt is worth noting that, using operationgetItem,i ti sa l s op o s s i b l et or e t r i e v ea\\nsubset of the entries of a block. Similarly, using operationupdateItem, it is also580\\npossible to update just a subset of the entries of a block, in an atomic way.\\nThis implementation is also eﬀective, since DynamoDB controls distribution\\nand atomicity with reference to items.\\n4.3.3. Document store: MongoDB'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 26, 'page_label': '27'}, page_content='This implementation is also eﬀective, since DynamoDB controls distribution\\nand atomicity with reference to items.\\n4.3.3. Document store: MongoDB\\nIn MongoDB ([29], Section 2.4), which is a document store, a natural imple-585\\nmentation for a NoAM database can be based on a distinct MongoDB collection\\nfor each collection of blocks, and a single main document for each block. The\\ndocument for a blockb can be deﬁned as a suitable JSON/BSON serialization\\nof the complex value of the entries inb, plus a special ﬁeld to store the block\\nkey id of b, as required by MongoDB,{\\n id:id}.590\\nWith reference to a NoAM data representationD, consider a blockb in a\\ncollectionC havingblockkey id.I fbcontainsjust anentry e, then the document\\nfor b is just a serialization ofe.O t h e r w i s e ,i fb contains multiple entries, we use\\nall the entries in blockb to create a new document. Speciﬁcally, we proceed by\\n26'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28'}, page_content='col lection Player\\nid\\n document\\nmary\\n{\\nid:”mary”,\\nusername:”mary”,\\nﬁrstName:”Mary”,\\nlastName:”Wilson”,\\ngames[0]: { game:”Game:2345”, opponent: ”Player:rick” },\\ngames[1]: { game:”Game:2611”, opponent: ”Player:ann” }\\n}\\nFigure 13: Alternative implementation in MongoDB\\nbuilding a documentd for b as follows: (i) the collection nameC is used as the595\\nMongoDB collection name; (ii) the block keyid is used for the special top-level\\nid ﬁeld{\\n id:id} of d; (iii) then, each entry in the blockb is used to ﬁll a (possibly\\nnested) ﬁeld of documentd. See Figure 12.\\nTheretrievalofablock, givenitscollection C andkey id, canbeimplemented\\nby performing aﬁnd operation, to retrieve the main document that represents600\\nall the block (with its entries). The storage of a block can be implemented using\\nan insert operation, which saves the whole block (with its entries), in an atomic\\nway. It is worth noting that, using other MongoDB operations, it is also possible'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28'}, page_content='an insert operation, which saves the whole block (with its entries), in an atomic\\nway. It is worth noting that, using other MongoDB operations, it is also possible\\nto access and update just a subset of the entries of a block, in an atomic way.\\nAn alternative implementation for MongoDB is as follows. Each block b\\n605\\nis represented, again, as a main document forb, but using a distinct top-level\\nﬁeld-value pair for each entry in the NoAM data representation. In particular,\\nfor each entry (ek,ev), the document forb contains a top-level ﬁeld whose name\\nis a coding for the entry key (access path)ek, and whose value is either an\\natomic value or an embedded document that serializes the entry valueev.F o r610\\nexample, according to this implementation, the data representation of Figure 8\\nleads to the result shown in Figure 13.\\n4.4. Experiments\\nWe will now discuss a case study of NoSQL database design, with refer-\\nence to our running example. For the sake of simplicity, we just focus on the\\n615'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 27, 'page_label': '28'}, page_content='4.4. Experiments\\nWe will now discuss a case study of NoSQL database design, with refer-\\nence to our running example. For the sake of simplicity, we just focus on the\\n615\\n27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29'}, page_content='representation and management of aggregates for games.\\nData for each game include a few scalar ﬁelds and a collection of rounds.\\nThe important operations over games are: (1) the retrieval of a game, which\\nshould read all the data concerning the game; and (2) the addition of a round\\nto a game.620\\nAssume that, to manage games, we have chosen a key-value store as the\\ntarget system. The candidate data representations are: (i) using a single entry\\nfor each game (as shown in Figure 9, in the following calledEAO); (ii) splitting\\nthe data for each game in a group of entries, one for each round, and including\\nall the remaining scalar ﬁelds in a separate entry (as shown in Figure 11, called\\n625\\nRounds).\\nWe expect that the ﬁrst operation (retrieval of a game) performs better in\\nEAO, since it needs to read just a key-valuepair, while the second one (addition\\nof a round to a game) is favored byRounds, which does not require to rewrite\\nthe whole game.630'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29'}, page_content='EAO, since it needs to read just a key-valuepair, while the second one (addition\\nof a round to a game) is favored byRounds, which does not require to rewrite\\nthe whole game.630\\nWe ran a number of experiments to compare the above data representations\\nin situations of diﬀerent application workloads. Each game has, on average, a\\ndozen rounds, for a total of about 8KB per game. At each run, we simulated\\nthe following workloads: (a) game retrievals only (in random order); (b) round\\nadditions only (to random games); and (c) a mixed workload, with game re-\\n635\\ntrieval and round addition operations, with a read/write ratio of 50/50. We ran\\nthe experiments using diﬀerent database sizes, and measured the running time\\nrequiredby the workloads. The targetsystem was Oracle NoSQL,deployed over\\nAmazon AWS on a cluster of four EC2 servers.\\n1\\nThe results are shown in Figure 14. Database sizes are in gigabytes, timings640\\nare in milliseconds, and points denote the average running time of a single op-'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 28, 'page_label': '29'}, page_content='1\\nThe results are shown in Figure 14. Database sizes are in gigabytes, timings640\\nare in milliseconds, and points denote the average running time of a single op-\\neration. The experiments conﬁrm the intuition that the retrieval of games (Fig-\\nure 14(a)) is always favored by theEAO data representation, for any database\\nsize. On the other hand, the addition of a round to an existing game (Fig-\\n1 This activity was supported by A WS in Education Grant award.\\n28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 29, 'page_label': '30'}, page_content='0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nGame Retrieval\\nEAO Rounds\\n(a) Game Retrieval\\n0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nRound Addition\\nEAO Rounds\\n(b) Round Addition\\n0,00\\n0,05\\n0,10\\n0,15\\n0,20\\n0,25\\n0,30\\n0,35\\n0,40\\n0,45\\n0,50\\n1 2 4 8 16 32 64 128 256 512\\nMixed Load (50/50)\\nEAO Rounds\\n(c) Mixed Load\\nFigure 14: Experimental results\\nure 14(b)) is favored by theRounds data representation. Finally, the exper-645\\niments over the mixed workload (Figure 14(c)) show a general advantage of\\nRounds over EAO, which however decreases as the database size increases.\\nOverall, it turns out that theRounds data representation is preferable.\\nWe also performed other experiments on a data representation that does\\nnot conform to the design guidelines proposed in this paper. Speciﬁcally, a data650\\nrepresentation that divides the rounds of a game into independent key-value'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 29, 'page_label': '30'}, page_content='not conform to the design guidelines proposed in this paper. Speciﬁcally, a data650\\nrepresentation that divides the rounds of a game into independent key-value\\npairs, rather than keeping them together in a same block, as suggested by our\\napproach. In this case, the performance of the various operations worsens by at\\nleast an order of magnitude. Moreover, with this data representation it is not\\npossible to update a game in an atomic way.\\n655\\nOverall, these experiments show that: (i) the design of NoSQL databases\\nshould be done with care as it aﬀects considerably the performance and consis-\\n29'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31'}, page_content='tency of data access operations, and (ii) our methodology provides an eﬀective\\ntool for choosing among diﬀerent alternatives.\\n5. Related works660\\nAlthough several authors have observed that there is a need for data-model\\napproaches to the design and management of NoSQL databases [9, 10, 11],\\nvery few works have addressed this issue,especially from a general and system-\\nindependent point of view. Indeed, most of them propose a solution to a speciﬁc\\nproblem in a limited scenario.\\n665\\nFor instance, Pasqualin et al. have recently shown how a document-oriented\\nmodel can be eﬃciently implemented in a NoSQL document store [30]. Sim-\\nilarly, Olivera et al. [31] and de Lima and Mello [32] have proposed a data-\\nmodel based methodology for the design of NoSQL document database [32],\\nwhereas Chevalier et al. have addressed the speciﬁc problem of leveraging on\\n670\\na document-oriented model for implementing a multidimensional database in a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31'}, page_content='whereas Chevalier et al. have addressed the speciﬁc problem of leveraging on\\n670\\na document-oriented model for implementing a multidimensional database in a\\nNoSQL document store [33] and in a column-oriented NoSQL database [34].\\nMost of the other contributions to data modeling for NoSQL systems come\\nfrom on-line papers, usually published in blogs of practitioners, that discuss\\nbest practices and guidelines for modeling NoSQL databases, most of which\\n675\\nare suited only for speciﬁc systems. For instance, [5] lists some techniques for\\nimplementing and managing data stored in diﬀerent types of NoSQL systems,\\nwhile [35] discusses design issues for the speciﬁc case of key-value datastores.\\nSimilarly, Mior et al. [36] have recently proposed an approach to the problem of\\nschema design for the speciﬁc class of extensible record stores. On the system-\\n680\\noriented side, [6, 7, 8] illustrate design principles for the speciﬁc cases of HBase,'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 30, 'page_label': '31'}, page_content='schema design for the speciﬁc class of extensible record stores. On the system-\\n680\\noriented side, [6, 7, 8] illustrate design principles for the speciﬁc cases of HBase,\\nMongoDB, and Cassandra, respectively. However, none of them tackles the\\nproblem from a general perspective, as we advocate in this paper.\\nRecently, Ruiz et al. have proposed a reverse engineering strategy aimed at\\ninferring the implicit schema of NoSQL databases [37]. This approach supports685\\nthe idea that, even in this context, a model-baseddescriptionofthe organization\\n30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32'}, page_content='of data is very useful during the entire life-cycle of a data set.\\nTo the best of our knowledge, this paper presents the ﬁrst general design\\nmethodology for NoSQL systems with initial activities that are independent of\\nthe speciﬁc target system. Our approach to data modeling is based on data690\\naggregates, a notion that is central in NoSQL databases where application data\\nare grouped in atomic units that are accessed and manipulated together [3].\\nThe notion of aggregate also occurs in other contexts with a similar meaning.\\nFor example, in Domain Driven Design [19], a widely followed object-oriented\\nsoftware development approach, an aggregate is a group of related application\\n695\\nobjects, used to govern transactions and distribution. Also Helland [20] advo-\\ncates the use of aggregates (there called entities) as units of distribution and\\nconsistency. In this framework, Baker et al. [38] propose the notion of entity'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32'}, page_content='cates the use of aggregates (there called entities) as units of distribution and\\nconsistency. In this framework, Baker et al. [38] propose the notion of entity\\ngroups, a set of entities that can be manipulated in an atomic way. They also\\ndescribe a speciﬁc mapping of entity groups to Bigtable [22], which however700\\nmakes the approach targeted only to a speciﬁc NoSQL system. Our approach is\\nbased on a more abstract database model, NoAM, and is system independent,\\nas it is targeted to a wide class of NoSQL systems.\\nThe issue of identifying data accessunits in database design shows some\\nsimilarities with problems studied in the past, such as: (i) the early works\\n705\\non vertical partitioning and clustering [39], with the idea to put together the\\nattributes that are accessed together and to separate those that are visited\\nindependently, and (ii) the more recent approaches to relational (or object-\\nrelational) storage of XML documents [40], where various alternatives obviously'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 31, 'page_label': '32'}, page_content='independently, and (ii) the more recent approaches to relational (or object-\\nrelational) storage of XML documents [40], where various alternatives obviously\\nexist, with tables that can be very small and handle individual edges, or very710\\nwide and handle entire paths, and many alternatives in between.\\nA major observation from [9] is that the availability of a high-level represen-\\ntation of the data remains a fundamental tool for developers and users, since it\\nmakes understanding, managing, accessing, and integrating information sources\\nmuch easier, independently of the technologies used. We have addressed this715\\nissue by proposing NoAM, an abstract data model that makes it possible to\\ndevise an initial phase of the design process that is independent of any speciﬁc\\n31'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 32, 'page_label': '33'}, page_content='system but suitable for each.\\nAlong this line, SOS [41] is a tool that provides a common programming\\ninterface towards diﬀerent NoSQL systems, to access them in a uniﬁed way.720\\nThe interface is based on a simple, high-level common data model which is\\ninspired by those of non-relational systems and provides simple operations for\\ninserting, deleting, and retrieving database objects. However, the deﬁnition of\\ntools for data access is complementary to data models and design issues.\\nFinally, Jain et al. discusses the potential mismatch between the require-725\\nments of scientiﬁc data analysis and the models and languages of relational\\ndatabase systems [42], whereas Alagiannis et al. [43] advocate a new database\\ndesign philosophy for emerging applications. This paper tries to provide a con-\\ntribution to these problems.\\n6. Conclusion\\n730\\nIn this paper we have argued how data modeling can be useful in the No-\\nSQL arena. Speciﬁcally, we have proposed a comprehensive methodology for'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 32, 'page_label': '33'}, page_content='tribution to these problems.\\n6. Conclusion\\n730\\nIn this paper we have argued how data modeling can be useful in the No-\\nSQL arena. Speciﬁcally, we have proposed a comprehensive methodology for\\nthe design of NoSQL databases, which relies on an aggregate-oriented view of\\napplication data, an intermediate system-independent data model for NoSQL\\ndatastores, and ﬁnally an implementation activity that takes into account the735\\nfeatures of speciﬁc systems.\\nReferences\\n[1] F. Bugiotti, L. Cabibbo, P. Atzeni, R. Torlone, Database design for NoSQL\\nsystems, in: Conceptual Modeling - 33rd International Conference, ER\\n2014, Atlanta, GA, USA, October 27-29, 2014. Proceedings, 2014, pp. 223–740\\n231.\\n[2] R. Cattell, Scalable SQL and NoSQL data stores, SIGMOD Record 39 (4)\\n(2010) 12–27.\\n[3] P. J. Sadalage, M. J. Fowler, NoSQL Distilled, Addison-Wesley, 2012.\\n32'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 33, 'page_label': '34'}, page_content='[4] M. Stonebraker, Stonebraker on NoSQL and enterprises, Comm. ACM745\\n54 (8) (2011) 10–11.\\n[5] I. Katsov, NoSQL data modeling techniques, Highly Scalable Blog,\\nhttps://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/,\\naccessed February 2016 (2012).\\n[6] A. Khurana, Introduction to HBase Schema Design, ;login: The Usenix750\\nmagazine 37 (5) (2012) 29–36.\\n[7] M. Hamrah, Data Modeling at Scale: MongoDB + Mon-\\ngoid, Callbacks, and Denormalizing Data for Eﬃciency,\\nhttp://blog.michaelhamrah.com/2011/08/data-modeling-at-scale-mongodb-mongoid-callbacks -\\n(Accessed February, 2016) (2011).755\\n[8] A. Chebotko, A. Kashlev, S. Lu, A Big Data Modeling Methodology for\\nApache Cassandra, in: IEEE International Congress on Big Data, 2015,\\npp. 238–245.\\n[9] P. Atzeni, C. S. Jensen, G. Orsi, S. Ram, L. Tanca, R. Torlone, The rela-\\ntional model is dead, SQLis dead, and I don’t feel sogoodmyself, SIGMOD\\n760\\nRecord 42 (2) (2013) 64–68.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 33, 'page_label': '34'}, page_content='pp. 238–245.\\n[9] P. Atzeni, C. S. Jensen, G. Orsi, S. Ram, L. Tanca, R. Torlone, The rela-\\ntional model is dead, SQLis dead, and I don’t feel sogoodmyself, SIGMOD\\n760\\nRecord 42 (2) (2013) 64–68.\\n[10] A. Badia, D. Lemire, A call to arms: revisiting database design, SIGMOD\\nRecord 40 (3) (2011) 61–69.\\n[11] C. Mohan, History repeats itself: sensible and NonsenSQL aspects of the\\nNoSQL hoopla, in: EDBT, 2013, pp. 11–16.765\\n[12] C. Batini, S. Ceri, S. B. Navathe, Conceptual Database Design: An Entity-\\nRelationship Approach, Benjamin/Cummings, 1992.\\n[13] F. Bancilhon, Object-oriented database systems, in: Proceedings of the\\nSeventh ACM SIGACT-SIGMOD-SIGART Symposium on Principles of\\nDatabase Systems, March 21-23, 1988, Austin, Texas, USA, 1988, pp. 152–770\\n162.\\n33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 34, 'page_label': '35'}, page_content='[14] P. Atzeni, P. Merialdo, G. Mecca, Data-intensive web sites: Design and\\nmaintenance, World Wide Web 4 (1-2) (2001) 21–47.\\n[15] S. Ceri, P. Fraternali, A. Bongio, M. Brambilla, S. Comai, M. Matera,\\nDesigning Data-Intensive Web Applications, Morgan Kaufmann, 2003.775\\n[16] G. Mecca, A. O. Mendelzon, P. Merialdo, Eﬃcient queries over web views,\\nIEEE Trans. Knowl. Data Eng. 14 (6) (2002) 1280–1298.\\n[17] S. Abiteboul, P. Buneman, D. Suciu, Data on the Web: From Relations to\\nSemistructured Data and XML, Morgan Kaufmann, 1999.\\n[18] M. Stonebraker, U. C¸etintemel, “one size ﬁts all”: An idea whose time780\\nhas come and gone (abstract), in: Proceedings of the 21st International\\nConferenceonDataEngineering,ICDE2005,5-8April2005,Tokyo,Japan,\\n2005, pp. 2–11.\\n[19] E. Evans, Domain-Driven Design, Addison-Wesley, 2003.\\n[20] P. Helland, Life beyond distributed transactions: an apostate’s opinion, in:785\\nCIDR 2007, 2007, pp. 132–141.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 34, 'page_label': '35'}, page_content='2005, pp. 2–11.\\n[19] E. Evans, Domain-Driven Design, Addison-Wesley, 2003.\\n[20] P. Helland, Life beyond distributed transactions: an apostate’s opinion, in:785\\nCIDR 2007, 2007, pp. 132–141.\\n[21] S. Abiteboul, R. Hull, V. Vianu, Foundations of Databases, Addison-\\nWesley, 1995.\\n[22] F. Chang, et al., Bigtable: A distributed storage system for structured\\ndata, ACM Trans. Comput. Syst. 26 (2).790\\n[23] Oracle,OracleNoSQLDatabase, http://www.oracle.com/us/products/database/nosql/,\\naccessed February 2016.\\n[24] J. Shute, et al., F1: A distributed SQL database that scales, PVLDB 6 (11)\\n(2013) 1068–1079.\\n[25] MongoDB Inc., MongoDB, http://www.mongodb.org, accessed February795\\n2016.\\n34'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 35, 'page_label': '36'}, page_content='[26] Amazon Web Services, DynamoDB, http://aws.amazon.com/dynamodb,\\naccessed February 2016.\\n[27] D. Pritchett, BASE: An ACID alternative, ACM Queue 6 (3) (2008)48–55.\\n[28] V. Vernon, Implementing Domain-Driven Design, Addison-Wesley, 2013.800\\n[29] K. Chodorow, MongoDB: The Deﬁnitive Guide, O’Reilly Media, 2013.\\n[ 3 0 ]D .P a s q u a l i n ,G .S o u z a ,E .L .B u r a t t i ,E .C .d eA l m e i d a ,M .D .D e lF a b r o ,\\nD. Weingaertner, A case study of the aggregation query model in read-\\nmostly NoSQL document stores, in: 20th Int. Database Engineering &\\nApplications Symposium (IDEAS ’16), IDEAS ’16, ACM, New York, NY,805\\nUSA, 2016, pp. 224–229.\\n[31] H. V. Olivera, M. Holanda, V. Guimarˆaes, F. Hondo, W. Boaventura, Data\\nmodeling for NoSQL document-oriented databases, in: 2nd Annual Int.\\nSymposium on Information Management and Big Data (SIMBig 2015),\\nVol. 1478 of CEUR Workshop Proceedings, 2015, pp. 129–135.\\n810\\n[32] C.de Lima, R.dos SantosMello, Aworkload-drivenlogicaldesignapproach'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 35, 'page_label': '36'}, page_content='Symposium on Information Management and Big Data (SIMBig 2015),\\nVol. 1478 of CEUR Workshop Proceedings, 2015, pp. 129–135.\\n810\\n[32] C.de Lima, R.dos SantosMello, Aworkload-drivenlogicaldesignapproach\\nfor NoSQL document databases, in: 17th Int. Conference on Information\\nIntegration and Web-based Applications & Services (iiWAS ’15), iiWAS\\n’15, ACM, New York, NY, USA, 2015, pp. 73:1–73:10.\\n[33] M. Chevalier, M. E. Malki, A. Kopliku, O. Teste, R. Tournier, Implemen-\\n815\\ntation of multidimensional databases with document-oriented NoSQL, in:\\n17th International Conference on Big Data Analytics and Knowledge Dis-\\ncovery, (DaWaK 2015), Vol. 9263 of Lecture Notes in Computer Science,\\nSpringer, 2015, pp. 379–390.\\n[34] M. Chevalier, M. E. Malki, A. Kopliku, O. Teste, R. Tournier, Implementa-820\\ntion of multidimensional databases in column-oriented NoSQL systems, in:\\n19th EastEuropeanConference on Advances in Databases and Information\\nSystems (ADBIS 2015), 2015, pp. 79–91.\\n35'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 36, 'page_label': '37'}, page_content='[35] T. Olier, Database design using key-value tables,\\nhttp://www.devshed.com/c/a/mysql/database-design-using-key-value-tables/,825\\naccessed February 2016 (2006).\\n[36] M. J. Mior, K. Salem, A. Aboulnaga, R. Liu, Nose: Schema designfor nosql\\napplications, in: 32ndIEEEInternationalConferenceonData Engineering,\\nICDE 2016, Helsinki, Finland, May 16-20, 2016, 2016, pp. 181–192.\\n[37] D. S. Ruiz, S. F. Morales, J. G. Molina, Inferring Versioned Schemas from830\\nNoSQL Databases and Its Applications, in: 34th International Conference\\non Conceptual Modeling (ER 2015), 2015, pp. 467–480.\\n[38] J. Baker, et al., Megastore: Providing scalable, highly available storage for\\ninteractive services, in: CIDR 2011, 2011, pp. 223–234.\\n[39] T. J. Teorey, J. P. Fry, The logical record access approach to database835\\ndesign, ACM Comput. Surv. 12 (2) (1980) 179–211.\\n[40] D. Florescu, D. Kossmann, Storing and querying XML data using an\\nRDMBS, IEEE Data Eng. Bull. 22 (3) (1999) 27–34.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'Elsevier', 'creationdate': '2016-10-15T16:41:06+05:30', 'author': 'Paolo Atzeni', 'elsevierwebpdfspecifications': '6.5', 'keywords': 'Data models; Database design; NoSQL systems', 'moddate': '2016-10-15T16:41:06+05:30', 'subject': 'Computer Standards & Interfaces,Accepted manuscript,doi:10.1016/j.csi.2016.10.003', 'title': 'Data Modeling in the NoSQL World', 'doi': '10.1016/j.csi.2016.10.003', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling]_Data_Modeling_in_the_NoSQL_World.pdf', 'total_pages': 37, 'page': 36, 'page_label': '37'}, page_content='design, ACM Comput. Surv. 12 (2) (1980) 179–211.\\n[40] D. Florescu, D. Kossmann, Storing and querying XML data using an\\nRDMBS, IEEE Data Eng. Bull. 22 (3) (1999) 27–34.\\n[41] P. Atzeni, F. Bugiotti, L. Rossi, Uniform access to NoSQL systems, Inf.\\nSyst. 43 (2014) 117–133.840\\n[42] S. Jain, D. Moritz, D. Halperin, B. Howe, E. Lazowska, SQLShare: Results\\nfromamulti-yearSQL-as-a-Servicee xperiment, in: Proceedingsofthe 2016\\nInternational Conference on Management of Data, SIGMOD Conference\\n2016, San Francisco, CA, USA, June 26 - July 01, 2016, 2016, pp. 281–293.\\n[43] I. Alagiannis, R.Borovica-Gajic,M.Branco, S.Idreos, A. Ailamaki, NoDB:845\\neﬃcient query execution on raw data ﬁles, Commun. ACM 58 (12) (2015)\\n112–121.\\n36'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nIEEE TRANSACTIONS ON Knowledge and Data Engineering,  manuscript ID 1 \\n \\nHaery: a Hadoop based Query System on Accumulative and  \\nHigh-dimensional Data Model for Big Data \\nJie SONG 1, Hongyan HE 1, Richard THOMAS 2, Yubin BAO 3, Ge YU 3 \\nAbstract——Column-oriented stores, known for their scalability and flexibility, are a common NoSQL database implementation \\nand are increasingly used in big data management. In column -oriented stores, a “full -scan” query strategy is inefficient and the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='and are increasingly used in big data management. In column -oriented stores, a “full -scan” query strategy is inefficient and the \\nsearch space ca n be reduced if data is well partitioned or indexed, however there is no pre -defined schema for building and \\nmaintaining partitions and indexes at lower cost. We leverage an accumulative and high -dimensional data model, a \\nsophisticated linearization algori thm, and an efficient query algorithm, to solve the challenge of how a pre -defined and well -\\npartitioned data model can be applied to flexible and time -varied key-value data. We adapt a high-dimensional array as the data \\nmodel to partition the key -value dat a without additional storage and massive calculation; improve the Z -order linearization \\nalgorithm, which map multidimensional data to one dimension while preserving locality of the data points , for flexibility; efficiently'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='algorithm, which map multidimensional data to one dimension while preserving locality of the data points , for flexibility; efficiently \\nbuild a n expansion mechanism for the data model to support  time-varied data. The result is Haery, a column -oriented store, \\nbased on a distributed file system and computing framework. In experiments, Haery is compared with Hive, HBase, Cassandra, \\nMongoDB, PostgresXL and HyperDex in terms of query performance. With results indicat ing Haery on average performs 4.57x, \\n4.23x, 3.55x, 1.79x, 1.82x and 120.6x faster, respectively. \\nIndex Terms—Key-value data, Column-oriented store, Multi-dimensional data model, Linearization, Accumulation  \\n——————————   \\uf075   —————————— \\n1 INTRODUCTION\\nIn the big data era, traditional relational databases can no \\nlonger meet the requirements of query performance and \\nscalability [1]. Researchers are eager to find an effective way \\nto manage massive data. Column\\n-oriented stores, which are'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='longer meet the requirements of query performance and \\nscalability [1]. Researchers are eager to find an effective way \\nto manage massive data. Column\\n-oriented stores, which are \\nnewly emerged NoSQL databases, are wildly accepted by \\nboth indu stry and academia. Column -oriented stores use \\ntables, wide columns or column families as their fundamen-\\ntal data models. In these models, a record is represented as a \\ncollection of key -value pairs (the column name is the key) \\nsuch that each possible key app ears at most once in the col-\\nlection. [2]. \\nIn this paper, we focus on partition based query optimiza-\\ntions of column -oriented stores. Generally, queries can be \\noptimized by partition pruning, i.e, if data is well partitioned \\nby keys, then the query can be op timized by only scanning \\nthe matched partitions. The advantage of a column -oriented \\nstore is flexibility; it relies on a free -formed and soft schema \\nso that arbitrary key -value pairs can be stored. Unfortunate-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='the matched partitions. The advantage of a column -oriented \\nstore is flexibility; it relies on a free -formed and soft schema \\nso that arbitrary key -value pairs can be stored. Unfortunate-\\nly, because of this flexibility, column-oriented stores lack a \\npre-defined schema so key based partitions are difficult to \\nmaintain. For example  of composite partitioning, rows (rec-\\nords) do not contain same keys and new keys are freely in-\\ntroduced. Conversely, in a traditional database, the data fol-\\nlows a pre-defined and rigid schema so that it is well parti-\\ntioned, the cost of which is flexibility. Consequently a \\ntradeoff is made between normalization and flexibility of the \\nschema. \\nWe have built key -based partitions on massive key -value \\ndata without redu cing flexibility. These partitions improve \\nquery performance by greatly reducing the search space. In \\ncolumn-oriented stores, the key -value data is organized in \\ntables. Given two related key -value pairs, the relationships'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='query performance by greatly reducing the search space. In \\ncolumn-oriented stores, the key -value data is organized in \\ntables. Given two related key -value pairs, the relationships \\nbetween them are “different keys (columns) of the same rec-\\nord (row)” or “same key (column) of different records \\n(rows)”. If we segment each key by its value, and let keys be \\ndimensions, then rows can be categorized into a high -\\ndimensional data model (HDM for short). The HDM is a \\nlogical data model, by which the search space can be reduced \\nto some cells of the HDM when a query is performed. This \\ninstinctive solution challenges the flexibility of a key -value \\ndata model. The following issues need to be solved: \\n(1\\n) How to ensure the efficiency of query and storage on \\nHDM, especially given the sparsity of HDM due to the fact \\nthat keys in different rows are diverse; \\n(2\\n) Given the lack of well-formed schema, how to fit the \\nnewly imported keys;'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='HDM, especially given the sparsity of HDM due to the fact \\nthat keys in different rows are diverse; \\n(2\\n) Given the lack of well-formed schema, how to fit the \\nnewly imported keys; \\n(3) Considering the extremely large address space of \\nHDM, how to design the mapping mechanism, to linearize \\nthe HDM to storage, and to expand without changing the \\nexisting storage; \\n(4) The query algorithm and implementation. \\nIn this paper we propose Haery ( Hadoop query), a Ha-\\ndoop based query system that uses  an accumulative HDM \\nfor big data. As a query system, it highlights the query opti-\\nmization of column-oriented stores, and is based on Hadoop \\nHDFS as the storage and Hadoop MapReduce as the compu-\\nting framework  (it also support s other computing frame-\\nworks). Our contributions are as follows: \\n(1\\n) Drawing on the experience of partitions in relational \\ndatabase, we propose an accumulative HDM to partition \\nkey-value data. The HDM is \"accumulative\" in the sense that'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 0, 'page_label': '1'}, page_content='(1\\n) Drawing on the experience of partitions in relational \\ndatabase, we propose an accumulative HDM to partition \\nkey-value data. The HDM is \"accumulative\" in the sense that \\nnew keys and values can be dynamically introduced. \\n(2) We propose a linearization algorithm, as an extensible \\nand flexible improvement of the Z -order curve [3] and pro-\\nxxxx-xxxx/0x/$xx.00 © 200x IEEE        Published by the IEEE Computer Society \\n———————————————— \\n\\uf0b7 Jie SONG and Hongyan HE are with the Software College, Northeastern \\nUniversity, Shenyang, 110819, China. E -mail: songjie@mail.neu.edu.cn. \\nand  2322710332@qq.com. \\n\\uf0b7 Richard THOMAS is with School of Information Technology and Electrical \\nEngineering, The University of Queensland, Queensland, Australia  \\n\\uf0b7 Yunbing Bao and Ge YU are with the School of Computer Science and \\nEngineering, Northeastern University, Shenyang, 110819, China. E -mail: \\nbaoyb@mail.neu.edu.cn and yuge@mail.neu.edu.cn.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n2 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\npose an address tree, to map the logical data model to physi-\\ncal addresses. \\n (3) Haery, as a system, contributes to the implementation \\nexperience of NoSQL databases. \\nFrom the aspect of query optimization, the differences be-\\ntween Haery and other partitioned or indexed NoSQL data-\\nbases are as follows: \\n(1) Partitioning techniques face challenges of maintaining \\nflexibility and extensibility. Haery adopts a schema -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='tween Haery and other partitioned or indexed NoSQL data-\\nbases are as follows: \\n(1) Partitioning techniques face challenges of maintaining \\nflexibility and extensibility. Haery adopts a schema -\\ndependent partition approach which has a finer granularity \\nthan sharding techniques  [4], and is easy to maintain and \\nextend. \\n(2) Indexing techniques face challenges of large storage \\nand search cost in a big data environment. Compared with \\nindexing, Haery does not depend on any pre -computation \\nand materialization techniques, so the storage cost is very \\nlow. Haery also prefers calculation to sea rching, avoiding \\ncomplexity explosion in a big data environment.  \\n(3) HDM in Haery is easier to maintain than partitions \\nand indexes \\nThis paper evaluates the core algorithm of Haery, com-\\nparing query performance, loading performance and storage \\ncost of Haery with t hose of other data stores, such as Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex .'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='paring query performance, loading performance and storage \\ncost of Haery with t hose of other data stores, such as Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nThe results show that Haery has a marked advantage in que-\\nry performance regardless of the data size and query com-\\nplexity. The extra calculation required during data loading is \\nless than these other stores, and the additional storage cost is \\nnegligible. \\nThe rest of this paper is organized as follows. Section 2 \\noverviews related work. Section 3 provides a detailed de-\\nscription of the HDM and section 4 explains the flat  Z-order \\nlinearization and addressing algorithms. Section 5 describes \\nthe accumulation strategy and section 6\\n describes the query \\napproaches. Section 7 briefly introduces the system architec-\\nture of Haery, explaining each component of the system. \\nSection 8 evaluates the loading and querying performance of \\nHaery, comparing these with of the performance of Hive,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='ture of Haery, explaining each component of the system. \\nSection 8 evaluates the loading and querying performance of \\nHaery, comparing these with of the performance of Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nFinally, conclusions and future works are summarized in \\nsection 9. \\n2 RELATED WORK \\nIn recent years, column-oriented stores which store arbitrary \\nkey-value pairs, ha ve attracted much attention in big data \\nresearch. Research on indexing techniques and on schema or \\nlogical data models of column -oriented stores are related to \\nHaery.  \\nWu et al. [ 5] proposes a two -level indexing framework \\nwhich contains local and global indexes. The local index \\nmaintains information on each node. The global index con-\\nsists of selected local indexes in order to save storage and \\nimprove query efficiency. The proposed Ef ficient B -tree \\nadopts a B+ tree to build the local index and a cost model -\\nbased adaptive strategy to select local indexes. The global'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='improve query efficiency. The proposed Ef ficient B -tree \\nadopts a B+ tree to build the local index and a cost model -\\nbased adaptive strategy to select local indexes. The global \\nindex is organized as a BATON network [6]. However, it is \\noptimized for only querying single attributes, not for multi -\\ndimensional queries. Similar to Efficient B -tree, RT-CAN [7] \\nadopts an R-tree to build a local index and a CAN network to \\nbuild a global index. These optimizations are based on a \\npeer-to-peer structure. Centralized EMINC [8] and A-tree [9] \\nindexes are propose d in a master -sla\\nve structure which is \\nalso widely accepted in big data environments. While \\nproviding query performance optimization, the cost is huge \\nto maintain and rebuild indexes. \\nITHBase [10] and IHBase [11] employ secondary indexes. \\nBoth build an index table based on index keys. The key-value \\npairs in the index table are retrieved from the original data \\nset. When a query is performed, the index table is accessed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='Both build an index table based on index keys. The key-value \\npairs in the index table are retrieved from the original data \\nset. When a query is performed, the index table is accessed \\nfirst, then the matched data is located by the mappings be-\\ntween the index table and indexe d keys. Secondary indexes \\nbenefit query performance when indexed keys are contained \\nin the query conditions. To reduce the cost of random query-\\ning, Zou et al. [12] proposed CCIndex which stores original \\ndata in an index table. When a query is performed, it  only \\nscans the index table without looking up the mappings be-\\ntween the index table and indexed keys. However, it is diffi-\\ncult to guarantee fault tolerance and consistency. A second-\\nary index is easier to maintain than two -level indexes. Both \\ntwo-level indexes and secondary indexes are based on specif-\\nic index structures, which are difficult to maintain when the \\ndata set is updated frequently. They are also not efficient'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content='two-level indexes and secondary indexes are based on specif-\\nic index structures, which are difficult to maintain when the \\ndata set is updated frequently. They are also not efficient \\nwhen querying high-dimensional data due to the indexes on \\nmany columns are extreme huge.  \\nMulti-dimensional indexes have been studied extensively. \\nCommon multi-dimensional indexes include Grid Index [13], \\nKD-trees [14] and R-Trees [15]. They divide the space into a \\nk-dimensional space which consists of grids, and allocate \\naddresses for each grid. Query results can be found by locat-\\ning specific grids, which is faster than the original query \\nstrategy. Based on this idea, MD- HBase [16] and SHG -tree \\n[17] were proposed. The storage of SHG\\n-tree is inefficient \\ndue to the sparsity of high-dimensional data, and MD-HBase \\nhas weaknesses in data consistency, and brings extra costs \\nand latency when data is unevenly distributed. Many re-\\nsearchers have come up with new index structures to satisfy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content=\"has weaknesses in data consistency, and brings extra costs \\nand latency when data is unevenly distributed. Many re-\\nsearchers have come up with new index structures to satisfy \\ndifferent requirements. VAR-tree [18] aims to improve query \\nperformance by compression; CSA -tree [19] focuses on \\nmemory search; NV-tree [20] facilitates near inquiries, and so \\non. \\nThe optimization of schemas of non -relational databases \\nalso contributes to the query performance. Mior et al. [2 1\\n] \\npresented a system for re commending database schemas for \\nNoSQL applications. Automating the design process allows \\nthe proposed prototype, NoSQL Schema Evaluator (NoSE), \\nto produce efficient schemas and to examine more alterna-\\ntives than would be possible with a manual rule -based ap-\\nproach. They proposed a cost -based approach by a novel \\nbinary integer programming formulation to guide the map-\\nping from the application's conceptual data model to a data-\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 1, 'page_label': '2'}, page_content=\"proach. They proposed a cost -based approach by a novel \\nbinary integer programming formulation to guide the map-\\nping from the application's conceptual data model to a data-\\nbase schema, while we propose  the logic data model map-\\nping the database schema  to the physical dataset for query \\noptimization.Vajk et al. [2 2] defined some problems of sche-\\nma design in NoSQL database by compared with material-\\nized views in relational databases, and proposed a cost driv-\\nen model, which can optimize the mapping from the applica-\\ntion data model to a physical schema. They start with a nor-\\nmalized data schema, then identified a column store schema \\nwhich can serve the queries with minimal cost. However, it\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 3 \\n \\nis to optimize for monetary cost (storage and query) only \\ninstead of co nsidering performance, also the cost model is \\nusually not the case, which maybe overvalue the inexpensive \\nschema options with poor performance. And the solution is \\nnot general enough, it is only efficient for predefined queries.  \\nHyperDex [23,24] is a cloud database. The key insight be-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='schema options with poor performance. And the solution is \\nnot general enough, it is only efficient for predefined queries.  \\nHyperDex [23,24] is a cloud database. The key insight be-\\nhind HyperDex is the concept of hyperspace hashing in \\nwhich objects with multiple attributes are mapped into a \\nmultidimensional hyperspace. Haery’s initial idea is close to \\nHyperDex, however, the two systems have essential dif fer-\\nences. HyperDex relays on a predefined schema  of objects, \\nmodels the objects with same attributes into tables, divides \\nattributes of the table into different partitions, duplicat es \\nobjects of the table to every partition, builds the hyperspace \\nof each partition according to order ed hash values of its at-\\ntributes, tessellats the hyperspace into regions, assigns the \\nregions to servers, and stores the mappings as data in the \\ncorrdinator server. As the comparsion, Haery relays on a \\nflexible and accumulative schema of rows, models the rows'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='regions to servers, and stores the mappings as data in the \\ncorrdinator server. As the comparsion, Haery relays on a \\nflexible and accumulative schema of rows, models the rows \\nwith different keys into a table, builds the key-cube accord-\\ning to the segments on value range of each key, expands key-\\ncube to support new keys, divideds rows into cells, and cal-\\nculates addresses of cells. We compare the two systems in \\nthe Table 1. \\nIn Table 1, some concepts will be explained in \\nthe rest paper. \\nTABLE 1 DIFFERENCES BETWEEN HYPERDEX AND HAERY \\n \\nFacing the “volume”, “variety” and “variability” charac-\\nteristics of big data, index based solutions have three weak-\\nnesses: indexes require a large amount of storage, it is costly \\nto build indexes for various keys, and it is too complex to \\nmaintain and rebuild indexes when dealing with large scale \\nand frequently updated data sets. Haery solves these prob-\\nlems by an accumu lative and high-dimensional data model.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='maintain and rebuild indexes when dealing with large scale \\nand frequently updated data sets. Haery solves these prob-\\nlems by an accumu lative and high-dimensional data model. \\nThere are three categories of multidimensional data models \\nfor NoSQL databases. \\n(1\\n) Basic multidimensional data models, which are a sim-\\nple technique, take the data sets as points in multidimen-\\nsional space, then divide data attributes (keys) as dimensions. \\nFor example, Chevalier et al. [25] propose an implementation \\nof traditional OLAP (On-Line Analytical Processing) systems \\nby column-oriented and document-oriented database;  \\n(2) Statistical multidimensional models, which are more \\ncomplicated than basic models, leverage specific aggregation \\nfunctions to achieve hierarchies of dimensions. For example, \\nIqbal et al. [26] propose a histogram method, which iterative-\\nly splits histogram buckets until the given space limit is \\nreached, to summarize multidimensional data represented as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='Iqbal et al. [26] propose a histogram method, which iterative-\\nly splits histogram buckets until the given space limit is \\nreached, to summarize multidimensional data represented as \\na tuple -independent probabilistic model. The proposed \\nmethod. \\n(3) Structured multidimensional data model s, which are \\nmore powerful to support hierarchies of dimensions. For \\nexample, we have prop osed a paper proposes a structured \\nand distributed MOLAP technique, named DOLAP (distrib-\\nuted OLAP), based on Hadoop distributed file system (HDFS) \\nand MapReduce program model, it can also support complex \\naggregation operations between hierarchical structures [27\\n]. \\nHaery adopts an improved basic multidimensional data \\nmodel. On one hand, all the above data models would be \\nrefactored when a new dimension is introduced, or current \\ndimension hierarchies are updated. For basic model s, new \\ndimensions cause updating index spaces and remapping \\nindexes to addresses. For statistical'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='dimension hierarchies are updated. For basic model s, new \\ndimensions cause updating index spaces and remapping \\nindexes to addresses. For statistical\\n models, new data causes \\nre-calculation of the existing aggregation. For structured \\nmodels, updating dimension hierarchies causes the rebuild-\\ning of the complex data structure. The cost of this refactoring \\nis unaffordable in a big data environment. Haery treats refac-\\ntoring in a distinct way, creating a new data structure instead \\nof modifying the existing one. In most column -oriented \\nstores, to reduce the cost of modification, data items  are not \\ndeleted or modified but replaced. Following this idea, the \\ndata model may also be replaced instead of being modified. \\nThe previously mentioned works aim to solve a specific \\nproblem to meet some specific requirements. Consequently \\nthey concentrate on query performance optimization. Haery \\naims to solve a broader range of problems and is a general -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 2, 'page_label': '3'}, page_content='problem to meet some specific requirements. Consequently \\nthey concentrate on query performance optimization. Haery \\naims to solve a broader range of problems and is a general -\\npurpose, high-dimensional data supported, query optimized, \\nflexible and extendable data store. \\n3 MODEL \\nIn this section, the critical definitions for key-cube, which are \\nfundamental to linearization, accumulation and query in the \\nrest of paper, are explained. The main symbols appearing in \\nthis paper are listed in Table 2. \\nTABLE 2 MAIN SYMBOLS IN THIS PAPER \\n \\n3.1 Key-values \\nIn column-oriented stores, data is accessed with a key -value \\ndata model. A table consists of rows, and a row consists of a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n4 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nunique row key and many objects. An object has key -value \\npairs associated with different timestamps, in which keys \\nwith different timestamps are the same but valu es with dif-\\nferent timestamps are different. Referred to by an entity -\\nrelationship conceptual model, a row stands for an entity \\nand objects are attributes of the entity, in which keys are at-\\ntribute names, and values are attribute values, while'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='relationship conceptual model, a row stands for an entity \\nand objects are attributes of the entity, in which keys are at-\\ntribute names, and values are attribute values, while \\ntimestamps represent different versions of attributes. \\n3.2 Segments \\nIn a column -oriented store, the values of a key may be nu-\\nmerical or textual data, and values may belong to a certain \\nrange. To partition these values, we define the segment as a \\nspecial set of values which belongs to the same range. \\nDefinition 1 Segment. A segment s represents a range of values, \\ns is a two -tuple <α, β> representing the lower boundary and \\nupper boundary of the range (α and β are numbers) \\nThere are three special forms of segment. \\n(1\\n) s0 =<-∞,+∞>, also named ALL, represents all values; \\n(2) s1=<-∞, α> and sw=<β, +∞>, named lower overflow \\nsegment and upper overflow segment; \\n(3) se=null, also named NULL, represents no value.  \\nGiven a value u of the key, we define u belongs to the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='(2) s1=<-∞, α> and sw=<β, +∞>, named lower overflow \\nsegment and upper overflow segment; \\n(3) se=null, also named NULL, represents no value.  \\nGiven a value u of the key, we define u belongs to the \\nsegment s, denoted as segment u≼s. s and ≼ may have fol-\\nlowing four forms: \\n(1) u≼s0 \\n(2) u≼se if u is null.  \\n(3) u is a numerical value, namely u≼s iff u∈(a, β]. \\n(4) u is textual value, namely u≼s iff hash(u)∈(a, β]. Func-\\ntion hash() is the hash value (integer) of a string. The func-\\ntions hash-1(-∞) and hash-1(+∞) are invalid. \\nFor segment sx and sy , their operations are defined as fol-\\nlows. \\n(1) Union of two segments, as sx∪sy ={u| u≼ sx or u≼ sy }, \\nand intersection of two segments, as sx∩sy ={u| u≼ sx and u≼ \\nsy } \\n(2) One segment is the sub-segment (proper subset) of the \\nother, as sx \\uf0cc sy iff ax <ay and βx >βy \\n(3) The special operations on se, such as sx ⊄se, se ⊄ sx, se \\uf0cc \\nse; and sx∩se =∅, se∩se = se \\n(4) The comparison between segments, as sx ≤ sy iff βx ≤'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='other, as sx \\uf0cc sy iff ax <ay and βx >βy \\n(3) The special operations on se, such as sx ⊄se, se ⊄ sx, se \\uf0cc \\nse; and sx∩se =∅, se∩se = se \\n(4) The comparison between segments, as sx ≤ sy iff βx ≤ \\nay, we know \"≤\" is total order relation on {s} \\n(5) sub(s) are the non-overlapping and non-empty subsets \\nof s, which are in ascending order. \\nPer the function sub(s), a segment s can be divided into \\nsub-segments. We call this segment refinement, and sub -\\nsegments are in ascending order according to the \" ≤\" opera-\\ntor. \\nDefinition 2 Segment Refinement. Segment refinement means \\nsegment s is divided into w sub-segments, which are in ascend-\\ning order, and are non -overlapping and non-empty. Segment s \\nis refined to sub(s) according to the definition of function sub() \\nin definition 1. The number of sub -segments is a fixed value, \\ncalled the refinement factor and denoted as w (w>3). \\nThe segmentation algorithm divides a segment into sub -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='in definition 1. The number of sub -segments is a fixed value, \\ncalled the refinement factor and denoted as w (w>3). \\nThe segmentation algorithm divides a segment into sub -\\nsegments with an equal interval which is defined as Δ=(β- \\na)/w. It is a simple algorithm and is abbreviated here. \\nAmong the sub(s), lower overflow sub-segment is included if \\ns is also a lower overflow su b-segment, and the same for \\nupper overflow segments. For example: \\nFor the initial state: s0=<-∞,+∞>,Δ=(β- a)/w, w=3 \\nThe sub-segment of s0 is: sub(s0)={ s1, s2, s3,..., sw}= {<-∞, a \\n>, < a, a +Δ>, < a +Δ, a +2Δ>,...,<β-Δ, β>, <β, +∞> } \\nThe sub-segment of s1 is: sub(s1)={<-∞, a -wΔ>,...,< a -2Δ,  a \\n-Δ>, < a -Δ, a >} \\nThe sub -segment of sw is: sub(sw)={ < β, β+Δ>, < β+Δ, \\nβ+2Δ>,...,<β+wΔ, +∞>,} \\nSegment refinement disperses a massive amount of  data \\non the sub -segments, and because of equal intervals, seg-\\nments may map different volumes of data. But, as is ex-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='β+2Δ>,...,<β+wΔ, +∞>,} \\nSegment refinement disperses a massive amount of  data \\non the sub -segments, and because of equal intervals, seg-\\nments may map different volumes of data. But, as is ex-\\nplained later, the data volume of segments tends to be equal \\n(equal frequency) after many iterations of segment refine-\\nment. \\nDefinition 3 Segment Hierarchy and Levels. A segment and \\nits sub-segments are organized as a hierarchy. Segment hierar-\\nchy is initiated as two levels at least, then expend to τ\\n levels. \\nThe root level is level 0 with a single segment s 0 (ALL). The \\nnext level contains predef ined w segments, which are sub(s0), \\nincluding upper and lower overflow segments. A segment in \\nlevel 1 may expanded to w sub -segments, with each level in \\nturn potentially expanding further. Or, a level may not expand \\nat all. The segment hierarchy is a “w -ary” ordered tree because \\nsub(s) is in ascending order. \\nA consequence of definition 3 is that the segment hierar-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='at all. The segment hierarchy is a “w -ary” ordered tree because \\nsub(s) is in ascending order. \\nA consequence of definition 3 is that the segment hierar-\\nchy expands gradually. We introduce the concept of version-\\ning to distinguish the segment hierarchies of different ex-\\npanded stages in section 5. \\nDefinition 4\\n Segment Index. Segment index is the identity of \\neach segment in a segment hierarchy, and is denoted as the sub-\\nscript \"j\" of segment s j. The segment hierarchy which contains \\nτ+1 levels may have at most  segments, so the index of the root \\nsegment is 0 and the in dexes of th e rest are coded \\nfrom 1 to n level -wise, and from left to right in a level. Since \\nlevel l contains at most power(w, l) segments, some segments \\nmay be absent because the refinement has not happened yet, but \\ntheir corresponding indexes are reserved. \\nFunction f. According to decimal coding, a segment range \\ncan be calculated by giving a segment index j. Given s1=<-∞,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='their corresponding indexes are reserved. \\nFunction f. According to decimal coding, a segment range \\ncan be calculated by giving a segment index j. Given s1=<-∞, \\nα> and sw=<β, +∞>, a function f(j) returns the segment by its \\nindex, that is,  sj=f(j) and sj =<aj, bj>=<fa(j), fb(j)>. Meanwhile \\nj= f-1(\\nsj)=f-1(<aj, bj>). If it exists, sj is the x-th ordered segment \\nin the level y of segment hierarchy, x, y, aj and bj are as fol-\\nlows (in the equation wy is “y-th power of w”, but not super-\\nscript): \\ny = logww+(w-1)j-1,              x=j- (wy-w)/(w-1) ; \\nwhen  j=(wy-1)/(w-1),         aj=-∞,  βj=2a-β+x·w1-y·(β-a) ; \\nwhen j=(wy+1-w)/(w-1),aj=2β-a+(x-1-wy)w1-y·(β-a), βj=+∞ ; \\notherwise aj=2β-a +(x-1-wy)·w1-y·(β-a), βj=2a-β+x·w1-y·(β-a) . \\nDefinition 5 Active Segment and Dormant Segment.  Given \\na segment hierarchy, a segment which has no sub -segments (a \\nleaf node) is an active segment; and segments which have sub -\\nsegments (non-leaf nodes) are dormant segments. \\n3.3 Key-cube'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 3, 'page_label': '4'}, page_content='a segment hierarchy, a segment which has no sub -segments (a \\nleaf node) is an active segment; and segments which have sub -\\nsegments (non-leaf nodes) are dormant segments. \\n3.3 Key-cube \\nKey-cube, which is a replacement for the key -value data \\nmodel, is a high -dimensional data model whose dimensions \\nare keys and cells are links to the data files. The key, which \\n \\n\\uf0e5\\n\\uf03d\\n\\uf03d\\n\\uf074\\n0l\\nlw n'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 5 \\n \\nrepresents a dimension, is divided into segments according \\nto their values. Key-cube is not a fixed data model but accu-\\nmulatively changes according to the keys and their values. \\nDefinition 6 Dimension and Dimension Index . In a key -\\ncube, a key is modeled as a dimension which categorizes values \\nof the key into non-overlapping active segments and se, in order'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='Definition 6 Dimension and Dimension Index . In a key -\\ncube, a key is modeled as a dimension which categorizes values \\nof the key into non-overlapping active segments and se, in order \\nto provide filtering, grouping and labeling. A dimension con-\\ntains segments of the expandable segment hierarchy. se is the \\nfirst segment of a dimension, and the rest of the segments are \\nactive segments of the corresponding segment hierarchy from \\nleft to right. Thus a dimension d contains at least w+1 segments \\nbecause the segment hierarchy is initiated with at least two lev-\\nels (see definition 3). A dimension index is the identity number \\nof the dimension. Let subscript i (1≤i≤n) represent the index of \\nd. And a table of dimension index, named as DTable, is used to \\nmap the keys (names) to the dimension indexes. \\nDefinition 7\\n Cell and Cell Index . A cell is specified by active \\nsegments (leaves) or  the null segment ( se) of a dimension. All'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='map the keys (names) to the dimension indexes. \\nDefinition 7\\n Cell and Cell Index . A cell is specified by active \\nsegments (leaves) or  the null segment ( se) of a dimension. All \\nrows are partitioned into the cells by the following rules: Let a \\nrow contains a key -value pair <k i, ui>, and for the correspond-\\ning dimension di, if the value u i≼sij, then the row is mapped to \\nsij. Rows in column -oriented store are organized as data files, \\nthus a cell contains physical directories of the data files where \\nthe corresponding rows are stored. Cell index is calculated by \\nthe linearization algorithm and the mapped directories are re-\\ntrieved by the address tree (see section 4). \\nDefinition 8 Segment Map . A segment map is a HashMap \\nwhose keys are integers, representing dimension indexes, the \\ncorresponding values of a key are also integers, representing \\nsegment indexes. Both keys and values are sorted. Let m be a \\nsegment map, the array m.keys are dimension indexes, and the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='corresponding values of a key are also integers, representing \\nsegment indexes. Both keys and values are sorted. Let m be a \\nsegment map, the array m.keys are dimension indexes, and the \\narray m[i].values are segment indexes of dimension di. \\nNormally, key value pairs are sparse, thus not all keys are \\nmodeled as dimensions, and a key -cube contains a large \\nnumber of empty cells. But the key -cube is a logical data \\nmodel, and as is explained in sections 4, the sparsity does not \\nwaste storage. A high -dimensional key-cube does not have \\nthe problem of storage explosion because most information \\nis calculated rather than stored. \\nDefinition 9 Dimensional Key and Trivial Key . Keys which \\nare selected as dimensions are dimensional keys, otherwise they \\nare trivial keys. Most keys are dimensional keys in a key -cube. \\nBy default, new keys are trivial keys, which may be upgraded to \\ndimensional keys. \\nDifferent from the selection of keys when building  an in-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='By default, new keys are trivial keys, which may be upgraded to \\ndimensional keys. \\nDifferent from the selection of keys when building  an in-\\ndex, dimensional keys not only frequently occur in query \\nconditions, but also in query results. On the contrary, trivial \\nkeys are keys whose values are not queried often, neither in \\nquery conditions nor in query results. There are a number of \\nkeys in the key-value data model, and among which dimen-\\nsional keys dominate, so that the key -cube has a high num-\\nber of dimensions. When a key is selected as a dimension, \\nand its range is determined according to the context, then s0 \\nto sw, which are levels 0 and 1  of the segment hierarchy, are \\ninitialized first.  \\n4 LINEARIZATION \\nA key -cube is a high -dimensional data model. There are a \\nlarge number of cells, even though most of them are empty, \\nbut the large address space of cell indexes is reserved. Haery \\nneeds a sophisticated linearization algorithm and addressing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='large number of cells, even though most of them are empty, \\nbut the large address space of cell indexes is reserved. Haery \\nneeds a sophisticated linearization algorithm and addressing \\nmechanism to encode and map the cells indexes to the file \\naddresses. A linearization algorithm is an algorithm which \\nmaps multi-dimensional data to one dimension. This section \\ndescribes a linearization algorithm that encodes the cells of a \\nkey-cube into a sequence. A space -filling curve (SFC) is a \\ncurve whose range contains the entire n -dimensional hyper-\\ncube, so that a curve is selected to design a linearization al-\\ngorithm. The cell index, which is the resul t of linearization, \\nmaps to physical directories by querying a tree -structure, \\ncalled an address tree.  \\nIn a key-cube, the adjacency of cells benefits the address-\\ning process because the matched cells for a query are normal-\\nly adjacent, thus they can be addr essed together. The ad-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='In a key-cube, the adjacency of cells benefits the address-\\ning process because the matched cells for a query are normal-\\nly adjacent, thus they can be addr essed together. The ad-\\ndressing of one cell could reuse the addressing information \\nof its adjacent cells, therefore, the indexes of neighbor cells \\nshould be continuous. Based on the classification of Asano T. \\n[28], the most popular SFCs are recursive SFCs, such as Z -\\norder, Gray and Hilbert, and non -recursive SFCs, such as \\nSweep and Scan. Sweep and Scan are simple, their time \\ncomplexities and flexibilities are better than the others, but \\nthey only maintain the continuity of one dimension and \\nbreak that of the  other dimensions. Meanwhile, Z -order is \\nfair and scalable on each dimension, and is relatively simpler \\nthan Gray and Hilbert  [29]. By Z -order, cells, especially \\nneighboring cells are encoded continuously. Z-order curve is \\nmore suitable choice for its fairness in all dimensions.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='than Gray and Hilbert  [29]. By Z -order, cells, especially \\nneighboring cells are encoded continuously. Z-order curve is \\nmore suitable choice for its fairness in all dimensions. \\nHowever, Z-order can only expand synchronously in all \\ndimensions. It is too rigid and requires the same number of \\nactive segments in each dimension. To solve this problem, a \\nflat Z-order algorithm is proposed, which only requires th at \\nthe number of active segments in dimensions are integral \\nmultiples of each other. If the conditions are not satisfied \\nnaturally, some null \\nsegments (se) are artificially added to the \\ndimensions. \\n \\nFig. 1. Six space curves in two-dimensional space \\nFigure 1-(e) and (f) shows the flat Z -order in comparison \\nwith the original Z -order in two dimensional space. Let the \\ninterleaving ratio of two dimensions be 1:2. Thus, in Figure \\n1\\n-(f), when the curve extends one dimension for one cell, it \\nextends the other dimension for two cells, while the original'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 4, 'page_label': '5'}, page_content='interleaving ratio of two dimensions be 1:2. Thus, in Figure \\n1\\n-(f), when the curve extends one dimension for one cell, it \\nextends the other dimension for two cells, while the original \\nZ-order can expand all the dimensions by the same number \\n(a) Sweep (b) Scan (c) Gray (d) Hilbert\\n(e) Z-order (f) Flat-zorder'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n6 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nof cells (interleaving ratio is 1:1). \\nDefinition 10 Binary length. Binary length θi of dimension i is \\n⌊log2max(sij)⌋ in which max(s ij) is the maximum index of a seg-\\nment in dimension i. Without loss of generality, let θ be the \\nminimum binary length among those of all dimensions, then \\nθi/θ is a positive integer. \\nGiven the active segments of dimension 1 to n, the index \\nof a corresponding cell is calculated as the following three'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='θi/θ is a positive integer. \\nGiven the active segments of dimension 1 to n, the index \\nof a corresponding cell is calculated as the following three \\nsteps in which arrays are indexed starting with 1. For clarity, \\nsij is the j-th segment in i-th dimension (see Table 1), and Fig-\\nure 2 is an example of these steps. \\n(1) Binarization: Convert index (j) of segment sij into a se-\\nquence of binary numbers. Let the sequence be stored in ar-\\nray xij[]. If necessary, several “\\n0”s are complemented on to \\nthe head of the sequence to ensure that the length of xij[] is θi. \\nThen xij[p] is 0 or 1 ( pi ∈ [1, θi]). For example, θ1=2, x12=[1,0]; \\nθ2=8, x25=[0,0,0,0,0,1,0,1] . \\n(2) Grouping: Convert xij[] into yij[] whose length is θ. Let \\n“+” be the string concatenation operator, then ∀ p ∈ [1,θ], yij[p] \\n= xij[(p·θi /θ)] + xij[(p·θi /θ)+1]+ xij[(p·θi /θ)+2]+...+ xij[(p·θi \\n/θ)+ θi /θ], For example, let θ2= 8 and θ=2, then y25=[0000, \\n0101]. \\n(3) Interleaving: Interleave the segments of each dimen-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='= xij[(p·θi /θ)] + xij[(p·θi /θ)+1]+ xij[(p·θi /θ)+2]+...+ xij[(p·θi \\n/θ)+ θi /θ], For example, let θ2= 8 and θ=2, then y25=[0000, \\n0101]. \\n(3) Interleaving: Interleave the segments of each dimen-\\nsion. Let array z[] contains n selected segments indexes for \\ndimension, respectively, then the index of a cell determined \\nby z[n] is { y1(z[1])[1]+ y2(z[2])[1]+…+ yn(z[n])[1]}+ {  y1(z[1])[2]+ \\ny2(z[2])[2]+… + yn(z[n])[2]}+…+{ y1(z[1])[θ]+ y2(z[2])[θ]+…+ y \\nn(z[n])[θ]}. Therefore： cell_index=  . \\nθ=2\\nθ1=2\\nθ2=8\\nS12\\nS25\\nx12= [1,0]\\nx25=[0,0,0,0,\\n        0,1,0,1]\\n(1)\\n(1)\\ny12=[1,0]\\ny25=[0000,0101]\\nz=[2,5]\\n(3)\\n(3)\\nindex=1000000101]\\n5 is 101\\n2 is 10\\n(1) Binarization (2 ) Grouping (3 ) Interleaving\\n(2)\\n(2)\\n \\nFig. 2. Example of (1) Binarization, (2) Grouping and (3) Interleaving \\nWe introduce the concept of b inary length  for dealing \\nwith an irregular n-dimensional key- cube, otherwise each \\ndimension should contain the same number of active seg-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='We introduce the concept of b inary length  for dealing \\nwith an irregular n-dimensional key- cube, otherwise each \\ndimension should contain the same number of active seg-\\nments, and has to expand all the dimensions synchronously. \\nThe complemented “0” in the head of  sequence only con-\\nsumes the address space but not storage.  \\n \\nFig. 3. Example of flat Z-order \\nThe cell indexes for a 4× 8 key-cube (both in decimal and \\nbinary) are shown in Figure 3 as an example. A nd in this \\ncase, the interleaving ratio is 1:2. Interleaving the binary co-\\nordinate values yields binary Z-values, and connecting the Z-\\nvalues in their numerical order produces the recursively flat \\nZ-shaped curve. \\nFunction g(): Specifies segments of each dimension, then \\none or more cells are determined.  Let m be a segment map \\ncontaining active segment indexes for n dimensions. A func-\\ntion g(m) returns cell indexes c[] mapped by m. Function g() \\nis implemented according to the flat Z -order. The size of c[] \\nis'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='containing active segment indexes for n dimensions. A func-\\ntion g(m) returns cell indexes c[] mapped by m. Function g() \\nis implemented according to the flat Z -order. The size of c[] \\nis \\nvalues i m\\nn\\ni\\n].[\\n1\\n0\\n\\uf02d\\n\\uf03d\\n\\uf050 . \\nDefinition 11 Cell Address and Address Tree . Given a cell, \\nits cell address is a group of physical directories in which the \\ncorresponding data is contained. An address tree is an ordered-\\ntree associated with a key-cube. It leverages the longest common \\nprefix naming scheme to map a cell index to a cell address. \\nA quad-tree is a tree data structure in which each internal \\nnode has exactly four children. An address tree has a maxi-\\nmum of θ levels, from level 0 to level θ-1. A θ-levels address \\ntree is similar to a quad -tree e xcept that each node in the \\nlevel (θ-2) has \\n) (\\n1\\n\\uf071 \\uf071i\\nn\\ni \\uf03d\\n\\uf050  children; then an internal node rep-\\nresents the longest prefix of its children’s indexes, and a leaf \\nrepresents a cell index. So that for an arbitrary address tree,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content=') (\\n1\\n\\uf071 \\uf071i\\nn\\ni \\uf03d\\n\\uf050  children; then an internal node rep-\\nresents the longest prefix of its children’s indexes, and a leaf \\nrepresents a cell index. So that for an arbitrary address tree, \\nan internal node maps to the parent directories of the cell \\naddress, and a leaf maps to the cell address. An address tree \\nis initialized as θ levels at most, or less levels for reducing its \\nsize, according to the scale of key-cube. \\nFunction r(): Given an address tree and cell indexes, de-\\nnoted as array c[], function r(c[]) returns physical directories \\nthat contain the data files of the cells in c[]. In r(), the directo-\\nries are addressed by comparing the prefix of the cell index \\nwith nodes. Since the indexes of adjacent cells are continu-\\nous, they are addressed together. Assuming the indexes in c[] \\na\\nre sorted, then when c[i] is addressed, the traversal path \\n(node list from the root to the matched node) is cached, then \\nc[i+1] is addressed along the path inversely. The address tree'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 5, 'page_label': '6'}, page_content='a\\nre sorted, then when c[i] is addressed, the traversal path \\n(node list from the root to the matched node) is cached, then \\nc[i+1] is addressed along the path inversely. The address tree \\ncontains θ levels at most, so if n cells are addressed one by \\none, the time complexity is O(θn), and the time complexity of \\nr() is no greater than O(θn). The algorithm is as follows:  \\n \\n\\uf0e5\\uf0e5\\n\\uf03d \\uf03d\\n\\uf071\\n1 1\\n])[ ( ] [\\np\\nn\\ni\\ni z i p y'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 7 \\n \\n \\n \\nFigure 4 shows the example of address tree of Figure 3. In \\nFigure 4, the selected cells in the left ma p to the node of ad-\\ndress tree in the center, and the node maps to the directories \\nin the right. I n Figure 4, a cell address contains only one di-\\nrectory, but in practice it would usually contain more. All the \\ndescendants of a node have a common prefix that is associat-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='in the right. I n Figure 4, a cell address contains only one di-\\nrectory, but in practice it would usually contain more. All the \\ndescendants of a node have a common prefix that is associat-\\ned with the node, and the root is associated with a computer.  \\n[0000]\\n...\\n[11]\\n[01]\\n...\\nPhysical Directories \\n(partly)\\n4\\\\\\nNode n\\nds\\\\\\nts\\\\\\n4\\\\\\n0\\\\\\n1\\\\\\n... ds\\\\\\nts\\\\\\n[10][00]\\n[0001]\\n[0010]\\n[0011]\\n[1000]\\n[1011]\\n[1100]\\n[1111]\\n...\\n...\\n[00001][00000] [10111][10110] [11111][11110]\\n...\\nCell with indexes\\nAddress tree (partly)\\nFig. 4. Example of address tree \\n \\nFig. 5. Example of segment refinement and key cube expansion  \\n5 ACCUMULATION  \\nHaery improves query performance by providing the pre -\\ndefined, well -formed and well -partitioned key -cube. Key -\\ncube has two features: high-dimensions for supporting a vari-\\nety of keys (section 4 explains why the high -dimensions do \\nnot cause performance problems); accumulation for perfor-\\nmance and flexibility. The later one is called key -cube expan-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='ety of keys (section 4 explains why the high -dimensions do \\nnot cause performance problems); accumulation for perfor-\\nmance and flexibility. The later one is called key -cube expan-\\nsion and explained in this section. Figure 5 shows the accu-\\nmulation of key -cube expansion by segment refinement and \\ndimension introduction. \\nOn one hand, as defined in definition 2 , segment refine-\\nment means segment s is divided into w sub-segments. Obvi-\\nously,  a dimension initialized as a root with only w active \\nsegments is too coarse to categorize massive data, so that, the \\ndata volume in a segment gets larger and larger when new \\ndata is imported, and th en, the segment is refined to sub -\\nsegments for reducing data volume in a segment. By this \\nmeans, the key-cube is expanded by segment refinement, for \\nexample the step (1) and (3) in Figure 5, s22 is refined to s27, s28, \\ns29 in step (1) and s33 is refined to s311 and s312  in step (3). The'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='example the step (1) and (3) in Figure 5, s22 is refined to s27, s28, \\ns29 in step (1) and s33 is refined to s311 and s312  in step (3). The \\ntrigger of segment refinement is that both the data volume \\nand data dispersion are larger than given thresholds. Data \\ndistribution is measured by a sampling approach and five -\\nnumber summary (minimum, Q1, median, Q3, maximum). \\nOn the other hand, new keys (by default trivial keys) are \\nintroduced with new data meanwhile the trivial keys may no \\nlonger be “trivial”, and new a dimension is accordingly add-\\ned to the key -cube. So that the key cube is expanded by di-\\nmension introduction, for example the step (2) and (4) in Fig-\\nure 5. Dimension introduction is triggered according to the \\nfrequency of a trivial key occurring in rows. \\nDefinition 12 Key-cube Expansion. A key-cube expansion is a \\nprocess of creating a new key\\n-cube based on a exsiting one, by in-\\ntroducing dimensions (new keys or trivial keys are upgraded as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='Definition 12 Key-cube Expansion. A key-cube expansion is a \\nprocess of creating a new key\\n-cube based on a exsiting one, by in-\\ntroducing dimensions (new keys or trivial keys are upgraded as \\ndimensional keys), or by refining active segments of dimensions \\n(one or more active segments refine to w sub -segments), or by \\nboth. Expansion is the key feature of the accumu lative key-cube \\nwhich improves both flexibility and query efficiency. \\nDefinition 13 Key-cube Version. The version of a key-cube is for \\ndistinguishing different key -cubes by their historical features. \\nEvery time a key -cube is expended, a new one is defined.  These \\nkey-cubes are numbered in a sequence order from  earliest to lat-\\nest, and their orders are their versions. The version of the initial \\nkey-cube is 0. \\nDefinition 14 Key-cube Backtrace. A key-cube backtrace is the \\nprocess of traversing from a later versioned key-cube to an earlier \\nversioned one, for comprehensive query on data partitioned by'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='Definition 14 Key-cube Backtrace. A key-cube backtrace is the \\nprocess of traversing from a later versioned key-cube to an earlier \\nversioned one, for comprehensive query on data partitioned by \\ndifferent key-cubes. The key-cube backtrace can be treated as the \\ninverse operation of key -cube expansion, by dimensio n reduc-\\ntion, or by segments consolidation (replacing the active segments \\nwith its parent), or by both. \\nFor example in Figure 5, steps (6) and (8) are segment con-\\nsolidation, and step (5) and (7) are dimension reduction. \\nFunction h. The function h(cube) returns the previous  ver-\\nsioned key-cube of the current one by querying the key -cube \\nmetadata. Notice that many concepts defined in the previous \\nsections associated with the key -cube, such as dimensions, \\nsegments, function f(), g\\n() and r(), linearization algorithm and \\naddress tree, these concepts of different versioned key -cube \\nare different too. Especially the linearization algorithm, when'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 6, 'page_label': '7'}, page_content='segments, function f(), g\\n() and r(), linearization algorithm and \\naddress tree, these concepts of different versioned key -cube \\nare different too. Especially the linearization algorithm, when \\na new key- cube is expanded, the linearization algorithm and \\naddress tree are all changed, mapping new data to different \\ncell addresses. \\nAs far as the query is concerned, every key-cube expansion \\nis a segmentation on the new data in the future, however, \\nexpansion does not affect the existing data. When a query is \\nissued, the targets are both the current data managed by the \\ncurrent key -cube, and “historical” data managed by earlier \\nversioned key- cubes. In these cases, Haery adopts a two -\\n \\n1\\nd1\\nd2\\ns11 s12 s13 s1e\\ns21\\ns22\\ns23\\ns2e\\nd1\\nd2\\nd3\\ns1es11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns31\\ns32\\ns33\\ns3e\\n78\\n3\\n5\\n4\\n6\\nd3\\nd1\\nd2\\ns1es11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns31\\ns32\\ns33 s3e\\ns310\\ns311\\ns312\\n2\\ns1e\\nd2\\nd1\\ns11 s12 s13\\ns21\\ns27\\ns23\\ns2e\\ns28\\ns29\\ns22\\nσ0 σ1 σ2 σ3 σ4'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n8 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nphase query strategy: first query all key -cubes and locate all \\ndata files as the search space; second query data in these files. \\nThe key-cube backtrace ensures the data files containing both \\ncurrent data and historical data are selected. The query per-\\nformance is dominated by the size of the search space, which \\ncan be greatly reduced by querying key -cubes. Key-cube ex-\\npansion ensures that the optimization effect of later versioned'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='formance is dominated by the size of the search space, which \\ncan be greatly reduced by querying key -cubes. Key-cube ex-\\npansion ensures that the optimization effect of later versioned \\nkey-cube is the same, or even better, than that of earlier ver-\\nsioned ones, and the optimization effect of earlier versioned \\nkey-cubes remains unchanged. Query performance is im-\\nproved by the accumulation strategy.  \\n6 QUERY \\nThe key-cube not only benefits queries on values (conditions) \\nby reducing the search space, but also benefits queries on \\nkeys (targets) by well -managed dimensional keys. And, due \\nto the accumulative key -cube, performance optimization is \\nstill in effect whe n new data is imported. In this section the \\nquery process is introduced. The query process of Haery in-\\ncludes query on key -cube and query on files, and the former \\nincludes backtrace on key-cube as mentioned in section 6. \\nDefinition 15\\n Query in Haery . A query in Haery includes two'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='cludes query on key -cube and query on files, and the former \\nincludes backtrace on key-cube as mentioned in section 6. \\nDefinition 15\\n Query in Haery . A query in Haery includes two \\nparts, targets and conditions. Targets is a list of keys, and condi-\\ntions includes keys and query ranges. Both targets and condi-\\ntions contain dimensional keys and trivial keys. The key -cube \\nimproves query performance if keys, no matter if they are in con-\\nditions or in targets, are dimensional keys. To reduce the search \\nspace, a key, if it is not contained in conditions but only in tar-\\nget, is treated as a special condition whose range is < -∞,+∞>. \\nQueries are denoted as: \\nquery={<key, ran ge, is_target >}={<k 1, α1, β1,true>,<k2, α2, \\nβ2,true>, … , <kc, αc, βc, false >} \\nFor example, a query “\\nselect a, b from table where a∈(0, 8] \\nand c∈(0, 10] ” is represent as {< a, 0, 8, true>,< b, -∞, + ∞, \\ntrue>,<c, 0, 10, false>}. \\nThe querying process is explained by the following steps:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='select a, b from table where a∈(0, 8] \\nand c∈(0, 10] ” is represent as {< a, 0, 8, true>,< b, -∞, + ∞, \\ntrue>,<c, 0, 10, false>}. \\nThe querying process is explained by the following steps:  \\n(1) Initial query on the latest key cube; \\n(2) Backtrace and query each versions of key-cubes;  \\n(3) Query on chunk files in a distributed and parallel man-\\nner.  \\nSteps (1) and (2) are to reduce the search space (queried \\nfiles), which can greatly improve the performance. Algorithm \\n2 explains the details of steps (1) and (2). Haery support dif-\\nferent computing frameworks which have different machines \\nor grammars to implement step (3), such as Hadoop Map Re-\\nduce. Given the diversity of implementations, details of step \\n(3) are abbreviated. \\n \\n \\n7  ARCHITECTURE \\nIn this section, the software architecture of Haery is explained \\nas in Figure 6\\n. Haery is a query focused column-oriented store \\nwhich is based on two i nfrastructures: a distributed compu-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='In this section, the software architecture of Haery is explained \\nas in Figure 6\\n. Haery is a query focused column-oriented store \\nwhich is based on two i nfrastructures: a distributed compu-\\nting framework and a distributed file system. Upon these, \\nseveral modules collaborate to implement the key -cube’s \\nmanagement and query engine, they are Cube -base, Proxy, \\nExpansion, Partition, Placement, and Calculation. T he source \\ncode of Haery is available on https://github.com/CloudLab-\\nNEU/Haery. \\nCube-base\\n Proxy\\nCalculation\\nSpark or MapReduce\\n1.cube metadata\\n3.cube \\ninformation\\n4. generate tasks\\nnew  cube\\n5. return\\nresults\\n      Haery core Computing \\nFramework\\nPartition\\nPlacement\\nHDFS\\nExpansion\\nData loading\\nData loading Data query\\n2. query  request \\nStorage\\n \\nFig. 6. Software architecture of Haery  \\n•Cube-base: The metadata is stored in the Cube-base module \\nwhich is implemented by an in -memory database on the \\nmaster node.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 7, 'page_label': '8'}, page_content='2. query  request \\nStorage\\n \\nFig. 6. Software architecture of Haery  \\n•Cube-base: The metadata is stored in the Cube-base module \\nwhich is implemented by an in -memory database on the \\nmaster node. \\n•Proxy: When Proxy receives a query from clients, it analyzes \\nand validates the query by referring to Cube -base. Proxy \\nthen submits the query statement to Calculation, and waits'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 9 \\n \\nfor the results. Finally, Proxy receives the data from Com-\\nputing Framework, generates the result view, and reports to \\nthe client. \\n•Calculation: Calculation is the core of Haery that imple-\\nments algorithm 2 in section 6. \\n•Expansion: Expansion is a trigger for key -cube expansion \\nand implements the algorithms and strategies mentioned in \\nsection 5. \\n•'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='ments algorithm 2 in section 6. \\n•Expansion: Expansion is a trigger for key -cube expansion \\nand implements the algorithms and strategies mentioned in \\nsection 5. \\n•\\nPartition and Placement: Partition horizontally and vertical-\\nly partitions rows of a cell into several chunk files and \\nPlacement places the chunk files in nodes.  \\n• Computing Framework. Haery is technically compatible \\nwith many distributed computing frameworks such  as \\nMapReduce and Spark. Currently only MapReduce is sup-\\nported, however, it can support other computing frame-\\nwork if the algorithms  mentioned in section 6 are imple-\\nmented using the framework. \\n•Storage. Key-value data is stored in a distributed file system, \\nHDFS for example. \\nWe briefly introduce the storage mechanism of Cube-base \\nand Storage. In a traditional data cube, a multi -dimensional \\narray is the uniform form of storage and the storage cost is \\nexpensive. In Haery,  the cube is not stored but calculated.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='and Storage. In a traditional data cube, a multi -dimensional \\narray is the uniform form of storage and the storage cost is \\nexpensive. In Haery,  the cube is not stored but calculated.  \\nEach cell contains no data but links to the physical directories, \\nand cell indexes are calculated, not stored. An address tree is \\nan in-memory data structure. For each segment, only the ac-\\ntive segment indexes are stored. For each dimension, only the \\ndimension index, dimension name (for query conditions), and \\nrange of values represented as an ordered tuple <index, name, \\nα, β>, are stored. For each key-cube, only its version is stored. \\nAll the data mentioned above is Haery’ s key-cube metadata.  \\nIn a traditional a column-oriented store, storing data with \\nits schema provides flexibility but waste s space. In \\nHaery, \\nstorage is saved by separating the schema and data through \\nthe uniform key-cube, and flexibility is ensured by accumula-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='its schema provides flexibility but waste s space. In \\nHaery, \\nstorage is saved by separating the schema and data through \\nthe uniform key-cube, and flexibility is ensured by accumula-\\ntion. A  data set is horizontally partitioned into dimensional \\nchunk files and trivial chunk files , implemented as Hadoop \\nArrayFile and SequenceFile. A dimensional chunk file only con-\\ntains values of dimensional keys because names of which are \\nfixed, ordered, and modeled as dimensions in the key -cube. \\nThey are inferred by their orders. A trivial chunk file contains \\nboth trivial keys and their values. \\n8  EXPERIMENTS \\nIn this section, we evaluate the core algorithms of Haery, and \\ncompare Haery with some popular NoSQL and relational \\ndatabases using generated datasets and various query work-\\nload.  \\n \\n8.1 Setup \\nScope. Verify the core algorithms, function g\\n() for lineari-\\nzation and function r() for addressing, proposed in this paper \\nin a standalone environment. Compare Haery with other da-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='8.1 Setup \\nScope. Verify the core algorithms, function g\\n() for lineari-\\nzation and function r() for addressing, proposed in this paper \\nin a standalone environment. Compare Haery with other da-\\ntabases from the point of view s of loading performance, que-\\nry performance and storage cost. \\nExperiment environment. We execute our experiments on \\na 24 node physical machine cluster. Each node has the same \\nconfiguration with a 1TB hard disk, 8 GB memory, 64-bit plat-\\nform, and moderate I/O performance, except for Intel Core 12 \\nnodes are i5 and the other 12 nodes are i7. The gigabi t ether-\\nnet is connected by a Dell PowerConnect 5548.  \\nExperiment Content. We conducted two experiments. One \\nin a standalone environment to evaluate the algorithms on \\nkey-cube. The other in a cluster environment to compare the \\nloading, querying and storage performance of Haery with \\ncompetitors. In Haery, there are four main alg orithms: func-\\ntions f, g'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='key-cube. The other in a cluster environment to compare the \\nloading, querying and storage performance of Haery with \\ncompetitors. In Haery, there are four main alg orithms: func-\\ntions f, g\\n, h and r. On one hand, function f calculates segment \\nrange by its index, and function h returns the key-cube of the \\nprevious version. These two function are lightweight algo-\\nrithms which contribute little to query performance. On the  \\nother hand, functions g and r are heavyweight algorithms. \\nThe effectiveness of function g is dominated by the lineariza-\\ntion algorithm, so we firstly compared the performance of Z -\\norder and flat Z-order. We also evaluated the performance of \\nbuilding and querying the address tree, which dominates the \\neffectiveness of function r.  \\nIn the cluster environment, the relationship between load-\\ning performance and loaded data volume are compared and \\nanalyzed. Query performance under different queries and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='In the cluster environment, the relationship between load-\\ning performance and loaded data volume are compared and \\nanalyzed. Query performance under different queries and \\ndatasets, an d the storage cost, are also compared and ana-\\nlyzed. An extend YCSB [30] is adopted as the query client. \\nSelection of competitors. We compare Haery with Hive, \\nHBase, Cassandra, MongoDB, PostgresXL and HyperDex . \\nThe above competitors are commonly used data s tores in big \\ndata environments. The main difference between them is their \\ndatabase models and data schema. Table 3 compares their \\ndifferences. \\nTABLE 3. DESCRIPTION OF SEVEN COMPETITORS \\n \\nExperiment data. To highlight Haery’s advantages of \\nhigh-dimensionality, accumulation and efficient querying, \\nand to support  the key/value building processes in the next \\ntwo paragraphs, we adopt the generated data. There are three \\nattributes for the dataset scale: number of dimensions, data \\nvolume and number of segments on dimensions. We create'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 8, 'page_label': '9'}, page_content='two paragraphs, we adopt the generated data. There are three \\nattributes for the dataset scale: number of dimensions, data \\nvolume and number of segments on dimensions. We create \\nfive datasets of increasing scales, as explained in Table 4\\n.  \\nTABLE 4. DESCRIPTION OF FIVE DATASETS \\n \\nLoading Process. The loading process of Haery is carefully \\nplanned because otherwise the accumulative mechanism does \\nnot work. The volume and number of dimensional keys of the \\ndatasets increase from S1 to S5 gradually, so that both values \\nand keys of the larger dataset contains the small er one, and \\nthe query workload is performed on all datasets. Haery ex-\\npends the key-cube when the data and dimensions increases. \\nThe loading processes of other system s follow their default \\nconfigurations. The reason that we use YCSB with a deeply'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n10 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\nextended loading module is that the original one does not \\nsupport such an accumulative mechanism.  \\nKey Building. To highlight the accumulative key-cube, the \\nnaming rule of key s is carefully planned. First, datasets are \\naccumulated from S1 to S5, where the larger one contains keys \\nwhich have occurred in the smaller one. Second, as shown in \\nTable 4, let x be the number of dimensional keys, and there'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='accumulated from S1 to S5, where the larger one contains keys \\nwhich have occurred in the smaller one. Second, as shown in \\nTable 4, let x be the number of dimensional keys, and there \\nare 0.2x trial keys, so that the dimensional key s are named as \\n“K0” to “K(x -1)”, while the trivial keys are named as “K(x)” \\nto “K(1.2x-1)”. By such naming convention, the trivial keys of \\na small dataset overlap the dimensional keys of a larger da-\\ntaset. For example, for S1, the dimensional keys are “K0” to \\n“K9”, and trivial keys are “K10” and “K11”. When the dataset \\nincreases in size to S2 whose dimensional keys are “K0” to \\n“K19”, and trivial keys are “K20” to “K23”, the trivial keys of \\nS1 expend to become dimensional keys in S2. For Hive and  \\nPostgresXL, a predefined schema is required before data is \\nloaded, so we create a table with columns “K0” to “K199” , \\nand let columns “K13” to “K199” be nullable. For HyperDex, \\nwe built five hyperspace for five datasets, each of them con-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='loaded, so we create a table with columns “K0” to “K199” , \\nand let columns “K13” to “K199” be nullable. For HyperDex, \\nwe built five hyperspace for five datasets, each of them con-\\ntains keys of S1 to S5, respectively.  \\nValue Building. For key-value pairs of a row in any da-\\ntasets, there are only two distinct values, one is a random \\nfloat from 0 to 106, named as the numeric value, the other is a \\nrandom 10-characters sequence, named as the textual value. \\nThe values of any  keys are either the numerical one or the \\ntextual one, and the chance of being numeric or textual is fif-\\nty-fifty. Without losing generality,  K0 and K1 are numeric \\nand textual values generated by the YCSB default data gener-\\nation algorithms. According to these, for any datasets, t here \\nare only 2 independent dimensions  and the others are dupli-\\ncation, while equal-width segments on a dimension are also \\nequal-frequency due to  data being uniformly distributed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='are only 2 independent dimensions  and the others are dupli-\\ncation, while equal-width segments on a dimension are also \\nequal-frequency due to  data being uniformly distributed \\nacross its range. Therefore, if the query ranges cover half of \\nthe numeric values and the textual values, respectively, \\nnamely half active segments on each dimension are selected, \\nas a result, 25% (1/22) rows are returned. \\nQuery workload. The query workload is generated by \\nYCSB using the customized Workload E (short ranges)  be-\\ncause Haery focus es on range query but not insert, update, \\nread-by-row-key, or other workload s. There are two attrib-\\nutes of a range query relevant to our experiment: number of \\nqueried dimensions and number of matched segments. In \\nHaery, dimensions which are not in the query conditions, are \\ntreated as ALL (all active segments are m atched). It means \\nthat querying on less dimensions does not means less calcula-\\ntion on key cube.  Therefore, all dimensions are contained in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='treated as ALL (all active segments are m atched). It means \\nthat querying on less dimensions does not means less calcula-\\ntion on key cube.  Therefore, all dimensions are contained in \\nthe query conditions. The matched segments could be more \\nor less. In the customized workload E of YCSB, we define that \\nEi is a query whose conditions covers 1/2i active segments of \\neach dimension, and due to the data distribution and value \\nduplication, the query rate of Ei is (1/2i)2. We implement E1 to \\nE6 because there are at least 64 segments in a dimension, and \\nthe query rates are from 0.25 (1/22) to approximate 0.00025 \\n(1/2\\n12). For other data  stores, the query workloads are the \\nsame as for Haery. \\n \\n8.2 Algorithms on key-cube \\nLinearization \\nWe compared the performance of flat Z-order and Z-order \\nusing different number s of dimensions. The datasets are \\nshown in Table 2 and the average linearization time of the \\ntwo algorithms are compared in Figure 7. As is analyzed in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='using different number s of dimensions. The datasets are \\nshown in Table 2 and the average linearization time of the \\ntwo algorithms are compared in Figure 7. As is analyzed in \\nsection 4, flat Z -order and Z -order have the same time com-\\nplexity; the experiment shows their performance is almost the \\nsame. \\n \\nFig. 7. Linearization time of flat Z-order and Z-order \\nFigure 7 shows that the performance of the proposed flat \\nZ-order is almost the same as that of Z -order. When more \\ndimensions are involved, the linearization time of flat Z-order \\nis slightly longer than that of Z -order. With the tiny cost, flat \\nZ-order improves flexibility by allowing the key-cube to be an \\nirregular cube, as mentioned in section 4. \\nAddressing \\nWe analyzed the performance of building an address tree, \\nand addressing ( querying) via an address tree. As per the \\nexplanation in section 4, it is not necessary for the addressing \\ntree to be a full -tree whose leaves are mapped to the finest -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='and addressing ( querying) via an address tree. As per the \\nexplanation in section 4, it is not necessary for the addressing \\ntree to be a full -tree whose leaves are mapped to the finest -\\ngranularity cells. The level of the addressing tree also does \\nnot need to be the maximum  one, but according to the num-\\nber of directories. According to definition 11, an address tree \\nis a quad -tree li nked structure, and the θ-level address tree \\napproximately maps 22(θ-1) directories. A computer with Linux \\nmanaging about 2 15 \\ndirectories is suitable for quick storage \\nand search [31], so that a 9-level address tree, which means 216 \\nmapped directories, is enough for indexing all directories in a \\nnode. In this experiment, we evaluated the performance of \\nbuilding and addressing the address tree with 11,12,13,14 \\nlevels, which are suitable for clusters with 32 (25=220/215), 128, \\n512, 2048 nodes, respectivel y. A large scale data center con-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 9, 'page_label': '10'}, page_content='building and addressing the address tree with 11,12,13,14 \\nlevels, which are suitable for clusters with 32 (25=220/215), 128, \\n512, 2048 nodes, respectivel y. A large scale data center con-\\ntains thousands of servers, and a middle-size data center may \\ncontain hundreds of servers [32]. Thus the experiment scale of \\na 2048-nodes cluster is sufficient. The experiment results are \\nshown in Figure 8. \\n \\nFig. 8. Time cost of building address tree and addressing  \\nFigure 8\\n-(a) shows that the build cost of an address tree \\n(logarithmic axis) is slight except when the number of levels \\n0\\n400\\n800\\n1200\\n1600\\n0\\n 20\\n 40\\n 60\\n 80\\n 100\\nLinearization  Time (s)\\nNumber of dimesions\\nZ-order\\nFlat z-order\\n1.8 \\n5.9 \\n32.5 \\n292.7 \\n1\\n10\\n100\\n11\\n 12\\n 13\\n 14\\nBuilding  Time (s)\\nNumber of Levels\\n(a) Buliding \\n0.039 \\n0.059 \\n0.075 \\n0.093 \\n0.026 \\n0.046 \\n0.056 \\n0.068 \\n0.00\\n0.05\\n0.10\\n11\\n 12\\n 13\\n 14\\nAddressing Time (s)\\nNumber of Levels\\nWithout locality\\nWith locality\\n(b) Addressing'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 11 \\n \\nis up to 14. As the number of levels increases, the time of \\nbuilding the address tree increases exponentially because the \\nnumber of nodes in every level increases exponentially. Gen-\\nerally, it is definitely sufficient that the master node, on which \\nthe addr ess tree resides, manages 2048 nodes . Building an \\naddress tree in about 4 minutes is affordable. The cost of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='erally, it is definitely sufficient that the master node, on which \\nthe addr ess tree resides, manages 2048 nodes . Building an \\naddress tree in about 4 minutes is affordable. The cost of \\nbuilding the address tree only happens once, and is counter-\\nacted by the query optimization. \\nFigure 8-(b) describes the average addressing time unde r \\ntwo conditions, addressing without locality and addressing \\nwith locality. “ With locality” indicates that the next queried \\ncell is close to the previous one, and vice versa. In a ll condi-\\ntions, the addressing time is no more than 0.1 second s; it is \\nfast enough for big data queries. The addressing time is line-\\narly related to the level of the address tree. For comparison, \\nwhen the levels of the address tree grow, the time of address-\\ning without locality increases faster, and that with locality \\nincreases more s lowly. In terms of addressing with locality, \\nadjacent nodes can be addressed together since their indexes \\nare continuous.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='increases more s lowly. In terms of addressing with locality, \\nadjacent nodes can be addressed together since their indexes \\nare continuous. \\n \\n8.3 Performance and Cost of Haery \\nA. Loading \\nSince Hive writes data into HDFS directly without writing \\nmetadata during the loading process, its loading performance \\nis the same as that of HDFS, therefore we do not compare \\nloading performance of Hive with the others. Figure 9\\n shows \\nthe average loading speed of HBase, Cassandra, MongoDB, \\nPostgresXL and HyperDex  on datasets with different data \\nvolumes. In the loading experiment, Haery’s loading perfor-\\nmance on average is 1.5 , 9.8, 1.4 and 37.3  times higher than \\nHBase, Cassandra, MongoDB and HyperDex, respectively.  \\n \\nFig. 9. Comparison of loading speed among six stores \\nAll of these data stores, except HyperDex  employ batch \\nloading. The batch loading process consist s of four steps: (1) \\nuploading the original data files to the nodes; (2) reading the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='All of these data stores, except HyperDex  employ batch \\nloading. The batch loading process consist s of four steps: (1) \\nuploading the original data files to the nodes; (2) reading the \\noriginal data files and writing  schema-related metadata; (3) \\nsharding\\n the data among nodes; (4) transfer ing original data \\nfiles to the system related file format. For these six data stores, \\nstep (1) is almost the same, Table 5 compares the remaining \\nthree steps and some details are emphasized as the following \\npoints. \\n(1) Compared to HBase, Cassandra and MongoDB, Haery \\nemploys a simpler loading approach, it transmits  the dataset \\nto the nodes, calculates the key-cube, and shards the original \\nfiles to chunk files on each node by a MapReduce task. The \\nformat of chunk files is lightweight because dimensional keys \\nare removed. Due to the metadata, key-cube and cell address, \\nare not stored data but are calculated results, there is no extra'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='format of chunk files is lightweight because dimensional keys \\nare removed. Due to the metadata, key-cube and cell address, \\nare not stored data but are calculated results, there is no extra \\nwork required to maintain them. The other three stores have \\nto maintain more schema -related metadata . MongoDB also \\ntakes time to transfer TextFile format to BSON file format \\nwhich is complex. \\nTABLE 5. LOADING STEPS OF FIVE DATA STORES \\n \\n(2\\n) The loading speed of PostgresXL, a distributed RBDMS, \\nis faster and stable.  Not only the schema, but also the parti-\\ntion of the distributed table are predefined, and then the load-\\ning process is simpl y parallel insert operations on Postgres \\nnodes. Haery‘s loading speed is 0.834 times slower than Post-\\ngresXL. However, it is still comparable, and in the application \\nscenario of NoSQL database, the predefined schema is infea-\\nsible, while changing table schmea dynamically is not sup-\\nported by PostgresXL.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='scenario of NoSQL database, the predefined schema is infea-\\nsible, while changing table schmea dynamically is not sup-\\nported by PostgresXL. \\n(3) As reported in Figure 4 in [24], HyperDex loads at least \\n4 times faster than Cassandra and MongoDB. However, it is \\nincomparable slow in our experiments. It loads 0.25 MB data, \\napproximate 2000 rows, per second. In [24], the loading per-\\nformance is not discussed, w e infer that the following two \\nreasons lead to the different results: First, a different dataset is \\nused. In [24], there are only 10 7 rows and each row only con-\\ntains 10 attributes; while in our experiments, there are a max-\\nimum 109 rows and e ach row contains 120 keys.  HyperDex \\nperforms an ordered hash function on every value of keys in \\neach row, and then hashes the row to the Hyperspace, finally \\nreplicating the object to each subspace. A massive dataset \\ndemonstrates how costly these calculation are. Second, Hy-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='each row, and then hashes the row to the Hyperspace, finally \\nreplicating the object to each subspace. A massive dataset \\ndemonstrates how costly these calculation are. Second, Hy-\\nperDex does not provide batch loading tools so data is loaded \\nfrom the client machine to the servers row by row. An \\nacknowledgement of the previous load is required before the \\nnext one is executed.  \\n(4) The systems load data slower when more data is load-\\ned, but the downtrend is not significant. For Haery, it is be-\\ncause the cost of expansion and linearization when the data \\nvolume increases. For the other stores, it is because the cost of \\nstep (2) increases with the data volume. Besides, with HDFS, \\nthe cost of locating the file also increases with the data vol-\\nume. \\nB. Query \\nFigures 10 and 11 show the query time of each system on \\nthe test cases ( E1, E2, E3, E4, E5 and E6) using different datasets \\n(S1 to S5). Generally, the query performance of Haery is the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 10, 'page_label': '11'}, page_content='B. Query \\nFigures 10 and 11 show the query time of each system on \\nthe test cases ( E1, E2, E3, E4, E5 and E6) using different datasets \\n(S1 to S5). Generally, the query performance of Haery is the \\nbest of the NoSQL database s, and is also better than Post-\\ngresXL in some cases. Query performance of HyperDex is the \\nworst. On an average of all cases, Haery is 4.57x, 4.23x, 3.55x, \\n1.79x, 1.82x and 120.6x faster than Hive, HBase, Cassandra, \\nMongoDB, PostgresXL and HyperDex, respectively.  \\n0\\n2\\n4\\n6\\n8\\n10\\n12\\n14\\n24\\n 72\\n 120\\n 168\\n 216\\n 264\\n 312\\n 360\\nLoading \\nperformance\\n (MB/s)\\nData \\nvolume (GB)\\nHaery\\n HBase\\n MongoDB\\nCassandra\\n HyperDex\\n PostgresXL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n12 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\n \\nFig. 10. Query performance of the six data stores \\n \\nFig. 11. Query performance of HyperDex  \\nThe dataset size increases from 36 GB to 360 GB. And for \\nqueries with different query rates (E1 to E6), the query time of \\nmost stores increases with data volumes  except PostgresXL \\nand HyperDex, and also increases with the query rate s. It \\nmatches the common sense expectation of database queries: \\nthe query performce  is better when less data is queried, or'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='and HyperDex, and also increases with the query rate s. It \\nmatches the common sense expectation of database queries: \\nthe query performce  is better when less data is queried, or \\nless data is returned.  \\nAfter carefully studying these stores, we believe some op-\\ntimizations of Haery benefits the query performance, such as, \\nthe linearization algorithm ensures that adjace nt cells are ad-\\ndressed together, and preferring calculation rather than look-\\nup table when mapping logical space (cell space) to the physi-\\ncal space.  Howerer, the dominant optimization is sharding \\neither by vertical partitions on keys, or by horizontal parti-\\ntions on rows, or both. We explain the results from this point \\nof view in Table 6. Some important points to consider are as \\nfollows: \\n(1) The query process of Haery is similar to Hive, but the \\nperformance is quite a bit better. Hive’s query performance is \\nclose to a “full scan” query. \\n(2) The query process of HBase and Cassandra are similar,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='performance is quite a bit better. Hive’s query performance is \\nclose to a “full scan” query. \\n(2) The query process of HBase and Cassandra are similar, \\ntheir vertical partitions benefit performance . However, the \\nhorizontal partitions do not provide a benefit due to the high \\ndimensional queries. \\n(3) Without vertical partitions, MongoDB partit ions data \\nK0 and K1 horizontally . For our test data set, partitions on \\nthese two keys provide a significant benefit  because the val-\\nues of the other keys are the same as these two, however, this \\nis not a general situation.  \\n \\nTABLE 6. COMPARING THE QUERY OPTIMIZATION \\n \\n (4) With b enfits of  predefined schema, PostgresXL skip s \\nthe dataset where the query keys are null, and then only the \\nincreased part of data is sca nned. Referring to Table 4, S1 has \\n36 GB data and 374M rows; S2 increases 36GB data and 187M \\nrows, S3 increases 72GB data and 187M rows due to new rows \\nhave more keys, S4 increases 144GB data and 187M rows, fi-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='36 GB data and 374M rows; S2 increases 36GB data and 187M \\nrows, S3 increases 72GB data and 187M rows due to new rows \\nhave more keys, S4 increases 144GB data and 187M rows, fi-\\nnally S5 increases 72GB data and 74M rows. From S2 to S4, the \\nnumber of increased rows are same. It is why query time on \\nS2 is less than that on S1, and query time on S2 to S4 are almost \\nthe same. So far we are not sure why query performance of S5 \\nis worse than that of S2 to S4 since only 74M rows in S5 are \\nscanned. Maybe the 120-keys table is too wide for PostgresXL. \\nThe performance advantage of PostgresXL is not a fair com-\\nparison to the NoSQL databases because the keys cannot be \\npredefined or predicted, and no mechanism indicates wheth-\\ner a key is absent or not ex pected in  the data files that are \\nscanned. Haery may kno w a key, as a dimensional one, is \\nabsent by querying a key -cube, but Haery does not know \\nwhether the absent one is a trivial key without scanning the \\ntrivial data files.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='scanned. Haery may kno w a key, as a dimensional one, is \\nabsent by querying a key -cube, but Haery does not know \\nwhether the absent one is a trivial key without scanning the \\ntrivial data files. \\n(5) HyperDex also rel ies on a predefined schema. In [24], \\nflexibility is explained to be supported by multiple hyper-\\nspaces in a table, but it does not explain how these hyper-\\nspaces are created automatically, namely schema expansion. \\nIn our experiments, we create five hyperspaces for S1 to S5, \\nwith 12, 24, 48, 96 and 120 keys , respectively. So essentially \\nqueries are performed on different “tables”. It may be the \\nreason why the query performance is related to the increased \\nrow number and query rate . For the same query rate, que ry \\nperformance of S1 is the worst, S2 and S4 are similar, and S5 is \\nthe best. Even so, its performance is not comparable to the \\nother stores. As we explain in Table 6, HyperDex cannot meet \\nthe requirements of high -dimensional queries on high -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='the best. Even so, its performance is not comparable to the \\nother stores. As we explain in Table 6, HyperDex cannot meet \\nthe requirements of high -dimensional queries on high -\\ndimensional data. \\nC. Storage \\nThe storage costs of each system are compared in Figure 12. \\nHaery costs the least storage and HyperDex costs the most. \\nThe storage cost s of Hive and Cassandra are similar to the \\nsize of the original data set. Taking dataset S5 (360 GB) for \\nexample, the storage cost of Hive, HBase, Cassandra, Mon-\\ngoDB, PostgresXL and HyperDex is respectively 2.9x, 4. 7x, \\n2.6x, 5.7x, 3.9x and 8.8x larger than that of Haery. \\n0\\n1000\\n2000\\n3000\\n4000\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(a) E1 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(e) E5 \\n0\\n400\\n800\\n1200\\n1600\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(c) E3 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(d) E4 \\n0\\n1000\\n2000'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 11, 'page_label': '12'}, page_content='(e) E5 \\n0\\n400\\n800\\n1200\\n1600\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(c) E3 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(d) E4 \\n0\\n1000\\n2000\\n3000\\n4000\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(b) E2 \\nHaery\\nHive\\nHBase\\nMongoDB\\nCassandra\\nPostgresXL\\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 108\\n 180\\n 252\\n 324\\nQuery time (\\ns)\\nData \\nvolume (GB)\\n(f) E6 \\n0\\n2000\\n4000\\n6000\\n8000\\n10000\\nE1\\n E2\\n E3\\n E4\\n E5\\n E6\\nQuery time (\\ns)\\nQuery Case \\nE\\nS1\\n S2\\n S3\\n S4\\n S5'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\nJie, et al: Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data 13 \\n \\n \\nFig. 12. Storage cost performance on two cases of seven data stores \\nThe experimental results are explained as follows: \\n(1) As explained in the loading experiment, there is no ex-\\ntra data being stored in Hive. Hive directly adopts the origi-\\nnal file format of TextFile to store key -value pair, therefore, \\nthe storage cost remains the same as the original data set.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='tra data being stored in Hive. Hive directly adopts the origi-\\nnal file format of TextFile to store key -value pair, therefore, \\nthe storage cost remains the same as the original data set.  \\n(2) The storage strategy of Haery is as simple as that of \\nHive. Compared with Hive, Haery saves storage for two rea-\\nsons: First, dimensional keys are not stored because they are \\nfixed and can be inferred by their indexes; Second, ArrayFile \\nand SequenceFile save more storag e than TextFile. Therefore, \\nthe storage cost is even lower than the original dataset. \\n(3) HBase is column oriented. The keys and values are \\nstored as HFile, which contains data blocks and index blocks. \\nThe index block stores three levels of indexes: root index, in-\\ntermediate level index and leaf level index. Storing these ad-\\nditional indexes are a performance cost for HBase. Therefore, \\nthe storage cost of HBase is higher than the original data set. \\n (4) Cassandra is column oriented. The keys and values are'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='ditional indexes are a performance cost for HBase. Therefore, \\nthe storage cost of HBase is higher than the original data set. \\n (4) Cassandra is column oriented. The keys and values are \\nstored as SSTable, which contains data files, index files (key \\nindexes) and filter files. Cassandra does not introduce much \\nmetadata to SSTable, it also provides a compaction mecha-\\nnism which merges multiple SSTables into a new one. There-\\nfore, the storage cost of Cassandra is slight ly less than the \\noriginal data set. \\n (5) MongoDB is document oriented, using BSON as the \\nstorage structure. BSON is a binary -encoded serialization of \\nJSON-like documents. Not only are the same keys stored re-\\npeatedly, but also there are massive descriptors to be stored. \\nTherefore, the storage cost is approximately double that of the \\noriginal data set. \\n(6) The storage of PostgresXL on e ach node is a Postgres \\ndatabase. In Postgres, rows are stored in heap files. Heap files'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='original data set. \\n(6) The storage of PostgresXL on e ach node is a Postgres \\ndatabase. In Postgres, rows are stored in heap files. Heap files \\nare structured as a collection of blocks, each containing a row, \\nnamely a collection of values without keys. This saves storage \\ncompared to the original dataset. But, the system tables and \\nindex on \\nthe primary key take additional storage. Overall, the \\nstorage cost is larger than the original data set. \\n(7) HyperDex replicates the data for each subspaces of a  \\nhyperspace, also there are five  hyperspaces for a table corre-\\nsponding to five datasets . Therfore, HyperDex is the most \\nstorage consuming store. \\n9 CONCLUSION \\nThis paper presents the design, implementation, and evalua-\\ntion of Haery, a column oriented store for big data. Haery is \\nbuilt on Hadoop HDFS and a distributed compu ting frame-\\nwork, e.g. MapReduce. We proposed the following models \\nand algorithms:  \\n(1\\n) Key-cube, which is a high -dimensional data model as'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='built on Hadoop HDFS and a distributed compu ting frame-\\nwork, e.g. MapReduce. We proposed the following models \\nand algorithms:  \\n(1\\n) Key-cube, which is a high -dimensional data model as \\nthe logical schema of the keys and values;  \\n(2) An i mproved Z -order based linearization algorithm \\nand an address tree, to map the cells to directories;  \\n(3) Accumulation, which is a key-cube expansion approach \\nto maintain the efficiency of the key -cube when new data is \\nimported;  \\n(4) Query algorithms to imp lement queries on key -cubes \\nand physical storage;  \\n(5) The system architecture, components and implementa-\\ntion of Haery.  \\nOur experimental results show that the loading and query \\nperformance of Haery is the most stable and efficient. Due to \\nthe accumulative data models, the query performance in-\\ncreases slowly when the data volume increases. There is no \\nadditional storage cost in Haery, what’s more, the storage cost \\nof dimensional keys are also saved. There are some reasons'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='creases slowly when the data volume increases. There is no \\nadditional storage cost in Haery, what’s more, the storage cost \\nof dimensional keys are also saved. There are some reasons \\nleading to these advantages.  \\n(1\\n) Haery adopts a pre-defined key-cube which both sup-\\nports high-dimensional data and partitions data into regions. \\nQuery performance improved by greatly red ucing the search \\nspace. \\n(2) With linearization and the address tree, no additional \\nstorage is required to store the key -cube, the mapping rela-\\ntionship between cells and directories are calculated rather \\nthan queried, and the locality of queried data is considered to \\nimprove the addressing performance.  \\n(3) Accumulation ensures that the well -partitioned key -\\ncube remains efficient when new keys and values are import-\\ned to Haery. \\nIn the future, we would optimize Haery as follows: dis-\\ncussing whether the indexes of cells should be compressed, \\nhow to compress them, and whether the compression affects'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='ed to Haery. \\nIn the future, we would optimize Haery as follows: dis-\\ncussing whether the indexes of cells should be compressed, \\nhow to compress them, and whether the compression affects \\nthe query performance. And propose more sophisticated data \\nstructures for the address tree. \\nACKNOWLEDGMENT \\nThis research is supported by the National Natural Science \\nFoundation of China (61672143, 61433008, U1435216, \\n61662057, 61502090, 61402090).  And the Fundamental Re-\\nsearch Foundations for the Central Universities  (N1616020 \\n03). We thank Shenqiang HU, a master student of Software \\nCollege, Northeastern University, for conducting experiments \\nin the revision that greatly improved the paper.  \\nREFERENCES \\n[1] Agarwal S, Priyusha M K. “A Transformation from Relational \\nDatabases to Big Data.” J. International Journal of Advanced Trends \\nin Computer Science & Engineering , 2015. \\n[2] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreos'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 12, 'page_label': '13'}, page_content='Databases to Big Data.” J. International Journal of Advanced Trends \\nin Computer Science & Engineering , 2015. \\n[2] Daniel Abadi, Peter Boncz, Stavros Harizopoulos, Stratos Idreos \\nand Samuel Madden, \"The Design and Implementation of Mod-\\nern Column-Oriented Database Systems\", J Foundations and \\nTrends® in Databases: Vol. 5: No. 3, pp 197-280. 2013 \\n[3] Morton, G.M.: “A computer oriented geodetic data base; and a \\nnew technique in file sequencing. ” Technical report, IBM Ltd., \\nOttawa, Canada.1966 \\n[4] Pramod J. Sadalage; Martin Fowler, \"4: Distribution Models\", \\nNoSQL Distilled, ISBN 0321826620, 2012 \\n0\\n200\\n400\\n600\\n800\\n1000\\n1200\\n36\\n 72\\n 108\\n 144\\n 180\\n 216\\n 252\\n 288\\n 324\\n 360\\nActual Storage\\n (GB)\\nData \\nvolume (GB)\\nHaery\\n Hive\\nHBase\\n MongoDB\\nCassandra\\n HyperDex\\nPostgresXL'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='1041-4347 (c) 2018 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more\\ninformation.\\nThis article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI\\n10.1109/TKDE.2019.2904056, IEEE Transactions on Knowledge and Data Engineering\\n14 IEEE TRANSACTIONS ON journal name,  manuscript id \\n \\n[5] Wu S, Jiang D, Ooi B C, et al. “Efficient B-tree Based Indexing for \\nCloud Data Processing ” J. Proceedings of the Vldb Endowment , \\n2010, 3(12):1207-1218.  \\n[6] Jagadish H V, Ooi B C, Vu Q H. “BATON: a balanced tree struc-\\nture for peer-to -peer networks ” Proc. International Conference on \\nVery Large Data Bases. VLDB Endowment, 2006:661-672. \\n[7] Wang J, Wu S, Gao H, et al. “Indexing multi-dimensional data in'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='ture for peer-to -peer networks ” Proc. International Conference on \\nVery Large Data Bases. VLDB Endowment, 2006:661-672. \\n[7] Wang J, Wu S, Gao H, et al. “Indexing multi-dimensional data in \\na cloud system ” Proc. ACM SIGMOD International Conference on \\nManagement of Data , SIGMOD 2010, Indianapolis, Indiana, Usa, \\nJune. 2010:591-602. \\n[8] Zhang X, Ai  J, Wang Z, et al. “An efficient multi -dimensional \\nindex for cloud data management ” Proc. International CIKM \\nWorkshop on Cloud Data Management , Clouddb 2009, Hong Kong, \\nChina, November. 2009:17 -24. \\n[9] Papadopoulos A, Katsaros D. “A-Tree: Distributed Indexing of \\nMultidimensional Data for Cloud Computing Environments ” \\nProc. IEEE Third International Conference on Cloud Computing \\nTechnology and Science. IEEE, 2011:407-414. \\n[10] Alternate indexed hbase implementation; speeds scans by add-\\ning indexes to regions rather secondary tables George L. Availa-\\nble on https://issues.apache.org/jira/browse/HBASE-2037. 2017'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='[10] Alternate indexed hbase implementation; speeds scans by add-\\ning indexes to regions rather secondary tables George L. Availa-\\nble on https://issues.apache.org/jira/browse/HBASE-2037. 2017  \\n[11] Indexed HBase. “An extnestion of HBASE core which support \\nfaster scans at the expense of larger RAM consumption. ” \\nhttps://github.com/gkulbak/ibase. 2017 \\n[12] Zou Y, Liu J, Wang S, et al. “CCIndex: A Complemental Cluster-\\ning Index on Distributed Ordered Tables for Multi -dimensional \\nRange Queries ” Proc. Ifip International Conference on Network and \\nParallel Computing. Springer-Verlag, 2010:247-261. \\n[13] Nievergelt J. “The Grid File: An Adaptable, Symmetric Multikey \\nFile Structure .” J. Acm Transactions on Database Systems , 1984, \\n9(1):38-71. \\n[14] Bentley J L. “Multidimensional binary search trees used for as-\\nsociative searching” J. Communications of the Acm, 1975, 18(9):509-\\n517. \\n[15] Guttman A. “R-trees: a dynamic index structure for spatial'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='sociative searching” J. Communications of the Acm, 1975, 18(9):509-\\n517. \\n[15] Guttman A. “R-trees: a dynamic index structure for spatial \\nsearching” J. Acm Sigmod Record, 1984, 14(2):47-57. \\n[16] Nishimura S, Das S, Agrawal D, et al. “MD-HBase: A Scalable \\nMulti-dimensional Data Infrastructure for Location Aware Ser-\\nvices” Proc. IEEE International Conference on Mobile Data Manage-\\nment. 2011:7-16. \\n[17] Liu Y, Liu Y, Kaikuo X U, et al. “SHG-Tree: An Efficient Index \\nStructure of Spatial Database ,“ J. Journal of Frontiers of Computer \\nScience & Technology, 2009. \\n[18] Dong Daoguo, Liang Liuhong, Xue Xiangyang. “VAR-Tree-A \\nNew High-Dimensional Data Index Structure .” J. Journal of Com-\\nputer Research & Development, 2005, 42(1):10 -17. \\n[19] Liang J J, Feng Y C. “CSA-Tree: an optimized high-dimensional \\nindex tree for main memory access ” J. Chinese Journal of Comput-\\ners, 2007, 30(3):415-423. \\n[20] Lejsek H, Asmundsson F H, Jonsson B T, et al. “NV-tree: an'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='index tree for main memory access ” J. Chinese Journal of Comput-\\ners, 2007, 30(3):415-423. \\n[20] Lejsek H, Asmundsson F H, Jonsson B T, et al. “NV-tree: an \\nefficient disk -based index for approximate search in very large \\nhigh-dimensional collections. ” J. IEEE Transactions on Pattern \\nAnalysis & Machine Intelligence, 2008, 31(5):869-883. \\n[21] Mior M J, Salem K, Aboulnaga A, et al. “NoSE: Schema design \\nfor NoSQL applications ” Proc. Data Engineering (ICDE) , 2016 \\nIEEE 32nd International Conference on. IEEE, 2016: 181-192. \\n[22] Vajk T, Deák L, Fekete K, et al. “Automatic NoSQL schema de-\\nvelopment: A case  study” Proc. Artificial Intelligence and Applica-\\ntions. 2013: 656-663. \\n[23] Escriva R., Bernard W., and Emin G . S. \"HyperDex: A distribut-\\ned, searchable key-value store.\" Proc. ACM SIGCOMM 2012 con-\\nference on Applications, technologies, architectures, and protocols for \\ncomputer communication. ACM, 2012. \\n[24] Escriva R., Bernard W., and Emin G. S. \"HyperDex: A distribut-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='ference on Applications, technologies, architectures, and protocols for \\ncomputer communication. ACM, 2012. \\n[24] Escriva R., Bernard W., and Emin G. S. \"HyperDex: A distribut-\\ned, searchable key-value store for cloud computing.\" Computer \\nScience Department, Cornell University Technical Report \\n(2011). \\n[25] Chevalier M., El Malki M., Kopliku A., Teste O., Tournier R. \\nHow Can We Implement a Multidimensional Data Warehouse \\nUsing NoSQL?. In: Hammoudi S., Maciaszek L., Teniente E., \\nCamp O., Cordeiro J. (eds) Enterprise Information Systems. Lec-\\nture Notes in Business Information Processing, vol 241. Springer, \\nCham. 2015 \\n[26] Iqbal A, Wang H, Gao Q. “A Histogram Method for Summariz-\\ning Multi-dimensional Probabilistic Data ” J. Procedia Computer \\nScience, 2013, 19:971-976. \\n[27] Song J, Guo C, Wang Z, et al. “HaoLap: a Hadoop based OLAP \\nsystem for big\\n data.” J . Journal of Systems and Software, 2015, 102: \\n167-181. \\n[28] Asano T, Ranjan D, Roos T, et al. “Space Filling Curves and'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='system for big\\n data.” J . Journal of Systems and Software, 2015, 102: \\n167-181. \\n[28] Asano T, Ranjan D, Roos T, et al. “Space Filling Curves and \\nTheir Use in the Design of Geometric Data Structures ” Proc. Lat-\\nin American Symposium on Theoretical Informatics. Springer -\\nVerlag, 1997:3-15 \\n[29] Mokbel M F, Aref W G, Kamel I. “Analysis of Multi-\\nDimensional Space-Filling Curves ” J. GeoInformatica, 2003, \\n7(3):179-209. \\n[30] Cooper, Brian F., et al. \"Benchmarking cloud serving systems \\nwith YCSB.\" Proc. the 1st ACM symposium on Cloud computing . \\nACM, 2010. \\n[31] Roger Nelson, “ How many sub-directories should be put into a \\ndirectory”. https: //stackoverflow.com/questions/494730/ how-\\nmany-sub-directories-should-be-put-into-a-directory. 2017  \\n[32] Kai H. , Jack D. , Geoffrey C. F.. “Cloud computing and distrib-\\nuted systems: from parallel processing to the Internet of things ” \\nM, Morgan Kaufmann Publishers Inc , 2011 \\n \\n \\n \\n \\nJie Song Ph.D, \\nassociate professor,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2013; modified using iText® 7.1.1 ©2000-2018 iText Group NV (AGPL-version)', 'creator': 'Appligent AppendPDF Pro 5.5', 'creationdate': '2019-03-08T14:31:22-05:00', 'ieee issue id': '4358933', 'ieee article id': '8664133', 'title': 'Haery: a Hadoop based Query System on Accumulative and High-dimensional Data Model for Big Data', 'ieee publication id': '69', 'appligent': 'AppendPDF Pro 5.5 Linux Kernel 2.6 64bit Oct  2 2014 Library 10.1.0', 'subject': 'IEEE Transactions on Knowledge and Data Engineering; ;PP;99;10.1109/TKDE.2019.2904056', 'moddate': '2019-03-08T15:23:45-05:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[Data_Modeling_&_QueryOptimization]_Haery_a_Hadoop_based_Query_System_on_Accumulative.pdf', 'total_pages': 14, 'page': 13, 'page_label': '14'}, page_content='uted systems: from parallel processing to the Internet of things ” \\nM, Morgan Kaufmann Publishers Inc , 2011 \\n \\n \\n \\n \\nJie Song Ph.D, \\nassociate professor, \\nsongjie@mail.neu.e\\ndu.cn, his research \\ninterests include big \\ndata management  \\nand energy-efficient \\ncomputing. \\n  \\nHongYan HE  M.S \\ncandidate, her \\ncurrent research \\nfocuses on big \\ndata management. \\n \\n Richard Thomas  \\nPh.D., senior lec-\\nture, rich-\\nard.thomas@uq.edu\\n.au, his research \\ninterests include big \\ndata and software \\nengineering. \\n  \\nYubin BAO Ph.D., \\nprofessor, \\nbaoyb@mail.neu.e\\ndu.cn, his re-\\nsearch interests \\ninclude big data \\nmanagement. \\n \\n \\nGe Yu Ph.D., pro-\\nfessor, \\nyuge@mail.neu.edu.\\ncn, his research \\ninterests include \\ndatabase theory.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 0, 'page_label': '1'}, page_content='Nhan Nguyen \\nBuilding an E-commerce Application \\nUtilizing Firebase Cloud service  \\nMetropolia University of Applied Sciences \\nBachelor of Engineering \\nMobile solutions \\nBachelor’s Thesis \\n28 March 2022'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2'}, page_content='Abstract  \\nAuthor: Nhan Nguyen \\nTitle: Building an E-commerce Application Utilizing Cloud \\nservice \\nNumber of Pages: 33 pages + 1 appendices \\nDate: 28 March 2022 \\nDegree: Bachelor of Engineering \\nDegree Programme: Information of Technology \\nProfessional Major: Mobile Solutions \\nSupervisors: Antti Piironen, Principal Lecturer \\n \\nThe primary purpose of the thesis is to illustrate the way of establishing a serverless \\nbackend for the application by utilizing Firebase services. Besides, the thesis also \\ninvestigated the implementation and functioning of Firebase as well as its services. \\n \\nThe thesis is to establish a conceptual framework of Firebase services via an \\nexamination of Firebase documents and the implementation of a project utilizing \\nFirebase services and tools. Additionally, this research demonstrates how to create \\nFirebase project in the Firebase interface and how to integrate Firebase SDKs into the \\napplication.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 1, 'page_label': '2'}, page_content='Firebase services and tools. Additionally, this research demonstrates how to create \\nFirebase project in the Firebase interface and how to integrate Firebase SDKs into the \\napplication. \\n \\nThe thesis project culminates in the creation of an e -commerce application prototype. \\nThe application’s objective is to enable individuals to purchase and sell things related \\nto their own demands. The E -commerce website exists as a practical tool in order to \\nhelp firms handle orders, receive payments and manage logistics. All key aspects of \\nthe working prototype application, in general, were successfully implemented, apart \\nfrom a few that were excluded for different causes. \\n \\nApplying serverless backend service such as Firebase brings a huge benefit to people, \\nespecially developers, in terms of the significant reduction of maintenance of the \\ntraditional backend. \\nKeywords: Firebase, React, web applications'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 2, 'page_label': '3'}, page_content='Contents \\nList of Abbreviations \\n1 Introduction 1 \\n2 ReactJS 3 \\n2.1 React Virtual DOM 3 \\n2.2 Data Flow 5 \\n2.3 React Components 6 \\n2.4 JSX Syntax 6 \\n3 Firebase 8 \\n3.1 Overview 8 \\n3.2 Firebase Authentication service 9 \\n3.3 Cloud Function for Firebase service 11 \\n3.4 The Real-time Database services 12 \\n3.5 Firebase Hosting service 14 \\n3.6 Cloud Storage for Firebase service 16 \\n4 Implementation and result 18 \\n4.1 Project environment 18 \\n4.2 Project structure 19 \\n4.3 Elements of web application 20 \\n4.3.1 HTML 20 \\n4.3.2 Stylesheets CSS 18 \\n4.3.3 Javascript 19 \\n4.4 Firebase configuration 24 \\n4.5 Interface layout 25 \\n5 Deploying the website 29 \\n6 Testing 30 \\n7 Discussion 31 \\n8 Conclusion 33 \\nReferences 1'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5'}, page_content='List of Abbreviations \\nAJAX: Asynchronous JavaScript and XML is a collection of client -side web \\ndevelopment tools for creating asynchronous web applications that \\nmay transmit and receive data without interfering with other \\ncomponents of the user interface. \\nAPI: The Program Programming Interface (API) is a collection of public \\nmethods and attributes that your application utilizes to communicate \\nwith other objects. \\nDOM: The Document Object Model (DOM) is an HTML and XML document \\nprogramming interface. It depicts the page so that programs may \\nalter the structure, style, and content of the document. It uses nodes \\nand objects to represent the document. \\nJSON: JSON stands for JavaScript Object Notation and is a lightweight data \\nstorage and transmission format. When data is transmitted from a \\nserver to a web page, this is often utilized. \\nNpm: Node package manager is a package manag er that manages the \\nexternal code dependencies of an application.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 4, 'page_label': '5'}, page_content='server to a web page, this is often utilized. \\nNpm: Node package manager is a package manag er that manages the \\nexternal code dependencies of an application. \\nReact: It is a JavaScript framework and tool for creating user interface \\ncomponents. \\nRedux: It is a state container for JavaScript programs that is predictable. \\nREST: Representational State Transfer is a software architecture style that \\nestablishes web -based protocols for communication between \\ncomputer systems. \\nUI: User Interface refers to the area where people and machines \\ninteract.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 5, 'page_label': '6'}, page_content='JSX: Javascript XML is a React syntax that enables HTML to be included \\nin JavaScript code. \\nMVC: Model–view–controller is a software design pattern that is often used \\nin the development of user interfaces.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, page_content='1 \\n \\n1 Introduction \\nThe web has progressed from serving static HTML content to serving application-\\nlike complex user interfaces that are sometimes as capable as their native \\nequivalents. By contrast, Web browsers have stayed relatively stable throughout \\ntime. \\nStatistics show that mobile devices accounted for 52.2% of total internet traffic in \\n2019. However, it takes an average of 22 seconds for a mobile landing page to \\ncompletely load. When you consider that 53% of users will abandon a mobile \\nwebsite if it takes mor e than 3 seconds to load, it  is no surprise that improved \\nmobile experiences are in high demand. \\nA huge percentage of web users have switched to native apps nowadays. This \\nsector has developed particularly rapidly in the e -commerce market, with a 54 \\npercent rise in 2017 alone. In exchange, spending time in browsers has declined \\ndramatically, and web developers are increasingly looking for methods to stay up \\nwith the experiences that native apps provide.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, page_content=\"dramatically, and web developers are increasingly looking for methods to stay up \\nwith the experiences that native apps provide. \\nJavaScript is currently one of the most powerful web d evelopment programming \\nlanguages available. JavaScript allows programmers to create extremely \\nresponsive websites with dynamic functionality that responds to user requests, \\nenhancing the user experience. \\nReact generates a memory -based data structure cache.  When the code is \\namended, React detects the changes and updates the browser accordingly. This \\nunique feature will improve the webpage's efficiency since the React library only \\nrenders components that change, rather than loading the whole page as in other \\nmethods. That is why, in this thesis, React, an open-source JavaScript toolkit, is \\nutilized to create a user-friendly, scalable, and high-performance homepage.  \\nThe purpose of this thesis  is to create a working responsive e -commerce web\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 6, 'page_label': '7'}, page_content='utilized to create a user-friendly, scalable, and high-performance homepage.  \\nThe purpose of this thesis  is to create a working responsive e -commerce web \\napplication for an online retailer using React and Firebase t o assist businesses'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8'}, page_content=\"2 \\n \\nin handling orders, receiving payments, and managing logistical processes and \\nprocedures. The React will handle the work in front -end, meanwhile, Firebase \\ncloud service helps manage the data and authentication. \\nThe thesis is divided into 8 sections. The sections of this thesis are organized in \\nthe same order as the phases of implementation of the project that was \\ndeveloped in this thesis. Sections 1 and 2 provide an overview of the technologies \\nthat will be used in the projects. The first section is devoted to ReactJs. The \\nsecond section delves deeper into Firebase. Additionally, the purpose of this part \\nis to analyze the advantages of using numerous services on Cloud Firebase. \\nFollowing the project's execution stages, Sections 4, 5, and 6 cover everything \\nfrom the design of the architecture to specific examples, all while discussing the \\ntechnologies that were employed to reach the ultimate result. Section 6 focuses\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 7, 'page_label': '8'}, page_content=\"from the design of the architecture to specific examples, all while discussing the \\ntechnologies that were employed to reach the ultimate result. Section 6 focuses \\non the testing procedure. Section 7 discusses the application's outcome and the \\nconclusion of the thesis. Finally, there are parts dedicated to observations on and \\ndebate of the whole endeavour.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9'}, page_content='3 \\n \\n2 ReactJS \\nReact is a library for designing composable user interfaces. As a result, \\nreusable UI components are more likely to be created, and data that changes \\nover time may be shown. The V in MVC is often replaced with React. React \\ntakes away the DOM from you, enabling a simpler programming approach and \\ngreater performance. \\n2.1 React Virtual DOM \\nThe DOM is an acronym for Document Object Model. It is commonly referred to \\nplay an important role of the contemporary internet nowadays. It is basica lly an \\nabstraction of how online documents look and work. However, it is not as fast as \\nother JavaScript operations since most of frameworks update not the part but \\nwhole the DOM without requirement. Although they are  not essential, these \\nupdates are made automatically even if you do not ask for them. Assume that \\nnine goods have been placed in a shopping cart at an online web store. For \\nexample, changing a name in a list of (n) names would cost (n+1) times harder'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 8, 'page_label': '9'}, page_content='nine goods have been placed in a shopping cart at an online web store. For \\nexample, changing a name in a list of (n) names would cost (n+1) times harder \\nfor a normal technology to recreate exact the same amount of names in that list. \\nThe Virtual DOM is existed before the creation of React, however, React makes \\nit accessible to developers for free. Virtual Dom is technically representation of \\nthe HTML Document Object  Model. For any DOM object, for instance a \\ncorrespondent or a light copy .  React has a virtual DOM object  of its own . The \\ncharacteristics of a virtual DOM are comparable to those of a real DOM. On the \\nother hand, it is unable to alter the perspective immed iately. It takes a long time \\nto manipulate the DOM. Update of Virtual DOM, on the other hand, is quicker \\ncomponent as well as does not alter the screen.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10'}, page_content='4 \\n \\n \\nFigure 1. How the DOM working in React [1] \\nAs shown in Figure 1, the virtual DOM in React is a duplicate of the actual DOM \\nthat is used for testing. React employs a technique known as \"diffing,\" which \\nmeans that when a JSX element is rendered, every single Virtual DOM element \\nis changed. This may seem to be inefficient, however, it is not since Virtual DOM \\nis very quick to update and has no influence on the overall process. In order to \\ndetect which virtual DOM has been altered, React compares an updated state of \\nthe DOM with an earlier state of the DOM. When React recognizes that the DOMs \\nhave been modified, it only updates the objects that have been modified to the \\nactual DOM. \\nAs a result, React accelerates the updating process by using Virtual DOM. In the \\nabove example, React would have just updated the changed item, leaving the \\nother things unchanged. When changing a page in an application, this makes a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 9, 'page_label': '10'}, page_content='above example, React would have just updated the changed item, leaving the \\nother things unchanged. When changing a page in an application, this makes a \\ndifference since React may only make modifications to the areas of the DOM that \\nare necessary. A large part of the rationale for React increasing popularity within \\nthe developer community is due to this virtual DOM modification mechanism. \\nWhen utilizing React Virtual DOM, there are benefits and drawbacks to consider. \\nThe use of JSX and hyper script enables us to create multiple frontends for a'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, page_content=\"5 \\n \\nsingle application; allow the creation of apps without having to consider state \\ntransitions; no need for React to be used simple and effective in raising output; \\nReact diffing techniques are very quick and efficient are just a few of the many \\nadvantages of the ReactJS library. Mean while, React has a few drawbacks, \\nincluding a lack of differentiation between static and dynamic components and \\nthe use of a substantial amount of memory. In memory, a full copy of the DOM is \\npreserved. \\n2.2 Data Flow \\nFramework, for example, like Angular use tw o-way data binding. When using \\nAngular's two -way data binding, for example, changing the model instantly \\nupdates the view and the other way around. An input field in the model may also \\nalter the model. It works well in the majority of cases, however switch ing to a \\ndifferent model may trigger cascade changes in other models. Again, because \\nthe state is modifiable by both view and controller, the data flow might be\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, page_content=\"different model may trigger cascade changes in other models. Again, because \\nthe state is modifiable by both view and controller, the data flow might be \\nunexpected in certain instances. Flux or Redux with React might be a better \\napproach to prevent such uncertainties as both designs follow one-way data flow. \\nCascading updates and modifications are not visible in a one-way data flow.  \\nAn application's states and models may be more tightly coordinated when data is \\nsent in a single path across the syst em. Additionally, a one -way data flow \\nsimplifies and clarifies the design. Flux architecture is a functional approach. In \\nthis case, the view is seen as a result of the current state of the application. \\nEventually, if the state receives some modifications the view also gets re -\\nrendered 16 automatically. In addition, the states provide a comparable \\nperspective, which aids in the application's comprehension and predictability.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 10, 'page_label': '11'}, page_content=\"rendered 16 automatically. In addition, the states provide a comparable \\nperspective, which aids in the application's comprehension and predictability. \\nData from parent to child in an application flow in a single path to increase \\npredictability. Any data may be updated from any view, anytime under this \\ntechnique. This also simplifies debugging if anything goes wrong.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, page_content='6 \\n \\n2.3 React Components \\nComponents are critical in React. To form the full user interface, these individual \\ncomponents are then layered within one another. When using components, it \\nbreaks up user interface into smaller, more manageable sections so that \\ndeveloper can focus on each one individually. The term UI refers to the user \\ninterface, or what is shown on the screen. Components  operate similarly to \\nJavaScript functions. They execute the exact same goal, but in quite distinct \\nenvironments and with very different techniques. They, like functions, accept \\nprops as inputs and return React components. These components define what \\nthe user sees on the screen while interacting with the interface. React \\ncomponents may be used to build a whole interface or simply parts of it. \\nReact components may be built in the same way that a JavaScript function is. \\nThis method takes props as arguments and returns text of html which normally is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, page_content='React components may be built in the same way that a JavaScript function is. \\nThis method takes props as arguments and returns text of html which normally is \\ncalled as JSX. These components are referred to as functional components. \\nAdditionally, a React component may be generated in a variety of various \\nmethods. Extending, inheriting, or deriving a class from  the primary component \\ntied to an object is another method of creating a component. [2] \\nAdditionally, stateless functional components are possible. By rendering each \\ncomponent, the user interface experience becomes more responsive and \\nefficient. \\n2.4 JSX Syntax \\nJSX is neither a string nor a hypertext markup language. It is a JavaScript \\nsyntactic extension with statically typed syntax. It is comparable to an object -\\noriented programming language that is optimized for use with current web \\nbrowsers. To design and develop the user interface, it is advised to utilize JSX in'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 11, 'page_label': '12'}, page_content='oriented programming language that is optimized for use with current web \\nbrowsers. To design and develop the user interface, it is advised to utilize JSX in \\nconjunction with React. While it has all of the capabilities of JavaScript, it may'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13'}, page_content=\"7 \\n \\nseem to be a template language at first look. JSX is used to generate the React \\nelement. It is also capable of being displayed to the React Virtual DOM. \\nTo begin, it is faster: Because the JSX source code is converted to JavaScript, \\nthe outcome is highly optimized. When compared to similar JavaScript code, JSX \\nproduced code is quicker. JSX has been shown to be 13 % quicker on iOS and \\n29% faster in Android. \\nSecond, it is more secure: JSX is statically coded and largely typesafe, in contrast \\nto JavaScript. When programs are created with JSX, their quality improves \\nsignificantly, since many problems are discovered dur ing the compilation \\nprocess. Additionally, it provides debugging capabilities at the compiler level.  \\nTo round things up, JSX provides a class structure that is extremely comparable \\nto Java, which means that developers no longer have to deal with JavaScrip t's \\nbasic prototype-based inheritance system. However, since the expressions and\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 12, 'page_label': '13'}, page_content=\"to Java, which means that developers no longer have to deal with JavaScrip t's \\nbasic prototype-based inheritance system. However, since the expressions and \\nstatements in JSX are almost comparable to those in JavaScript, programmers \\nalready familiar with JavaScript may begin using it right away. Additionally, there \\nare proposals f or language -services for editors / integrated programming \\nenvironments (IDEs), such as code completion to make coding easier. \\nTo assist with the presentation of nested components, JSX elements may be \\ngiven as children. Different kinds of children may be co mbined, allowing for the \\nusage of JSX children with string literals. This is another JSX attribute that \\ncorresponds to an HTML element. [3] \\nMultiple children may be added to a JSX expression. Therefore, it must be \\nincluded in a div if the component is requ ired to render several items. Within \\nenclosing, JavaScript expressions may be passed as children.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14'}, page_content=\"8 \\n \\n3 Firebase \\nFirebase developed from Envolve, a former firm started by James Tamplin and \\nAndrew Lee in 2011 offer a backend development solution. The feature \\ncombination in Firebase speeds the integration of cloud databases into online \\nand mobile applications. Firebase is a Backend as a Service platform that \\nsignificantly lowers configuration and setup time. \\n3.1 Overview \\nFirebase is a Cloud -hosted, NoSQL database tha t employs a document -model. \\nWhile allowing for real -time data sharing and syncing across users, it can be \\nscaled horizontally. This is ideal for cross-platform apps, such as those for mobile \\nphones. In addition to server -based applications, Firebase's robu st user-based \\nsecurity makes it ideal for off-line usage. \\nFirebase is a self-scaling service built on top of Google's infrastructure. Firebase \\nadds analytics, authentication, performance monitoring, messaging, and crash \\nreporting to the features of a typic al NoSQL database. The integration is\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 13, 'page_label': '14'}, page_content='adds analytics, authentication, performance monitoring, messaging, and crash \\nreporting to the features of a typic al NoSQL database. The integration is \\nextensive, given it is a Google product. Additionally, Google Ads, the Google Play \\nStore are also integrated with AdMob. \\nCreating a Firebase application from the ground up is not difficult. As seen in \\nFigure 2, the app lication may be classified as developing, growing, or earning. \\nDevelopers are able to make their own selections to use these pillars. Certain \\ntechnologies, such as A/B Testing, Analytics, App Distribution are provided free \\nof charge.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 14, 'page_label': '15'}, page_content='9 \\n \\n \\nFigure 2. The overview of Firebase Cloud service [4] \\n3.2 Firebase Authentication service \\nCurrently, many online services rely on authentication in order to identify users \\nas well as safeguard data by limiting access. For example, Google requires an \\naccount and password to access some programs. The authentication process \\nhas been complicated by the addition of a third -party authentication mechanism \\n- APIs. Thus, A simple API that allows users to log in through federated providers \\nis provided by Firebase to complete the authentication process. It also works in \\nconcert with the real-time database to limit access, as demonstrated in Figure 3.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16'}, page_content=\"10 \\n \\n \\nFigure 3. Firebase Authentication service [5] \\nGoogle, GitHub, Facebook, and Twitter are all popular federated providers. If \\ndevelopers alter Firebase, they do not need to sign in again, since it is already \\nconnected with these providers' authentication systems. A connection to the \\nauthentication system will be establ ished as soon as an account has been \\nvalidated in the database. \\nPassword recovery and user verification are both available via the Firebase user \\ninterface. Users may create a temporary account without revealing their identity, \\nwhich is then linked to their  federated provider -based account. Additionally, it \\nsupports a technology named Smart Lock in remembering and signing in \\nautomatically using the credentials. The Firebase authentication SDK requires \\nthe user's email address as a sign-in credential.  \\nFirebase Auth requires the least amount of time to implement into your code. The\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 15, 'page_label': '16'}, page_content=\"the user's email address as a sign-in credential.  \\nFirebase Auth requires the least amount of time to implement into your code. The \\nreal Firebase authentication project takes more time to develop than Firebase \\nAuth. It is an online library that enables clients to alter typical Firebase situations \\nin order to give user authentication implementations in the form of user interface \\nscreens. Firebase Auth supports a variety of authentication methods on the web \\nand on mobile devices and is very simple to use. There are three primary stages \\nto integrating Firebase Auth user authentication into apps such as configuring\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, page_content=\"11 \\n \\nsign-in methods, configuring the sign-in UI, and completing the sign-in flow using \\nFirebase Auth. \\nThe Firebase Authentication SDK is somewhat more time -consuming than the \\nprevious one. Additionally, it enab les authentication process verification over \\ncomplete management of the outlook. Eventually, this option will maintain a \\ncomprehensive authentication process that includes several stages to \\nauthenticate the user's account. It is used to perform a variety o f functions, \\nincluding sign-in and sign-out, automated log-in, and data updating for new users. \\nAdditionally, the Firebase Authentication SDK facilitates connection with identity \\nproviders and anonymous authentication. Firebase Authentication is generally \\napproached via the Firebase Authentication SDK. There are the following steps \\nsuch as establish a sign -in method, create user interface flows for your login \\nmethods, and provide the user's credentials to the Firebase Authentication SDK.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, page_content=\"such as establish a sign -in method, create user interface flows for your login \\nmethods, and provide the user's credentials to the Firebase Authentication SDK. \\n3.3 Cloud Function for Firebase service \\nAs seen in Figure 4, Cloud Functions is a component of the Firebase and Google \\nCloud platforms that enables developers to handle fewer configurations, such as \\napp updates and general security for Google services and Firebase products. A \\nCloud function connects the Firebase application to the Google server, which \\nexecutes it. It minimizes boilerplate code by generating an admin SDK, third-party \\nservices, and a Cloud Function. The command is used to scale the design's \\nusage automatically depe nding on the computing data. As a result, human \\nmaintenance is not required for updating settings, credentials, or \\ndecommissioning. Cloud Function assumes users' security and privacy \\nobligations. Following the addition of a command line, the Cloud Function  is\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 16, 'page_label': '17'}, page_content=\"decommissioning. Cloud Function assumes users' security and privacy \\nobligations. Following the addition of a command line, the Cloud Function  is \\nisolated from the application logic, allowing the client to freely deploy code on the \\nprimary server. Since a result, developers choose to master their application, as \\nthese are delivered through Google and Firebase features.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18'}, page_content=\"12 \\n \\n \\nFigure 4. Cloud Function for Firebase [6] \\nThe Firebase project is deployed with a specified provider to enable this \\ncapability. Then, using JavaScript, the Cloud Function and Firebase CLI are \\ninstalled to manage the desired services. Through the Firebase CLI, the Firebase \\nconsole may be used to control and see a function: create a Cloud Function, write \\nit, deploy it, and monitor it. \\n3.4 The Real-time Database services \\nTraditionally, real -time databases have been used to demonstrate how a \\ndatabase system may overcome the real -time limitation in order to create a \\nreliable database system. Due to the fact that it offers an easy API -based \\ncapabilities, this is Firebase's flagship service. When new information is added, it \\nis automatically synced as JSON across all devices. A real-time database is one \\nthat works with both web and mobile versions on the same device and exchanges \\ndata between devices in an extremely short period of time, whereas a batch\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 17, 'page_label': '18'}, page_content='that works with both web and mobile versions on the same device and exchanges \\ndata between devices in an extremely short period of time, whereas a batch \\ndatabase takes a long time to sync.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19'}, page_content='13 \\n \\n \\nFigure 5. The example of Firebase real-time database. \\nIt restricts access to particular information by using a few codes provided by the \\nclient. No operation or maintenance server is necessary since the cloud service \\nprovides a real -time database that is ac cessible from anywhere (based on \\nNoSQL). The Real-time database, like the other products, is available for Android, \\niOS, Web, C++, Unity Platform, and JavaScript, in addition to the other platforms \\nmentioned above. There are up to 100 000 concurrent connec tions and 1,000 \\nconnections per second supported by each database in Real-time Database. \\nIn order to increase client access time, the data set is adjusted and improved due \\nto the use of a new database and integration of the real-time database with Cocoa \\nPods or Gradle, among other factors. Additionally, this technique allows for \\nwriting to take place even when the client is not connected, while still'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 18, 'page_label': '19'}, page_content='Pods or Gradle, among other factors. Additionally, this technique allows for \\nwriting to take place even when the client is not connected, while still \\nguaranteeing client security and confidentiality. For the purpose of constructing a \\nFirebase Real-time Database, the following are the five basic stages: Configure \\nData and Listen for Changes, enable Offline Persistence, and safeguard your \\ndata by integrating the Firebase Realtime Database SDKs.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20'}, page_content='14 \\n \\nIn order to do this, the Firebase SDKs platform, which communicates  with the \\nFirebase Cloud service, was established. Administrators have the ability to read, \\nprogrammatically send, access, and establish Firebase Auth via the \\nAdministration SDK. To understand how Cloud Storage works, consider the \\nfollowing steps: connect the Firebase SDKs for Cloud Storage, create a \\nreference, upload or download your files, and safeguard them. \\n3.5 Firebase Hosting service \\nWeb developers may use Firebase Hosting to host not just static websites, but \\nalso dynamic websites that are entirely built of HTML, CSS, and JavaScript files. \\nIn order to satisfy a range of needs, it is a software that maintains static web -\\nhosting services. Because HTTPS is based on SSL, consumers gain from faster \\nwebsite access, while developers profit from the ability to launch web applications \\nwithout registering an HTTPS connection certificate with the server. The Firebase'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 19, 'page_label': '20'}, page_content='website access, while developers profit from the ability to launch web applications \\nwithout registering an HTTPS connection certificate with the server. The Firebase \\nCommand Line Interface (CLI) allows for the creation and processing of Firebase \\nin a matter of seconds.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 20, 'page_label': '21'}, page_content='15 \\n \\n \\nFigure 6. The flow of using Firebase Hosting [7] \\nWhen Firebase Hosting is used in conjunction with a contemporary website, the \\nmaterial is totally protected using zero -configure SSL. The chemicals are \\nreceived quickly since Firebase Hosti ng is cached on SSDs and CDN edges. \\nAdditionally, it has an undo feature in case of development errors, making \\nFirebase hosting a complete management solution. \\nTo run Firebase Hosting, the Firebase CLI is considered a full -featured solution \\nfor websites, a pplications, and Progressive Web Apps (PWA). The following \\nphases should be noted for front -end implementation of features, infrastructure, \\nand customized tooling: Install the Firebase command-line interface (CLI), create \\na project directory, and publish the site.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22'}, page_content='16 \\n \\n3.6 Cloud Storage for Firebase service \\nVideos, images, and documents are stored online using Firebase Cloud Storage, \\nand they are backed up using Google Cloud Storage (which is free). The \\ninfrastructure for processing large files is created and mainta ined via the use of \\na clear API. As a result, the Realtime Database Rule enables for the distribution \\nof documents to certain individuals. This shows that since it is associated with \\nFirebase Authentication, the storage has been cached for faster access. \\nUploading and distributing files from mobile devices that are accessible to both \\nGoogle Cloud and Firebase is straightforward with the help of Firebase SDKs, \\nwhich are available on the Firebase website. Due to the automated scaling \\ncapabilities of Cloud Sto rage, there is no need to move the document to a \\ndifferent storage service provider. Depending on the file type, filename, or file \\nsize, declarative security for Firebase files may be given. Although the network is'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 21, 'page_label': '22'}, page_content='different storage service provider. Depending on the file type, filename, or file \\nsize, declarative security for Firebase files may be given. Although the network is \\ndisqualified, Firebase SDKs continue to o perate in an unbroken manner; for \\nexample, if a video is not successfully uploaded, the movie will resume \\ntransmission from the point at which it was interrupted when a proper internet \\nconnection is obtained. \\n \\nFigure 7. An example of Cloud Storage for Firebase service'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 22, 'page_label': '23'}, page_content='17 \\n \\nFor this purpose, the Firebase SDKs platform was created to communicate with \\nthe Firebase Cloud service, which was developed by Google. Firebase \\nAuthentication may be read, programmatically sent, and accessed via the Admin \\nSDK, which is available only in a privileged context. To demonstrate how Cloud \\nStorage works, the following procedures are taken: Creating a reference, \\nuploading or downloading your files, and safeguarding them are all possible using \\nthe Firebase SDKs for Cloud Storage.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24'}, page_content='18 \\n \\n4 Implementation and result \\nIn this section, the essential phases of the development process and the results \\nproduced are described. A well performing website is constructed with five main \\npages. The Authentication page is the webs ite entrance that asks user \\nidentification, which is accomplished by provide username and password. \\nAuthentication is needed to access an entire site. The Homepage enables visitors \\nto do a variety of tasks, including seeing a list of goods with photographs, filtering \\nthe products by name as well as category, selecting the products which is \\ndisplayed in the Cart page. In general, the Order page summarizes all user -\\nplaced orders. Following that comes the Cart page. The Cart page enables \\nvisitors to browse the list of items. Administrator accounts enable users to control \\nall items, users, and orders from the admin page. \\n4.1 Project environment \\nPrior to developing the application, the environment must be configured. The'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 23, 'page_label': '24'}, page_content='all items, users, and orders from the admin page. \\n4.1 Project environment \\nPrior to developing the application, the environment must be configured. The \\nNodeJS version 13.12.0 is the first item on the list of the tools that are being \\nused here. Essentially, NodeJS is a tool that supports JavaScript on both the \\nclient and the server. Node.js is a ground-breaking platform that supports both \\nthe client and the server. \\nThe next tool is Visual Studio Code, which is one of the most popular software \\ndevelopment tools today since it is packed into a single graphical user interface \\nand includes certain snippets such as ES7 React/Redux to make writing easier \\nand more convenient. In addition, the Firebase CLI version 10.5.0 maintains, \\nviews, and deploys Firebase projects using a variety of tools, and from there, \\nwe can configure communication between the Firebase Cloud and a local \\nproject, which is the last step.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 24, 'page_label': '25'}, page_content=\"19 \\n \\n4.2 Project structure \\nFigure 8 depicts the project 's structure, which is separated into three major \\nelements. The index.html file is the primary HTML file for our application, which \\ncontains your React code and serves as a container for React to render. The \\nproject's CSS is to style its apps and component s. The style property adds \\ndynamically calculated styles to React applications at render time. It takes a \\nJavaScript object instead of a CSS string. CSS files are divided into two major \\nfiles: index.css and App.css. App.js, the JavaScript file that contain s the other \\ncomponents, functions as a container for them, and redux is vital to the \\napplication's operation. The last portion contains the file fireConfig.js for \\nconfiguring Firebase.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 25, 'page_label': '26'}, page_content='20 \\n \\n \\nFigure 8. The structure of project \\n4.3 Elements of web application \\n4.3.1 HTML/CSS and JavaScript  \\nThe HTML core section is seen in figure 9. This file includes the links to external \\nCSS sources, which is Bootstrap, in this project. The web application is assigned \\nthe “root” id.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 26, 'page_label': '27'}, page_content='21 \\n \\n \\nFigure 9. The screenshot of HTML file \\nCSS specifies how HTML components should appear on screen, in print, or in \\nother media. CSS eliminates a great deal of labour. It is capable of simultaneously \\ncontrolling the layout of many web pages. In this appl ication, the stylesheets \\ninclude Index.css, App.css, authentication.css, layout.css as well as \\nproducts.css. \\n \\nFigure 10. The screenshots of product.css file'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, page_content='22 \\n \\nThe application was built by multiple parts which are Authentication, Homepage, \\nOrders and Cart. In terms of Authentication, there are two pages which are \\nrespectively Login and Register pages. They are the website entrance that asks \\nfor user identification, which is accomplished by providing username and \\npassword. Authentication is needed to access an entire site. The Homepage \\nenables visitors to do a variety of tasks, including seeing a list of goods with \\nphotographs, and filtering the products by name as well as category. Selecting a \\nparticular product link  to the product detail. The next one is the Order page. In \\ngeneral, the Order page summarizes all user placed orders. Following that comes \\nthe Cart page. The Cart page enables visitors to browse the list of items. \\nAdministrator accounts enable users to con trol all items, users, and orders from \\nthe admin page. Using the React -router-dom package ensures that pages are \\nnavigated accurately and quickly.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, page_content='Administrator accounts enable users to con trol all items, users, and orders from \\nthe admin page. Using the React -router-dom package ensures that pages are \\nnavigated accurately and quickly. \\nThe following list of additional components is shown in Figure 11. Considering \\nthe layout of the website, the re are three main parts which are respectively \\nFooter, Header and Layout. First, component Footer is used to display the footer \\nof the page. Secondly, component Footer is used to display the footer of the page. \\nLast, Component Layout is the custom componen t combine the Header, Footer \\ncomponents and the content to be passed to the Layout. Aside from that, the \\napplication was developed as part of a larger set of components. Component \\nLoader is simply the spinner which is displayed when loading resources. \\nComponent AdminPage, which is only visible to admin users, contains three \\nprimary sections: a list of users, a list of items that may be edited/removed, and'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 27, 'page_label': '28'}, page_content='Component AdminPage, which is only visible to admin users, contains three \\nprimary sections: a list of users, a list of items that may be edited/removed, and \\na list of user orders.  Component CartPage displays the list of products ready to \\ncheck out. Component Homepage allows users to do a range of functions, \\nincluding seeing a list of items  with accompanying images and filtering products \\nby both name and category. Component OrdersPage is place that us er may \\nmanage their paid orders. Component ProductInfo, as its name implies, displays \\ninformation about the product, such as its name, category, and price. Component \\nLoginPage is the initial page of the website, user need a valid account with \\nusername and password. Component RegisterPage allows users register a new \\naccount with correct email, password and confirmation password.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 28, 'page_label': '29'}, page_content='23 \\n \\n \\nFigure 11. The screenshot list of components \\n4.3.2 Redux \\nRedux is the one of most popular libraries to manage state of the application, with \\nfour pillars such Predictable, Centralized, Debuggable and Flexible. Predictable \\nmeans that Redux enables you to create apps that act consistently, are simple to \\ntest, and operate in a variety of settings. Centralized means t hat Adding \\nsophisticated features like undo/redo, state persistence, and other capabilities by \\ncentralizing your application\\'s data and logic is a great way to improve \\nperformance. Debuggable means that Redux DevTools, it is simple to see when, \\nwhen, why, and how your app\\'s state has changed in real time. Changes may be \\nlogged and debugged using \"time travel debugging\" in Redux\\'s design. Error \\nreports can also be sent to the server. Flexible means that Redux can be used \\nwith almost any UI layer and has a thriving add-on ecosystem.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 29, 'page_label': '30'}, page_content='24 \\n \\nIn the application, we use redux to manage the state of products when loading, \\nadding and removing stuffs. \\n \\nFigure 12. The screenshot of redux file \\n4.4 Firebase configuration \\nIn the application, we use Firebase cloud services such as Authentication, \\nFirestore Database, Hosting. To set up the services in the application, we follow \\nthe following steps: \\n• Creating a Firebase project \\n• Installing the SDK and initialize Firebase \\n• Accessing the Firebase in the application'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31'}, page_content='25 \\n \\n \\nFigure 13. The configuration file of Firebase in the application \\n4.5 Interface layout \\nThe screenshots below are of the application. The figure 14 is the homepage that \\nshows a list of goods and a search box for quickly finding things by name. \\nAdditionally, customers may filter items by category on the Homepage. By \\nchoosing certain goods, as seen in Figure 15, the detail page displays some \\nproduct information and a button to add the item to the basket. Once an order \\nhas been placed, the user may manage it via the Order page in figure 16. \\nFollowing the addition of products to the cart, figure 17 displays the list of chosen \\nitems and allows the user to check out the order. The Orders tab displays a list \\nof all previously placed orders in chronological order, allowing the user to quickly \\nexamine the total cost of the transaction as well as the pro duct details. For \\nauthentication, there are two pages. In figure 18, there is a login page, and there'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 30, 'page_label': '31'}, page_content='examine the total cost of the transaction as well as the pro duct details. For \\nauthentication, there are two pages. In figure 18, there is a login page, and there \\nis a register page. Register page is the only one where users are supposed to fill'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 31, 'page_label': '32'}, page_content='26 \\n \\nin things like their username and password. The user can switch between  the \\nRegister and Login pages on their own. \\n \\nFigure 14. The Home page \\n \\nFigure 15. The product detail page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 32, 'page_label': '33'}, page_content='27 \\n \\n \\nFigure 16. The Orders page \\n \\nFigure 17. The Cart page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 33, 'page_label': '34'}, page_content='28 \\n \\n \\nFigure 18. The Login page \\n \\nFigure 19. The Register Page'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35'}, page_content=\"29 \\n \\n5 Deploying the website \\nIn order to get started, install the Firebase tools that will allow you to deploy \\nyour app, and then log into Firebase from your terminal to get things going. If \\nyou haven't already done so, you'll be prompted to enter your email address as \\nwell as your password to continue. It is also necessary to examine the root \\ndirectory of your React application. Your React application will now need the \\nuse of Firebase to be setup.  \\nAs we go, we configure through a series of questions and configuration options, \\nsuch as Select Hosting: Configure and deploy Firebase Hosting sites with a \\ngraphical user interface. Select From the drop-down menu, choose your \\nFirebase Project to work on. After that, you'll need to inform Firebase where to \\nlook for your assets after they've been deployed, which you can do here. Create \\nReact App creates a build folder that includes all the production assets by\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 34, 'page_label': '35'}, page_content=\"look for your assets after they've been deployed, which you can do here. Create \\nReact App creates a build folder that includes all the production assets by \\ndefault, and this is what you should expect. Assuming the default configuration \\nhas not been changed, this should be set to the build state. Finally, you have \\nthe option of having Firebase rebuild your existing build/index.html file with a \\nnew one created by itself. You will not be able to use this functionality until you \\nhave a fully functional version of your application. When presented with this \\nchoice, you should pick N. (No). You should see two new files on your hard disk \\nwhen the starting process is complete. firebase.json and firebaserc are two \\ndifferent types of firebase. Because these files include information about your \\nFirebase hosting setup, you should save and commit them to your git \\nrepository. Everything seems to be in working order, so you can continue with \\ndeploying your application.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36'}, page_content=\"30 \\n \\n6 Testing \\nIt's also critical to do testing to guarantee that the application is functioning and \\ncompatible with the vast majority of browsers available. Moreover, the web text \\nis reviewed for grammatical errors in addition to being error-free. \\nEverything was checked, goods were seen with images, the list was sorted both \\nascending and descending, the products were filtered by name as well as \\ncategory.  Preferred things were put to the cart and deleted from the basket as \\nappropriate. The system functions just as it should in every respect. There were \\nno problems with the examination. \\nThe responsiveness test was conducted on different environments such as \\nAndroid, iOS, and Windows operating systems. There was a favorable conclusion \\nsince not all of the mobile phones that were tested were able to connect to the \\nwebsite without experiencing any difficulties. It seems like all of the visuals, \\nbuttons, and oth er parts of the design are being shown appropriately on the\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 35, 'page_label': '36'}, page_content='website without experiencing any difficulties. It seems like all of the visuals, \\nbuttons, and oth er parts of the design are being shown appropriately on the \\nscreen.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, page_content='31 \\n \\n7 Discussion \\nIt is now simpler than ever to develop a website utilizing a number of platforms \\nand computer languages, such as PHP, Vue, and JavaScript, as a result of recent \\ntechnological advancements . The majority of website builders, mostly \\nconventional ones such as WordPress, provide pre -designed website templates \\nfor users who do not have technical skills. These templat es, on the other hand, \\nare often incapable of being customized as readily as developers would want. \\nOne of the most significant disadvantages is the high initial and ongoing costs of \\ncreating and maintaining the process.  \\nAs a result, we chose React for th is project. JavaScript offers a number of \\nbenefits, including its speed, simplicity, server load, and interoperability with other \\nprogramming languages, to name a few. JavaScript is more efficient since it does \\nnot rely on external resources or a backend server. Because JavaScript is totally'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, page_content=\"programming languages, to name a few. JavaScript is more efficient since it does \\nnot rely on external resources or a backend server. Because JavaScript is totally \\ninterwoven with HTML and CSS, it can be introduced to any web page in a matter \\nof seconds. \\nKnown for its effectiveness in the development of online applications, React is \\none of the most popular JavaScript lightweight frameworks for web development. \\nAs a form of cache, React retains a copy of its data structures in memory. Each \\ntime the code is changed, React automatically identifies and propagates the \\nchanges to the browser, ensuring that the user experience is uninterr upted. The \\nperformance of the website will be improved as a result of this unique feature. \\nThere are several advantages to using Firebase cloud services. The technique \\naids in the discovery of information by customers and increases the website's \\npresence o n the internet. Additionally, the rapid and secure hosting solution\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 36, 'page_label': '37'}, page_content=\"aids in the discovery of information by customers and increases the website's \\npresence o n the internet. Additionally, the rapid and secure hosting solution \\nfacilitates the deployment of web applications. Encryption without the need for \\nsetup Firebase Hosting use SSL encryption to protect the information. Domains \\nare protected from external attacks and data breaches thanks to the use of SSL \\ntechnology. Firebase Hosting makes use of SSD and content delivery networks \\n(CDNs) to deliver material quickly and improve application performance.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 37, 'page_label': '38'}, page_content=\"32 \\n \\nPayment and bills will not be the focus of this project due  to lack of time and \\nresources. However, additional development of these characteristics is required \\nin order to boost the website's economic efficiency. Customer service \\nrepresentatives should notify customers of any changes to the store's promotion \\nand discount policy through email on a regular basis. Another tip is to increase \\nyour website's search engine visibility and network marketing efforts, both of \\nwhich are becoming more important when running an online business. \\nCustomers who are not fluent in English should be able to place orders in a range \\nof languages, including but not limited to specific languages such as German and  \\nFinnish.\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39'}, page_content='33 \\n \\n8 Conclusion \\nThe goal of this thesis is to utilize React to create an e-commerce website that is \\nfast, flexible, and easy to use . The website should also be able to manage the \\nbackend effectively just like, for instance,  Firebase’s cloud services.  This thesis \\nwas a success since the website functions as expected. \\nThere are several features on this website that allow visitors to do a variety of \\ntasks such as seeing a list of goods with thumbnails and full -sized photographs, \\narranging the products by size and adding or deleting things from the shopping \\ncart. \\nThis project was made using React in the front-end and Firebase plays a role as \\nthe backend site. The project was put online after it had completed the \\ndevelopment phase. \\nDifferent platforms were used to co nduct the responsiveness, functionality, and \\nbrowser compatibility tests. We were pleased with the end outcome since our \\nwebsite suited all our needs.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 38, 'page_label': '39'}, page_content='Different platforms were used to co nduct the responsiveness, functionality, and \\nbrowser compatibility tests. We were pleased with the end outcome since our \\nwebsite suited all our needs. \\nThere are a lot of useful features in this web app, yet it is still basic a s well as  \\nappealing enough for an online business. At a modest cost, it gives customers a \\nway to grow their company.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 39, 'page_label': '40'}, page_content='1 \\n \\n \\n \\nReferences  \\n1. What is Diffing Algorithm. Online. 02 February 2022.  <What is Diffing \\nAlgorithm>. Accessed 3 April 2022. \\n2. Horton Adam. Mastering React. Book. 23 February 2016. Accessed 3 April \\n2022. \\n3. Occhino, Tom. 2015. React Native: Bringing modern web techniques to \\nmobile. Online. 26 March 2015. < React Native: Bringing modern web \\ntechniques to mobile>. Accessed 5 April 2022.  \\n4. Sutch, Caelin. 2020. “What is Firebase?”. Online. 17 May 2020. <What is \\nFirebase>. Accessed 5 April 2022. \\n5. 2021. Firebase - Introduction. Online. 15 July 2021. < Firebase- \\nIntroduction>. Accessed 5 April 2022. \\n6. Ogbonna, Chizoba. 2022. Device to Device Push Notification using Cloud \\nFunctions for Firebase. Online. 27 March 2017. <Device to Device Push \\nNotification using Cloud Functions for Firebase>. Accessed 12 April 2022. \\n7. Firebase hosting. Online. <Firebase hosting>. Accessed 12 April 2022.'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 40, 'page_label': '41'}, page_content='Appendix 1 \\n1 (1) \\n \\nFigures \\nFigure 1. How the DOM working in React 4 \\nFigure 2. The overview of Firebase Cloud service. 9 \\nFigure 3. Firebase Authentication service 10 \\nFigure 4. Cloud Function for Firebase 12 \\nFigure 5. The example of Firebase real-time database. 13 \\nFigure 6. The flow of using Firebase Hosting. 15 \\nFigure 7. An example of Cloud Storage for Firebase service 16 \\nFigure 8. The structure of project 20 \\nFigure 9. The screenshot of HTML file 21 \\nFigure 10. The screenshots of product.css file 21 \\nFigure 11. The screenshot list of components 23 \\nFigure 12. The screenshot of redux file 24 \\nFigure 13. The configuration file of Firebase in the application 25 \\nFigure 14. The Home page 26 \\nFigure 15. The product detail page 26 \\nFigure 16. The Orders page 27 \\nFigure 17. The Cart page 27 \\nFigure 18. The Login page 28 \\nFigure 19. The Register Page 28'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '2022-05-03T05:41:28-07:00', 'moddate': '2022-05-03T05:41:28-07:00', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Building an E-commerce Application.pdf', 'total_pages': 42, 'page': 41, 'page_label': '42'}, page_content='Appendix 2 \\n1 (1)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='Academic Journal of Nawroz University (AJNU), Vol.10, No.3, 2022  \\nThis is an open access article distributed under the Cre ative Commons Attribution License  \\nCopyright ©2017. e-ISSN: 2520- 789X \\nhttps://doi.org/10.25007/ajnu.v11n3a14 80 \\n410 \\n \\nFirebase Efficiency in CSV Data Exchange Through PHP-\\nBased Websites \\nRidwan Boya Marqas1, Saman M. Almufti 2, Renas Rajab Asaad 3 \\n1,2,3Department of Computer Science, Nawroz University, Kurdistan Region  –  Iraq \\nABSTRACT \\nA database is a collection of data that may be organized into two distinct forms: relational databases, which use \\nSQL (structure query language), and distributed databases, which use No SQL (non-relational SQL). Both types of \\ndatabases are referred to as databases. The amount of information around the globe is increasing at an exponential \\nrate, leading to the development of data as the world gets  more technologically advanced and computerized. In'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='rate, leading to the development of data as the world gets  more technologically advanced and computerized. In \\nthe vast majority of studies, the NoSQL database is simpl y referred to by its acronym, \"NoSQL.\" In this \\ninvestigation, data is transferred from a website written in PHP t o a CSV file by way of a NoSQL database known \\nas Firebase. To import and export the experimental data, it was necessary to use two different CSV files, each of \\nwhich included 1,000 and 4,997 records, respectively. Exchangin g CSV files with a website that was built using \\nPHP was the method that this study used to test the performance of  Firebase.  \\nKEYWORDS: Database, Firebase, Website, PHP, NoSQL.  \\n1. INTRODUCTION\\nThe term \"database\" is mostly used to refer to a \\ncollection of data that has been structured, saved, and \\naccessible via a computer system (Ullman 2007). \\nToday\\'s digital systems are susceptible to data with \\nhuge dimensions because of the extent, diversity, and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='accessible via a computer system (Ullman 2007). \\nToday\\'s digital systems are susceptible to data with \\nhuge dimensions because of the extent, diversity, and \\ncomplexity of the data universe. [citation needed] In \\naddition, enormous amounts of data must be saved, \\nmanaged, or analyzed using cloud and social media \\nstorage and management systems (Ramzan and Bajwa \\n2018). NoSQL databases are used for enterprise and \\nopen-source database management. These databases \\nstore vast amounts of data across a network of \\ncomputers. The name \"Not Just SQL\" was given to the \\nNoSQL database management system to dispel the \\nwidespread belief that SQL cannot be contained \\n(Ganesh Chandra 2015). \\nNoSQL makes it possible to scale horizontally; hence, \\nits many implementations are always kept on distinct \\nservers. The column-based NoSQL database stores \\ndata in a single huge table, as opposed to the relational \\ndatabase, which stores data in numerous tables. This is'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='servers. The column-based NoSQL database stores \\ndata in a single huge table, as opposed to the relational \\ndatabase, which stores data in numerous tables. This is \\ndone to make autoscaling easier (Lee and Zheng 2015). \\nFirebase is a NoSQL database that is hosted on the \\ncloud and runs in that environment. Even when the \\nlocal cache network is off, it is still feasible to \\nsynchronize all the devices that are linked together. \\nThis database is driven by events, and in comparison, \\nto ordinary SQL databases, it operates considerably \\ndifferently (Moroney and Moroney 2017a). PHP was \\ninitially developed by Rasmus Lerdorf, a Canadian of \\nDanish ancestry, and the most recent version, PHP 7, \\nincorporates all the most recent enhancements (Jentsch \\n1997). \\nThis article looks at how well Firebase performs while \\nimporting and exporting CSV files with websites that \\nare based on the PHP programming language. \\n2. MATERIALS AND METHODS \\nA. Programming Hypertext Protocol (PHP)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 0, 'page_label': '1'}, page_content='importing and exporting CSV files with websites that \\nare based on the PHP programming language. \\n2. MATERIALS AND METHODS \\nA. Programming Hypertext Protocol (PHP)  \\nWeb pages are often built using a dynamic \\nprogramming language called PHP, which may be'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n411 \\n \\n \\nfound on a broad variety of websites and web \\napplications that are constantly updated (Mon et al. \\n2019). To access and save data on a variety of \\nplatforms, the PHP code may be modified to operate \\nwith other web scripting languages. Comma Separated \\nValue (CSV) files may be read and written using \\nfunctions in PHP (Mon et al. 2019). \\nLarge volumes of data are frequently sent across \\ndisconnected applications using CSV files. Commas \\nare commonly used to separate spreadsheet fields in \\nCSV files, whereas system end- of-line characters are \\ncommonly used to separate CSV records in Microsoft \\nExcel or Text pad (Hapeez, Yassin, and Hamzah 2010). \\n2.1 Non-relational database (NoSQL) \\nNon-relational databases, such as NoSQL, do not \\nhave a defined structure and may be accessed using a \\nsimple query language. Data may be distributed and'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content=\"2.1 Non-relational database (NoSQL) \\nNon-relational databases, such as NoSQL, do not \\nhave a defined structure and may be accessed using a \\nsimple query language. Data may be distributed and \\nreproduced in a less controlled environment using \\nNoSQL's huge database. For the foreseeable future, \\nthis database will maintain the ability to store data \\nindependently of any other databases. NoSQL \\ndatabases are organized hierarchically. It's also \\ncapable of handling large volumes of data at high \\nspeeds. The structure of a NoSQL database is \\nhorizontal. NoSQL databases like Cassandra and \\nHBase are among the few that may be used in a NoSQL \\napplication. \\nIn general, there are numerous Relational databases \\nand non-Relational databases. SQL databases include \\nMySQL, Oracle, Microsoft SQL Server, PostgreSQL, \\nand DB2, whereas NoSQL databases include Redis, \\nAmazon DynamoDB, Cassandra, Scylla, HBase, \\nFirebase, MongoDB, Couchbase, Neo4j, Datastax\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='MySQL, Oracle, Microsoft SQL Server, PostgreSQL, \\nand DB2, whereas NoSQL databases include Redis, \\nAmazon DynamoDB, Cassandra, Scylla, HBase, \\nFirebase, MongoDB, Couchbase, Neo4j, Datastax \\nEnterprise Graph, Elasticsearch, Splunk, and Solr. See \\nFig. 1. \\n \\nFig. 1.  Classes of SQL and NoSQL \\nThis table compares several aspects of SQL and \\nNoSQL Databases, such as their scalability and pricing \\nas well as their capacity to hold a large quantity of data \\nand how quickly they can be accessed. \\n3. PROPOSED METHOD \\nThis section describes the proposed technique for \\nreading CSV files and importing data into the firebase \\ndatabase, as well as exporting data from the firebase \\nservice. The execution time was calculated to assess the \\neffectiveness of importing and exporting PHP-based \\nwebsite files using firebase Information. \\n3.1 PHP & CSV \\nPHP needs the following functions for reading and \\nwriting data to and from CSV files: \\nI. FOpen operation \\nThis function is used to open a CSV file'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 1, 'page_label': '2'}, page_content='3.1 PHP & CSV \\nPHP needs the following functions for reading and \\nwriting data to and from CSV files: \\nI. FOpen operation \\nThis function is used to open a CSV file \\nTABLE 1 \\nComparison of SQL and NoSQL criteria  \\nCriteria RELATIONAL \\nDATABASE \\n(SQL) \\nNoSQL \\ndiversity Open and \\nclosed source \\nOpen source  \\nScalability Upgrade a \\nsingle server \\nwith devices \\nUsing standard servers scale \\nhorizontally \\nprice Costly data \\naccess \\nsolution \\ninexpensive than open source \\nand cheaper update \\nAmount of \\ndata \\nLimited Vast data hold \\naccessibility Affect by \\nsingle fail \\nUnaffected by one point of failure \\nthat’s distributed \\nExecution \\ntime \\nLong process \\ntime \\nShort process time \\nComplexity Complex \\ndata creation \\nLess complex data creation \\nimplementati\\non \\nSmall \\nimprovement \\noccurs \\nEach stage own improvement \\noccurs \\nuniformity Structured  Unstructured  \\nSecurity Strong \\nSecurity \\nSecurity not included is related to \\nother parts \\n   \\n   \\n.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n412 \\n \\n \\nusing open (file, mode), where the file \\nrepresents the target file and mode represents \\nthe access required for reading or writing in \\nthe CSV file. \\nII. Fgetcsv function \\nThis function is used to read data from a \\nCSV file by iteratively parsing an open file \\nline by line and checking for data fields. \\nfgetcsv (file, length, separator), where the file \\nis the target file, length is the maximum \\nlength of a CSV row, and separator is a \\ncomma to separate CSV data. \\nIII.  Fputcsv function \\nThis function is used to write data to a \\nCSV file fputcsv (file, fields), where the file is \\nthe destination file and fields are the data \\narray. \\nIV.  Fclose Method \\nThis function is used to open the CSV file fclose (file), \\nwhere the file represents the file to be opened. \\nThe features of Firebase are broken down into several \\ncategories and described in Table 2.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='where the file represents the file to be opened. \\nThe features of Firebase are broken down into several \\ncategories and described in Table 2. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n3.2 PHP & FIREBASE \\nFirebase is a cloud-based, real-time database \\ndeveloped for mobile and web apps, although it \\ncannot be used directly with PHP to create websites. \\nFirebase stores data as JSON; hence, the Composer \\ndependency manager, which provides a standard \\nstructure for managing PHP and library dependencies, \\nis required for linking PHP with Firebase. \\n \\nI. Getreference function  \\nThis function is used to retrieve and push \\nvalues from the source database. \\ngetreference (DB) \\nwhere DB is the Database being \\nreferenced. \\nII. Push functionality \\nThis method inserts a list of data records \\ninto the Firebase database. When adding a \\nnode to a list of data, the Firebase database \\ngenerates a new unique key. \\nPush(Data)'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 2, 'page_label': '3'}, page_content='II. Push functionality \\nThis method inserts a list of data records \\ninto the Firebase database. When adding a \\nnode to a list of data, the Firebase database \\ngenerates a new unique key. \\nPush(Data) \\nData represents the list of data to be \\nuploaded into Database. \\nIII. The getValue method  \\nretrieves data from the Firebase database. \\ngetValue() \\nTABLE 2 \\nCharacteristics of firebase  \\nCriteria FIREBASE \\nType Cloud hosted \\nRealtime \\nDatabase \\nModel \\nDocument \\nStore \\nDevelop by Google \\ncompany \\nRelease 2012 \\nCommercial Yes \\nCloud based Yes \\nServer OS Hosted \\nScheme of \\nData \\nFree schema \\nXML support No \\nSQL No \\nAccess \\nmethods and \\nAPI’s \\nAndroid \\nSupport \\nprogram \\nlanguage \\niOS \\nServer-side \\nscripts \\nJavaScript \\nAPI \\nTriggers RESTful \\nHTTP API \\nConsistency Java \\nForeign keys JavaScript \\nIntegrity Objective-C \\nAuthenticatio\\nn \\nFunctionality \\nare limited \\nwith rules'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n413 \\n \\n \\n \\nFig. 2.  process of import & export. \\nFigure 2 depicts the data import and export processes \\nfor both the firebase databases. \\n4. EXPERIMENT & RESULT \\nThis section demonstrates the results of an \\nexperiment including the import and export of data \\nfrom the firebase databases, with download speeds of \\n34.65 Mbps and upload speeds of 36.06 Mbps. \\nIn the experiment, two hospital-related CSV files \\nwere employed; the first CSV file included 1000 \\nentries, while the second CSV file contained 4997 \\nrecords; both files had 11 columns, as seen in (figure 3). \\n \\nFig. 3.  columns name of CSV file \\n(Figure 4) depicts the full system connection \\nbeginning with the connection from the computer to \\nthe 000webhost server. The server hosts the website \\nthat connects to the distinct databases listed below: \\n• Firebase: in Google server Using NoSQL. \\n \\nFig. 4.  Connection System'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content=\"the 000webhost server. The server hosts the website \\nthat connects to the distinct databases listed below: \\n• Firebase: in Google server Using NoSQL. \\n \\nFig. 4.  Connection System \\nA connection system in (figure 4) demonstrates the \\nconnection of the system used that import and export \\nprocess occurred through it which the connection \\ndistributes between firebase and CSV file by using \\n000webhost hosting server. \\nIn the Firebase online database, a table named hospital \\nwas created according to the CSV structure depicted in \\n(figure 5). \\n \\nFig. 5.  demonstrates the hospital table database \\nstructure online Firebase \\nTable 3 depicts the execution times for importing and \\nexporting data in the firebase database. \\nTABLE 3 \\nduration of Firebase's execution \\nprocess Record No. Firebase \\nimport 1000 138.649094 seconds \\n4997 735.564 seconds \\nexport 1000 0.495048046 seconds \\n4997 0.774774 seconds \\nTable 3 displays the import and export execution\"),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 3, 'page_label': '4'}, page_content='process Record No. Firebase \\nimport 1000 138.649094 seconds \\n4997 735.564 seconds \\nexport 1000 0.495048046 seconds \\n4997 0.774774 seconds \\nTable 3 displays the import and export execution \\ntimes for 1000 and 4997 CSV entries in firebase.  \\n(Figure 6) depicts the import process execution time \\nfor 1000 and 4,997 CSV records into Firebase, while \\n(Figure 7) depicts the export process execution time for \\n1000 and 4,997 CSV records in the Firebase database.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='Academic Journal of Nawroz University (AJNU), Vol. 10, No.3, 2022                                               \\n414 \\n \\n \\n \\nFig. 6.  demonstrates the import process for Firebase \\n \\nFig. 7.  demonstrates the export process for Firebase \\nThe import and export process show the percentage \\nfor 1000 and 4997 CSV record which for import one \\nuse 18% for 1000 and 82% for 4997, while the export \\nprocess show 16% for 1000 and 84% for 4997 CSV \\nrecord. \\n5. CONCLUSION AND RECOMMENDATIONS \\nThis study presents a time performance assessment \\nfor the process of data transferring (import and export) \\nbetween CSV files and Firebase online databases with \\na PHP-based website. \\nThe experimental results for two CSV files containing \\n1000 and 4997 records indicate that the data exporting \\nprocess for Firebase in a PHP-based website is faster \\nthan the importing process. This is because, during the \\nimport process, the connection between the website \\nand Firebase is indirect and requires more time to'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='than the importing process. This is because, during the \\nimport process, the connection between the website \\nand Firebase is indirect and requires more time to \\nreach the database on an external server, whereas the \\nexport process directly reaches the database and \\nretrieves the data. \\nREFERENCES \\nBinani, Sneha, Ajinkya Gutti, and Shivam Upadhyay. \\n2016. “SQL vs. NoSQL vs. NewSQL- A \\nComparative Study.” 6(1):43–46. \\nDB-Engines.com. 2020. “Firebase Realtime Database \\nvs. Sqlite Comparison.” Retrieved June 2, 2020 \\n(https://db-\\nengines.com/en/system/Firebase+Realtime+Dat\\nabase%3BSQLite). \\nGanesh Chandra, Deka. 2015. “BASE Analysis of \\nNoSQL Database.” Future Generation Computer \\nSystems 52:13–21. \\nHapeez, Mohammad Shukri, Mohd Ihsan Mohd \\nYassin, and Mustafar Kamal Hamzah. 2010. \\n“Storage of Online HPLC Data for \\nPharmaceutical Research Applications Using \\nXML Database.” Proceedings of the 2010 5th IEEE \\nConference on Industrial Electronics and \\nApplications, ICIEA 2010 1605–9.'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='Pharmaceutical Research Applications Using \\nXML Database.” Proceedings of the 2010 5th IEEE \\nConference on Industrial Electronics and \\nApplications, ICIEA 2010 1605–9. \\nJentsch, Birgit. 1997.                             Gender Politics \\nand Post-Communism: Reflections from Eastern \\nEurope and the Former Soviet Union. Vol. 3. \\nLee, Chao Hsien and Yu Lin Zheng. 2015. “Automatic \\nSQL-to-NoSQL Schema Transformation over the \\nMySQL and HBase Databases.” 2015 IEEE \\nInternational Conference on Consumer \\nElectronics - Taiwan, ICCE-TW 2015 426–27. \\nMon, Cho Thet, Su Su Hlaing, Mie Mie Tin, Mie Mie \\nKhin, Tin Moh Moh Lwin, and Khin Mar Myo. \\n2019. “Code Readability Metric for PHP.” 2019 \\nIEEE 8th Global Conference on Consumer \\nElectronics, GCCE 2019 929–30. \\nMoroney, Laurence and Laurence Moroney. 2017a. \\n“An Introduction to Firebase.” The Definitive \\nGuide to Firebase 1–24. \\nMoroney, Laurence and Laurence Moroney. 2017b. \\n“The Firebase Realtime Database.” The Definitive'),\n",
       " Document(metadata={'producer': 'GPL Ghostscript 9.26', 'creator': '', 'creationdate': \"D:20220916121519Z00'00'\", 'moddate': \"D:20220916121519Z00'00'\", 'author': '', 'title': '', 'subject': '', 'keywords': '', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_Data_Exchange_with_CSV_Files_in_PHP_Base.pdf', 'total_pages': 5, 'page': 4, 'page_label': '5'}, page_content='“An Introduction to Firebase.” The Definitive \\nGuide to Firebase 1–24. \\nMoroney, Laurence and Laurence Moroney. 2017b. \\n“The Firebase Realtime Database.” The Definitive \\nGuide to Firebase 51–71. \\nRamzan, Shabana and Imran Sarwar Bajwa. 2018. “SS \\nSymmetry An Intelligent Approach for Handling \\nComplexity by Migrating from Conventional \\nDatabases to Big Data.” \\nStephens, Jon and Chad Russell. 2004. Beginning \\nMySQL Database Design and Optimization. \\nUllman, J. D. 2007. “A First Course in Database \\nSystems.” Pearson Education India. Wang, K. C. \\n2018. Systems Programming in Unix/Linux.  \\n \\n \\n18%\\n82%\\nImport\\n1000\\n4997\\n16%\\n84%\\nexport\\n1000\\n4997'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 0, 'page_label': '1'}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/362539877\\nFIREBASE -OVERVIEW AND USAGE\\nArticle\\xa0\\xa0in\\xa0\\xa0Journal of Engineering and Technology Management · August 2022\\nCITATIONS\\n35\\nREADS\\n15,713\\n5 authors, including:\\nAnil Trimbakrao Gaikwad\\nBharati Vidyapeeth Deemed to be University\\n19 PUBLICATIONS\\xa0\\xa0\\xa039 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Anil Trimbakrao Gaikwad on 07 August 2022.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1178] \\nFIREBASE - OVERVIEW AND USAGE \\nPankaj Chougale*1, Vaibhav Yadav*2, Dr. Anil Gaikwad*3 \\n*1,2Student, Bharati Vidyapeeth Deemed To Be University, Pune. Institute Of Management,  \\nKolhapur, India. \\n*3Guide, Bharati Vidyapeeth Deemed To Be University, Pune. Institute Of Management,  \\nKolhapur, India. \\nABSTRACT \\nThe web application has relied heavily on large amounts of websites and random data such as videos, photos, \\naudio, text, files and other inappropriate content. Icon It is difficult for the Relational Database Management'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='audio, text, files and other inappropriate content. Icon It is difficult for the Relational Database Management \\nSystem (RDBMS) to manage random data . Firebase is a new technology for managing large amounts random \\ndata. Very fast compared to RDBMS. This research paper focuses on us age of Firebase for Android and aims to \\nfamiliarize itself with its  functions, related concepts names, benefits and limitat ions. The paper also tries to \\nshow some nakedness Firebase features for building an Android app.  \\nKeywords: Cloud Based-Firebase, Cloud Based-Android. \\nI. INTRODUCTION \\nGoogle Firebase  may be a  Google-backed application development software  that allows developers to develop \\nIOS, Android and Web apps. Firebase provides tools for tracking analytics, reporting and fixing app crashes, \\ncreating marketing and products experiment. \\nII. METHODOLOGY \\nFirebase offers variety of services, including:'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 1, 'page_label': '2'}, page_content='creating marketing and products experiment. \\nII. METHODOLOGY \\nFirebase offers variety of services, including: \\nAnalytics – Google Cloud Analytics for Firebase offers free, unlimited reporting’s on as many as 500  or more \\nseparate events. Statistics present content on user behavior in IOS and Android app lications to enable best \\ndecision-making about improving an application performance and marketing. \\n \\n(source: https://i.ytimg.com/vi/8iZpH7O6zXo/maxresdefault.jpg) \\nAuthentication: Firebase authentication makes it easy for developers to protect authentication systems and \\nimproves login and boarding experience for users. This the feature offers an all -in-one identity solution, \\nsupporting email accounts and password, phone auth, moreov er like Google login, Facebook, GitHub, Twitter \\nand more.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1179] \\n \\n(source: https://firebase.google.com/images/products/auth/auth -3.png) \\nCloud Messaging: Firebase Cloud Messaging (FCM) may be various messaging tool allowing companies to trust \\nand receive messages on IOS, Android and therefore the web at no cost. \\n \\n(source: https://i.ytimg.com/vi/sioEY4tWmLI/maxresdefault.jpg) \\nRealtime Database: The Firebase Realtime Database may be a cloud -based NoSQL site that allows data to be \\nstored and synchronized between users in real time. Information is synced to all clients in real time and is'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 2, 'page_label': '3'}, page_content='stored and synchronized between users in real time. Information is synced to all clients in real time and is \\nalways available when the app is offline. \\n \\n(source: https://media.geeksforgeeks.org/wp-content/uploads/20190421141241/gfg53.png)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1180] \\nCrashlytics: Firebase C rashlytics may be a real-time crash journalist assisting engineers  track, prioritize and \\nfix problems that diminish the quality of their applications. With Crashlytics, engineers spend less time \\nplanning and resolving crashes and long construction features for his or her apps. \\n \\n(source: https://firebase.google.com/docs/crashlytics) \\nPerformance: The Cloud-Firebase Performance Monitoring feature provides developers with an'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 3, 'page_label': '4'}, page_content='(source: https://firebase.google.com/docs/crashlytics) \\nPerformance: The Cloud-Firebase Performance Monitoring feature provides developers with an \\nunderstanding of app lication features of their IOS and Android apps to help them decide where to reach and \\nwhen the performance of their applications is improved. \\n \\n(source: https://miro.medium.com/max/1400/1*yevJMct9erdfRGH8YgJ1_g.png ) \\nTest Lab:  Firebase Test Lab  may be cloud-based application testing infrastructure. By one performa nce, \\ndevelopers can test their I OS or Android apps across all device devices as well device con figuration. They will \\nsee results, including videos, screenshots and logs, inside the Firebase console. \\n \\n(source:https://d1qmdf3vop2l07.cloudfront.net/unwavering-snake.cloudvent.net/hash-\\nstore/15f9baced6535d0042fc1d098c262d2c.png)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1181] \\nUse Cases: \\nConditions for using Firebase include: \\nCreate on boarding flows:  Developers can provide users with a fast, intuitive login process Firebase \\nVerification. allow users to sign in to their apps via Google, Twitter, Facebook or GitHub accounts in but five \\nminutes. Engineers can track each step their ride flow to enhance use r experience. Additionally, engineers can \\nuse it Google Analytics Firebase entry events for each step flow ride, create panels to see where users are'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='use it Google Analytics Firebase entry events for each step flow ride, create panels to see where users are \\nleaving and using the remote control adjusting to make changes to their operating systems to determine how  \\nthose changes affect them conversion. \\nCustomize a “welcome back” screen:  Engineers can use it to make it your own to stop everything user very \\neasy experience customizable first user -supported screen favorites, usage history, location or language. \\nEngineers can explain by audience, by section, in user behavior and display targeted content to all audiences.  \\nGradually unleash new features: Developers can launch new features . \\nProgressively roll out new features:  Developers can introduce new low -risk features by first testing those \\nfeatures in some users to determine how they operate and how users respond. Then, If the engineers are \\nsatisfied. \\nIn June 2018, mobile security company Ap pthority reported thousands of I OS and Android devices mobile'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='satisfied. \\nIn June 2018, mobile security company Ap pthority reported thousands of I OS and Android devices mobile \\napplications expo sing more than 113 GB of information about 2,271 unprepared Firebase database.  As of \\nJanuary 2018, Authority researchers are scanning Android applications that use Firebase systems to store users \\ndata or content  and updated communication patterns for applications made on Cloud-Firebase domains.  \\nAfter scanning mor e than 2.7 million Android and I OS apps, researchers identified 28,502 mobile apps (1,275 \\nIOS and 27,227 Android) that connected and stored data within t he Firebase background. Of these, 3,046 apps \\n(600 IOS and pear, 446 Android) store data within 2,271 Firebase poorly designed websites that give anyone \\nthe ability to view their content. \\nThe database revealed more than 100 million user data records, includ ing the actual LinkedIn, Firebase,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='the ability to view their content. \\nThe database revealed more than 100 million user data records, includ ing the actual LinkedIn, Firebase, \\nFacebook and company data store tokens; 25 million GPS location records; more than 4 million health \\ninformation records are protected, such as medical information and chat messages; 2.6 million User IDs and \\npasswords; and  50,000 financial records, including payments, banking and transactions for Bit coin . \\nPricing: \\nFirebase offers a free 1 GB real -time data storage system and two paid subscriptions plans: Flame Plan ($ 25 \\nper month with 2.5 GB of storage) and Blaze Plan (pa y-as-you-go, $ 5per GB per storage). All programs include \\nA / B testing, statistics, application identification, authentication (without phone auth), cloud messaging, \\nCrashlytics, dynamic links, invitations, functionality monitoring, forecasting and remote  preparation. The main'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Crashlytics, dynamic links, invitations, functionality monitoring, forecasting and remote  preparation. The main \\ndifferences between programs include real -time shared storage  website, number of download tasks, Cloud \\nFirestone bandwidth, and more. Internet of Things Engineers share many common usage requirements across a \\nwide range of IoT  applications. Data collection, delivery of content with low latency, and to prevent \\ncommunication between devices and back -up services there are only three  of these common requirements.  \\nWhile meeting common needs  are often challenging from time to time, I oT development platforms  like Google \\nFirebase provide services and functionality that allows developers to satisfy many of those requirements. \\nBenefits of Google Firebase: \\nThe Firebase platform as a service includes a NoSQL document data store. Application s store data as JSON'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='Benefits of Google Firebase: \\nThe Firebase platform as a service includes a NoSQL document data store. Application s store data as JSON \\nobjects and interact with the database  employing a  JavaScript API. Mobile  or Android  developers have  the \\noption of using Android or I OS APIs, too.  The information store is intended  to scale with application demand, \\nso there\\'s no have to add servers, partition data or perform other database administration that comes  together \\nwith maintaining your own database  rear. The REST  full API includes queries  operations but not SQL \\noperations. The query API is ready-made to figure with channels of information. for instance, the \"on\" operation \\nlistens for changes in location and invokes a callback Function if a selected event occurs. There also are queries \\nand operators with SQL -equivalents, like order-By-Key, order-By-Value and order -By-Priority. The limit, limit -'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 4, 'page_label': '5'}, page_content='and operators with SQL -equivalents, like order-By-Key, order-By-Value and order -By-Priority. The limit, limit -\\nTo-First and limit-To-Last query operations can restrict  the amount of JSON documents returned by the query.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1182] \\nIn addition to storing dynamic contents within the firebase store, developers can store static content  that it will \\nbe required within the app in a Firebase managed service. Way of providing static content storage is the same \\nas installing a website; Firebase will do verify domain ownership, provide an SSL certificate and use it \\nthroughout Firebase content delivery network. Finally, name service records must be updated to become a map  \\nthe name of your hosted site.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='throughout Firebase content delivery network. Finally, name service records must be updated to become a map  \\nthe name of your hosted site. \\nAnother advantage of Firebase is that it provides support for offline operation. Website  functions are locally \\nrecorded and synchronized with the Firebase site when a network connection is established.  Google Firebase \\nincludes mobile data protection controls at rest. Data is transmitted using SSL / TLS 2048 -bit encryption, and \\nwithin the site , users are authenticated and may be restricted to certain functions using a set of security rules. \\nFirebase authentication uses an existing login server or client side code. Firebase supports currently usernames \\n/ password login such as social login resou rces like Google, Twitter and Facebook. Custom code can be used if \\nyou would like to receive your tokens.  In the case of users or in the case of  IOT, devices - already authorized,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='you would like to receive your tokens.  In the case of users or in the case of  IOT, devices - already authorized, \\nsecurity rules govern the activities they will perform and the data they wi ll access. Safety rules support reading \\n/ writing performance, integrated can expect, moreover as a guaranteed job. this can usually determine the \\ncorrect format of the information element and its data type. New apps automatically provide full read / write  \\naccess to the site; engineers should ensure that they review safety rules to limit the performance and breadth of \\ninformation the tool can use. Attackers can gain multiple or higher rights to compromise the integrity or \\nconfidentiality of information within a data store. \\nGoogle Cloud Firebase provides six price categories with its different services: - Free, Spark, Candle, Bonfire, \\nBlaze and Inferno. Sans Free tier, price from $ 5.00 / month on Spark to $ 1,499.00 / month on Inferno. Three to'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='Blaze and Inferno. Sans Free tier, price from $ 5.00 / month on Spark to $ 1,499.00 / month on Inferno. Three to \\nsix stages al l include unlimited real -time internet connection support with users, additionally as 1 TB data \\ntransfer. The biggest difference between categories is reflected in the final volume of real -time data and \\ntransfer grants. Spark, for example, offers 1 GB of s torage and 10 GB of transmission while Inferno, the leading \\ntier, offers 300 GB of storage space and allows up to 1.5 TB of transfer data. the three most advanced categories \\nalso offer non-public backup options to customers. \\nDrawbacks  Of  Google-Firebase: \\nFirebase is very useful for IOT apps  that use data that can use the Javascript API to access Content and security \\nservices. However, developers should look to other tools to increase the back -end performance of their I OT \\nsystem. Basic server side processing is provided, but more advanced a nalysis may require uploading IO T data'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='system. Basic server side processing is provided, but more advanced a nalysis may require uploading IO T data \\nto a different location, such as Hadoop or Spark. \\nAnother challenge is that the Firebase query function is limited. If you need more advanced query functions, \\nconsider sending data to an Elastic  Search server or collection for more search options. In addition, if visibility \\nand alerts are important when monitoring information distribution, consider tools such as Kibana, Elastic  \\nsearch data display. \\nIII. RESULTS AND DISCUSSION \\nOut-of-container notification response. you can use Firebase Notifications, a notification server with an Internet \\nconsole that allows anyone to send alerts to specific target audiences based on Firebase Analytics data.  \\nRealtime Database in Firestore uses NoSQL , a cloud -based website to store and sync real -time data between \\nusers and devices. In addition, Firebase Cloud Storage stores and stores user -generated content such as images,'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='users and devices. In addition, Firebase Cloud Storage stores and stores user -generated content such as images,  \\naudio, and video more smoothly. Using Cloud Tasks, you can measure your applica tions at any time in your life \\ncycle without measuring the servers in use.  Hosting and verification is easily managed efficiently in Firebase \\nwith simple methods and effective tools. The ready -to-use Firebase ML Kit APIs carefully add advanced features \\nto machine learning in  the operating system. The best thing is that your limited knowledge and experience in \\nmachine learning would not be an obstacle to adding a little ingenuity to an advanced app.  \\nIV. CONCLUSION \\nThis paper highlights research on the Firebase API provided by Google and its unique features. This paper helps  \\nyou to learn how to utilize the Firebase in the Android app based on the Clients  requirements. This also helps'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 5, 'page_label': '6'}, page_content='you to learn how to utilize the Firebase in the Android app based on the Clients  requirements. This also helps \\nto make android apps faster and more efficient as there is no PHP required as a third party language to \\ncommunicate with the website. Provides a secure communication channel and website directly from JAVA.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word 2010', 'creator': 'Microsoft® Word 2010', 'creationdate': '2021-12-26T11:46:02+05:30', 'author': 'Rampyari', 'moddate': '2021-12-26T11:46:02+05:30', 'rgid': 'PB:362539877_AS:1186442197909504@1659881271391', 'source': 'D:\\\\RAG\\\\project\\\\data\\\\pdfs\\\\[firebase]_FIREBASE_OVERVIEW AND USAGE.pdf', 'total_pages': 7, 'page': 6, 'page_label': '7'}, page_content='e-ISSN: 2582-5208 \\nInternational Research Journal of  Modernization in Engineering   Technology  and  Science \\n( Peer-Reviewed, Open Access, Fully Refereed International Journal ) \\nVolume:03/Issue:12/December-2021             Impact Factor- 6.752                                 www.irjmets.com   \\nwww.irjmets.com                              @International Research Journal of Modernization in Engineering, Technology and Science \\n [1183] \\nLearning materials are based on data provided online and refer to the examples provided. Google has been \\nconstantly updating Firebase, Ad Sense  is a beta section of Firebase. Not only for Android but also for cross -\\nplatform connectivity .The function can be expanded by adding new feature s and testing new possibilities to \\nAndroid apps. \\nV. REFERENCES \\n[1] https://firebase.google.com/ \\n[2] https://www.javatpoint.com/firebase \\n[3] https://www.tutorialspoint.com/firebase/index.html \\n[4] https://youtu.be/b1bGrWrx5Mo \\nView publication stats')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c75fac",
   "metadata": {},
   "source": [
    "#### Embedding and vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7c9be2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generate embeddings for document chunks using all-MiniLM-L6-v2.\n",
    "\n",
    "    Returns:\n",
    "        List of embeddings (numpy arrays)\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    embeddings = model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "47cab214",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 816.55it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Batches: 100%|██████████| 18/18 [00:18<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "embeddings = make_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44f140ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_in_chromadb(chunks, embeddings, collection_name=\"mit_documents\"):\n",
    "    \"\"\"\n",
    "    Store chunks and embeddings in ChromaDB.\n",
    "    \"\"\"\n",
    "\n",
    "    client = chromadb.PersistentClient(\"../data/vector_store\")\n",
    "    collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "    ids        = [str(uuid.uuid4()) for _ in chunks]\n",
    "    metadatas  = [chunk.metadata for chunk in chunks]\n",
    "    documents  = [chunk.page_content for chunk in chunks]\n",
    "    embeds     = [embedding.tolist() for embedding in embeddings]  # chromadb expects plain list, not numpy array\n",
    "\n",
    "    collection.add(\n",
    "        ids        = ids,\n",
    "        metadatas  = metadatas,\n",
    "        documents  = documents,\n",
    "        embeddings = embeds\n",
    "    )\n",
    "\n",
    "    print(f\"Successfully stored {len(chunks)} chunks in ChromaDB collection: '{collection_name}'\")\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d89e084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully stored 559 chunks in ChromaDB collection: 'mit_documents'\n"
     ]
    }
   ],
   "source": [
    "collection = store_in_chromadb(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7837a95f",
   "metadata": {},
   "source": [
    "### Retriever pipeline from Vectorstore (chromaDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f32c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retriever:\n",
    "    \n",
    "    def __init__(self, collection, model_name=\"all-MiniLM-L6-v2\", groq_api_key=\"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\"):\n",
    "        self.collection = collection\n",
    "        self.model      = SentenceTransformer(model_name)\n",
    "        self.groq       = Groq(api_key=groq_api_key)\n",
    "    \n",
    "    def retrieve(self, user_query: str, top_k: int = 5, threshold: float = 0.6) -> list:\n",
    "        query_embedding = self.model.encode(user_query).tolist()\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings = [query_embedding],\n",
    "            n_results         = top_k,\n",
    "            include           = [\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        filtered = []\n",
    "        for i in range(len(results[\"documents\"][0])):\n",
    "            if results[\"distances\"][0][i] < threshold:\n",
    "                filtered.append({\n",
    "                    \"content\"  : results[\"documents\"][0][i],\n",
    "                    \"metadata\" : results[\"metadatas\"][0][i],\n",
    "                    \"distance\" : results[\"distances\"][0][i]\n",
    "                })\n",
    "        \n",
    "        return filtered\n",
    "\n",
    "    def ask(self, user_query: str, top_k: int = 5) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve relevant chunks and pass them as context to Groq LLM.\n",
    "\n",
    "        Args:\n",
    "            user_query: The user's question\n",
    "            top_k: Number of chunks to retrieve\n",
    "\n",
    "        Returns:\n",
    "            LLM's response as a string\n",
    "        \"\"\"\n",
    "        # step 1: retrieve relevant chunks\n",
    "        retrieved_docs = self.retrieve(user_query, top_k=top_k)\n",
    "        \n",
    "        if not retrieved_docs:\n",
    "            return \"I could not find any relevant information in the documents to answer your question.\"\n",
    "        \n",
    "        # step 2: build context from retrieved chunks\n",
    "        context = \"\\n\\n\".join([doc[\"content\"] for doc in retrieved_docs])\n",
    "        \n",
    "        # step 3: build prompt\n",
    "        prompt = f\"\"\"You are a helpful assistant. Answer the user's question based only on the context provided below.\n",
    "            If the answer is not in the context, say \"I don't know based on the provided documents.\"\n",
    "\n",
    "            Context:\n",
    "            {context}\n",
    "\n",
    "            Question: {user_query}\n",
    "            Answer:\"\"\"\n",
    "        \n",
    "        # step 4: call groq LLM\n",
    "        response = self.groq.chat.completions.create(\n",
    "            model    = \"openai/gpt-oss-20b\",\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5711b1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 987.06it/s, Materializing param=pooler.dense.weight]                              \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer is **React Virtual DOM** is a lightweight, in‑memory representation of the real DOM that React maintains. It has the same structure and characteristics as a real DOM, but it is not rendered to the screen immediately. React updates this virtual DOM first, determines the minimal set of changes needed, and then applies those changes to the actual DOM. This process is much faster than manipulating the real DOM directly, improving rendering performance.\n"
     ]
    }
   ],
   "source": [
    "retriever = Retriever(collection=collection)\n",
    "similarity_threshold = 0.5\n",
    "retrieved_docs = []\n",
    "\n",
    "result = retriever.ask(\"What is React Virtual DOM?\", top_k=3)\n",
    "\n",
    "print(f\"Answer is {result}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
